{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for  OMA\n",
      "Loading LLM model NousResearch/Hermes-3-Llama-3.1-8B-GGUF None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4090) - 23587 MiB free\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/bkantz/.cache/huggingface/hub/models--NousResearch--Hermes-3-Llama-3.1-8B-GGUF/snapshots/307a5dfb59aa38d88b6cfd32f44b8ad7c1da9fb8/./Hermes-3-Llama-3.1-8B.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hermes 3 Llama 3.1 8B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = NousResearch\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Hermes-3-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128040\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128040\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7994 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Hermes 3 Llama 3.1 8B\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: PAD token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOG token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =  7605.33 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   532.31 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 10016\n",
      "llama_new_context_with_model: n_ctx_per_seq = 10016\n",
      "llama_new_context_with_model: n_batch       = 2048\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (10016) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1252.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   677.57 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    27.57 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128040', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.padding_token_id': '128040', 'general.basename': 'Hermes-3-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Hermes 3 Llama 3.1 8B', 'general.organization': 'NousResearch', 'general.type': 'model', 'general.size_label': '8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))\n",
    "sys.path.append(os.path.abspath(\"../../backend\"))\n",
    "sys.path.append(os.path.abspath(\"\"))\n",
    "\n",
    "from sqlalchemy.orm import aliased, joinedload, selectinload, contains_eager\n",
    "from sqlalchemy import text\n",
    "from rdflib.plugins.stores.sparqlstore import SPARQLStore\n",
    "from backend.model import (\n",
    "    DeclarativeBase,\n",
    "    Session,\n",
    ")\n",
    "\n",
    "from backend.ontology import OntologyManager, OntologyConfig, Graph\n",
    "from backend.explorative_support import TopicModelling, select\n",
    "from backend.llm_query import (\n",
    "    Entity,\n",
    "    Relation,\n",
    "    EntitiesRelations,\n",
    "    EnrichedEntity,\n",
    "    EnrichedRelation,\n",
    "    EnrichedEntitiesRelations,\n",
    "    SubjectLink,\n",
    "    SubjectInDB,\n",
    "    SubjectLinkDB,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from eval_config import DBPEDIA_CONFIGS, OMA_CONFIGS, SelectionDistribution, EvalConfig\n",
    "\n",
    "# db_setups = [DBPEDIA_CONFIGS[1], OMA_CONFIGS[1]]\n",
    "db_setups = [OMA_CONFIGS[0]]\n",
    "setup=db_setups[0]\n",
    "print(\"Generating for \", setup.name)\n",
    "store = SPARQLStore(\n",
    "    setup.sparql_endpoint,\n",
    "    method=\"POST_FORM\",\n",
    "    params={\"infer\": False, \"sameAs\": False},\n",
    ")\n",
    "graph = Graph(store=store)\n",
    "\n",
    "config = OntologyConfig()\n",
    "\n",
    "ontology_manager = OntologyManager(config, graph)\n",
    "topic_man = TopicModelling(\n",
    "    ontology_manager,\n",
    "    llm_model_id=setup.model_id,\n",
    "    conn_str=setup.conn_str,\n",
    ")\n",
    "llama_model = topic_man.llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "SL = TypeVar(\"SL\")\n",
    "\n",
    "\n",
    "class EnrichedDBEntity(Entity, arbitrary_types_allowed=True):\n",
    "    subject: SubjectInDB\n",
    "\n",
    "\n",
    "class EnrichedDBRelation(Relation, arbitrary_types_allowed=True):\n",
    "    link: SubjectLinkDB\n",
    "\n",
    "\n",
    "def safe_prob(probs: np.array):\n",
    "    if np.sum(probs) == 0:\n",
    "        return np.ones(len(probs)) / len(probs)\n",
    "    else:\n",
    "        return probs / probs.sum()\n",
    "\n",
    "\n",
    "def choose_entity(\n",
    "    links: list[SL],\n",
    "    rs: np.random.RandomState,\n",
    "    dist: SelectionDistribution = SelectionDistribution.INSTANCES,\n",
    ") -> SL:\n",
    "    probs = np.array([link.instance_count for link in links])\n",
    "    probs = (\n",
    "        safe_prob(probs)\n",
    "        if dist == SelectionDistribution.INSTANCES\n",
    "        else np.ones(len(probs)) / len(probs)\n",
    "    )\n",
    "    indices = np.arange(len(links))\n",
    "    choice = rs.choice(indices, p=probs)\n",
    "    return links[choice]\n",
    "\n",
    "\n",
    "def random_downgrade(\n",
    "    cls: SubjectInDB,\n",
    "    rs: np.random.RandomState,\n",
    "    dist: SelectionDistribution = SelectionDistribution.INSTANCES,\n",
    ") -> SubjectInDB:\n",
    "    def get_subclasses(subcls: SubjectInDB):\n",
    "        subclasses = [subcls.sub_classes] + [\n",
    "            get_subclasses(sub) for sub in subcls.sub_classes\n",
    "        ]\n",
    "        return [sub for sublist in subclasses for sub in sublist]\n",
    "\n",
    "    # print(f\"downgrading {cls.label}\", [sc.label for sc in cls.sub_classes])\n",
    "    subclasses = [cls] + get_subclasses(cls)\n",
    "    probs = np.array([float(sc.instance_count) for sc in subclasses])\n",
    "    probs = (\n",
    "        safe_prob(probs)\n",
    "        if dist == SelectionDistribution.INSTANCES\n",
    "        else np.ones(len(probs)) / len(probs)\n",
    "    )\n",
    "    # print(f\"{cls.label} probs\", probs)\n",
    "    choice = rs.choice(np.arange(len(subclasses)), p=probs)\n",
    "    return subclasses[choice]\n",
    "\n",
    "\n",
    "def choose_graph(\n",
    "    max_nodes=4,\n",
    "    top_k=top_k,\n",
    "    seed=seed,\n",
    "    max_tries=5,\n",
    "    topic_man: TopicModelling = None,\n",
    "    cfg: EvalConfig = DBPEDIA_CONFIGS[0],\n",
    "):\n",
    "    rs = np.random.RandomState(seed)\n",
    "    with Session(topic_man.engine) as session:\n",
    "        session.execute(text(\"SET TRANSACTION READ ONLY\"))\n",
    "        session.autoflush = False\n",
    "\n",
    "        def downgrade_link_subjects(link: SubjectLinkDB):\n",
    "            from_downgrade = random_downgrade(\n",
    "                link.from_subject, rs, cfg.selection_distribution\n",
    "            )\n",
    "            to_downgrade = random_downgrade(\n",
    "                link.to_subject, rs, cfg.selection_distribution\n",
    "            )\n",
    "            link.from_id = from_downgrade.subject_id\n",
    "            link.to_id = to_downgrade.subject_id\n",
    "            link.from_subject = from_downgrade\n",
    "            link.to_subject = to_downgrade\n",
    "            return link\n",
    "\n",
    "        from_subject_alias = aliased(SubjectInDB, name=\"from_subject\")\n",
    "        to_subject_alias = aliased(SubjectInDB, name=\"to_subject\")\n",
    "        best_links_db = session.execute(\n",
    "            select(SubjectLinkDB)\n",
    "            .filter(SubjectLinkDB.from_id != SubjectLinkDB.to_id)\n",
    "            # .filter(SubjectLinkDB.to_id != None)\n",
    "            .order_by(SubjectLinkDB.instance_count.desc())\n",
    "            .join(\n",
    "                from_subject_alias,\n",
    "                SubjectLinkDB.from_subject.of_type(from_subject_alias),\n",
    "            )\n",
    "            .join(to_subject_alias, SubjectLinkDB.to_subject.of_type(to_subject_alias))\n",
    "            # .options(\n",
    "            #     lazyload(SubjectLinkDB.from_subject.of_type(from_subject_alias)),\n",
    "            #     lazyload(SubjectLinkDB.to_subject.of_type(to_subject_alias)),\n",
    "            # )\n",
    "            .limit(top_k)\n",
    "        ).all()\n",
    "        best_links: list[SubjectLinkDB] = [link[0] for link in best_links_db]\n",
    "        choice = choose_entity(best_links, rs, cfg.selection_distribution)\n",
    "        choice_downgraded = downgrade_link_subjects(choice)\n",
    "\n",
    "        current_links = [\n",
    "            (\n",
    "                EnrichedDBRelation(\n",
    "                    entity=choice_downgraded.from_subject.label,\n",
    "                    relation=choice.label,\n",
    "                    target=choice_downgraded.to_subject.label,\n",
    "                    link=choice_downgraded,\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "        current_nodes = {\n",
    "            subject.label: EnrichedDBEntity(\n",
    "                identifier=subject.label,\n",
    "                type=subject.label,\n",
    "                subject=subject,\n",
    "            )\n",
    "            for subject in [\n",
    "                choice_downgraded.from_subject,\n",
    "                choice_downgraded.to_subject,\n",
    "            ]\n",
    "        }\n",
    "        retries = 0\n",
    "        while len(current_nodes) < max_nodes:\n",
    "            start_node = choose_entity(\n",
    "                list([nd.subject for nd in current_nodes.values()]),\n",
    "                rs,\n",
    "                cfg.selection_distribution,\n",
    "            )\n",
    "            left_right = rs.choice([0, 1])\n",
    "            extending_to = left_right == 0\n",
    "            query = (\n",
    "                select(SubjectLinkDB)\n",
    "                .where(SubjectLinkDB.from_id != SubjectLinkDB.to_id)\n",
    "                .where(SubjectLinkDB.to_id is not None)\n",
    "                .join(\n",
    "                    from_subject_alias,\n",
    "                    SubjectLinkDB.from_subject.of_type(from_subject_alias),\n",
    "                )\n",
    "                .join(\n",
    "                    to_subject_alias, SubjectLinkDB.to_subject.of_type(to_subject_alias)\n",
    "                )\n",
    "                .filter(\n",
    "                    SubjectLinkDB.link_id.not_in(\n",
    "                        [link.link.link_id for link in current_links]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            if extending_to:\n",
    "                query = query.where(SubjectLinkDB.from_id == start_node.subject_id)\n",
    "            else:\n",
    "                query = query.where(SubjectLinkDB.to_id == start_node.subject_id)\n",
    "            # print(\"Extending to\", start_node.label, \"extending_to\", extending_to)\n",
    "            new_links = session.execute(\n",
    "                query.order_by(SubjectLinkDB.instance_count.desc()).limit(top_k)\n",
    "            ).all()\n",
    "            new_links: list[SubjectLinkDB] = [link[0] for link in new_links]\n",
    "            if len(new_links) == 0:\n",
    "                if retries >= max_tries:\n",
    "                    break\n",
    "                retries += 1\n",
    "                continue\n",
    "            choice = choose_entity(new_links, rs, cfg.selection_distribution)\n",
    "\n",
    "            extending_direction = (\n",
    "                (choice.from_subject, choice.to_subject)\n",
    "                if extending_to\n",
    "                else (choice.to_subject, choice.from_subject)\n",
    "            )\n",
    "\n",
    "            downgraded_extend = random_downgrade(\n",
    "                extending_direction[1], rs, cfg.selection_distribution\n",
    "            )\n",
    "            # print(\n",
    "            #     \"Downgraded from\",\n",
    "            #     extending_direction[1].label,\n",
    "            #     downgraded_extend.label,\n",
    "            #     \"on link\",\n",
    "            #     choice.label,\n",
    "            # )\n",
    "            if extending_to:\n",
    "                choice.to_id = downgraded_extend.subject_id\n",
    "                choice.to_subject = downgraded_extend\n",
    "            else:\n",
    "                choice.from_id = downgraded_extend.subject_id\n",
    "                choice.from_subject = downgraded_extend\n",
    "            current_links.append(\n",
    "                EnrichedDBRelation(\n",
    "                    entity=choice.from_subject.label,\n",
    "                    relation=choice.label,\n",
    "                    target=choice.to_subject.label,\n",
    "                    link=choice,\n",
    "                )\n",
    "            )\n",
    "            current_nodes.update(\n",
    "                {\n",
    "                    subject.label: EnrichedDBEntity(\n",
    "                        identifier=subject.label,\n",
    "                        type=subject.label,\n",
    "                        subject=subject,\n",
    "                    )\n",
    "                    for subject in [extending_direction[0], downgraded_extend]\n",
    "                }\n",
    "            )\n",
    "        enriched_erl = EnrichedEntitiesRelations(\n",
    "            entities=[\n",
    "                EnrichedEntity(\n",
    "                    subject=topic_man.oman.enrich_subject(entity.subject.subject_id),\n",
    "                    **entity.model_dump(exclude=[\"subject\"]),\n",
    "                )\n",
    "                for entity in current_nodes.values()\n",
    "            ],\n",
    "            relations=[\n",
    "                EnrichedRelation(\n",
    "                    link=SubjectLink.from_db(relation.link, topic_man.oman),\n",
    "                    **relation.model_dump(exclude=[\"link\"]),\n",
    "                )\n",
    "                for relation in current_links\n",
    "            ],\n",
    "        )\n",
    "        return enriched_erl\n",
    "\n",
    "\n",
    "erl = choose_graph(seed=304, max_nodes=10, topic_man=topic_man, cfg=setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('co-orthology relation', 'has co-orthologous member', 'gene region'),\n",
       " ('co-orthology relation', 'has inparalogous member', 'inparalogy relation')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(link.entity, link.relation, link.target) for link in erl.relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"identifier\":\"Hierarchical clustering tree\",\"type\":\"Hierarchical clustering tree\",\"constraints\":[],\"subject\":{\"subject_id\":\"<http://purl.org/net/orth#HierarchicalClusteringTree>\",\"label\":\"Hierarchical clustering tree\",\"spos\":{\"rdfs:label\":{\"property\":\"rdfs:label\",\"label\":null,\"values\":[{\"value\":\"Hierarchical clustering tree\",\"label\":null}]},\"rdfs:subClassOf\":{\"property\":\"rdfs:subClassOf\",\"label\":null,\"values\":[{\"value\":\"<http://purl.obolibrary.org/obo/CDAO_0000012>\",\"label\":\"RootedTree\"}]}},\"subject_type\":\"class\",\"refcount\":0,\"descendants\":{},\"total_descendants\":0,\"properties\":{},\"instance_count\":0}}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erl.entities[0].model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f94e3484500>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def viz_graph(erl: EnrichedEntitiesRelations):\n",
    "    G = nx.DiGraph()\n",
    "    for node in erl.entities:\n",
    "        G.add_node(node.identifier, label=node.type)\n",
    "    for link in erl.relations:\n",
    "        G.add_edge(\n",
    "            link.entity, link.target, weight=link.link.instance_count, label=link.relation\n",
    "        )\n",
    "    pos = nx.shell_layout(G)\n",
    "    nx.draw_networkx(\n",
    "        G,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        labels={node.identifier: node.type for node in erl.entities},\n",
    "    )\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        G, pos, edge_labels={(link.entity, link.target): link.relation for link in erl.relations}\n",
    "    )\n",
    "    return G\n",
    "\n",
    "\n",
    "viz_graph(erl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a person has a birth place with populated place, and a person has a alma mater with school, and a radio station has a broadcast area with populated place, and a populated place has a leader name with athlete, and a person has a military service with military service, and a album has a auteur with person, and a river has a mouth place with populated place, and a person has a career station with career station, and a Sports team member has a career station with career station\n"
     ]
    }
   ],
   "source": [
    "def erl_to_templated_query(erl: EnrichedEntitiesRelations):\n",
    "    qs= []\n",
    "    for rel in erl.relations:\n",
    "        qs.append(f\"a {rel.entity} has a {rel.relation} with {rel.target}\")\n",
    "    return \", and \".join(qs)\n",
    "print(erl_to_templated_query(erl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"relations\":[{\"entity\":\"person\",\"relation\":\"birth place\",\"target\":\"populated place\"},{\"entity\":\"person\",\"relation\":\"alma mater\",\"target\":\"school\"},{\"entity\":\"radio station\",\"relation\":\"broadcast area\",\"target\":\"populated place\"},{\"entity\":\"populated place\",\"relation\":\"leader name\",\"target\":\"athlete\"},{\"entity\":\"person\",\"relation\":\"military service\",\"target\":\"military service\"},{\"entity\":\"album\",\"relation\":\"auteur\",\"target\":\"person\"},{\"entity\":\"river\",\"relation\":\"mouth place\",\"target\":\"populated place\"},{\"entity\":\"person\",\"relation\":\"career station\",\"target\":\"career station\"},{\"entity\":\"Sports team member\",\"relation\":\"career station\",\"target\":\"career station\"}],\"entities\":[{\"identifier\":\"person\",\"type\":\"person\",\"constraints\":[]},{\"identifier\":\"populated place\",\"type\":\"populated place\",\"constraints\":[]},{\"identifier\":\"school\",\"type\":\"school\",\"constraints\":[]},{\"identifier\":\"radio station\",\"type\":\"radio station\",\"constraints\":[]},{\"identifier\":\"athlete\",\"type\":\"athlete\",\"constraints\":[]},{\"identifier\":\"military service\",\"type\":\"military service\",\"constraints\":[]},{\"identifier\":\"album\",\"type\":\"album\",\"constraints\":[]},{\"identifier\":\"river\",\"type\":\"river\",\"constraints\":[]},{\"identifier\":\"career station\",\"type\":\"career station\",\"constraints\":[]},{\"identifier\":\"Sports team member\",\"type\":\"Sports team member\",\"constraints\":[]}],\"message\":\"Found Relations and Entities\"}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_erl(erl: EnrichedEntitiesRelations):\n",
    "    reduced_erl = EntitiesRelations(entities=erl.entities, relations=erl.relations)\n",
    "    return reduced_erl\n",
    "reduce_erl(erl).model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4090) - 23307 MiB free\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /home/bkantz/.cache/huggingface/hub/models--NousResearch--Hermes-3-Llama-3.1-8B-GGUF/snapshots/307a5dfb59aa38d88b6cfd32f44b8ad7c1da9fb8/./Hermes-3-Llama-3.1-8B.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hermes 3 Llama 3.1 8B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = NousResearch\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Hermes-3-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128040\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128040\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7994 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Hermes 3 Llama 3.1 8B\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: PAD token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOG token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =  7605.33 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   532.31 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 10016\n",
      "llama_new_context_with_model: n_ctx_per_seq = 10016\n",
      "llama_new_context_with_model: n_batch       = 1024\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (10016) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1252.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1252.00 MiB, K (f16):  626.00 MiB, V (f16):  626.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   677.57 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    27.57 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128040', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.padding_token_id': '128040', 'general.basename': 'Hermes-3-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Hermes 3 Llama 3.1 8B', 'general.organization': 'NousResearch', 'general.type': 'model', 'general.size_label': '8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llama_model = topic_man.llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\"You are a helpful assistant turning relational knowledge into natural language.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": EntitiesRelations(\n",
    "            entities=[\n",
    "                Entity(\n",
    "                    identifier=\"person 1\",\n",
    "                    type=\"person\",\n",
    "                ),\n",
    "                Entity(\n",
    "                    identifier=\"place 1\",\n",
    "                    type=\"place\",\n",
    "                ),\n",
    "                Entity(\n",
    "                    identifier=\"company 1\",\n",
    "                    type=\"person\",\n",
    "                ),\n",
    "            ],\n",
    "            relations=[\n",
    "                Relation(\n",
    "                    entity=\"company 1\",\n",
    "                    relation=\"employs\",\n",
    "                    target=\"person 1\",\n",
    "                ),\n",
    "                Relation(\n",
    "                    entity=\"person 1\",\n",
    "                    relation=\"residence\",\n",
    "                    target=\"place 1\",\n",
    "                ),\n",
    "            ],\n",
    "        ).model_dump_json(),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"a person is employed by a company and the same person resides in a place\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  4906 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2105.09 ms /  4986 tokens\n"
     ]
    }
   ],
   "source": [
    "response = llama_model.create_chat_completion(\n",
    "    # grammar=self.grammar_erl,\n",
    "    messages=messages + [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": erl.model_dump_json(),\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=-1,\n",
    "    temperature=0.7,  # get wild :)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a person is born in a populated place, attends a school, is employed by a radio station, is led by an athlete, serves in military service, creates an album, ends in a river, is in a career station, is a sports team member, and is related to a career station. Additionally, a populated place has a leader named an athlete, and a river has a mouth place.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Organisation member extending_to False\n",
      "Downgraded from organisation company on link organisation member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1143 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     700.79 ms /  1189 tokens\n",
      "  0%|          | 1/900 [00:01<21:35,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to written work extending_to False\n",
      "Downgraded from multi volume publication multi volume publication on link volumes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1152 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     361.06 ms /  1170 tokens\n",
      "  0%|          | 2/900 [00:02<16:02,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to True\n",
      "Downgraded from person person on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1090 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1090 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     327.57 ms /  1105 tokens\n",
      "  0%|          | 3/900 [00:03<14:50,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Organisation member extending_to False\n",
      "Downgraded from organisation organisation on link organisation member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     296.53 ms /  1225 tokens\n",
      "  0%|          | 4/900 [00:04<14:37,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Sports team member extending_to True\n",
      "Extending to Sports team member extending_to True\n",
      "Extending to Sports team member extending_to True\n",
      "Extending to Sports team member extending_to False\n",
      "Extending to Sports team member extending_to True\n",
      "Extending to Sports team member extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 677 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   677 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     267.01 ms /   690 tokens\n",
      "  1%|          | 5/900 [00:04<13:39,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to place extending_to True\n",
      "Downgraded from person function person function on link political leader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     330.86 ms /  1136 tokens\n",
      "  1%|          | 6/900 [00:05<14:21,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to route of transportation extending_to True\n",
      "Downgraded from station station on link route end\n",
      "Extending to station extending_to True\n",
      "Extending to route of transportation extending_to True\n",
      "Downgraded from station station on link route end\n",
      "Extending to route of transportation extending_to True\n",
      "Downgraded from station station on link route junction\n",
      "Extending to station extending_to False\n",
      "Downgraded from route of transportation road on link route junction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 2207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     635.65 ms /  2236 tokens\n",
      "  1%|          | 7/900 [00:07<15:56,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 673 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   673 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    34 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     507.59 ms /   707 tokens\n",
      "  1%|          | 8/900 [00:08<15:16,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to politician extending_to True\n",
      "Downgraded from person Organisation member on link prefect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1171 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    27 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     477.42 ms /  1198 tokens\n",
      "  1%|          | 9/900 [00:09<15:19,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to False\n",
      "Downgraded from sports event soccer tournoment on link champion in double male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     382.96 ms /  1217 tokens\n",
      "  1%|          | 10/900 [00:10<14:55,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link opening theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1287 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     722.68 ms /  1333 tokens\n",
      "  1%|          | 11/900 [00:11<15:44,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to person extending_to True\n",
      "Downgraded from educational institution school on link alma mater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1149 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.80 ms /  1174 tokens\n",
      "  1%|▏         | 12/900 [00:12<16:31,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 758 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   758 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     241.29 ms /   770 tokens\n",
      "  1%|▏         | 13/900 [00:13<14:15,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to True\n",
      "Downgraded from agent sports club on link producer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     312.67 ms /  1121 tokens\n",
      "  2%|▏         | 14/900 [00:14<13:37,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to cricket team extending_to True\n",
      "Extending to winter sport Player extending_to False\n",
      "Extending to winter sport Player extending_to True\n",
      "Extending to winter sport Player extending_to True\n",
      "Extending to winter sport Player extending_to False\n",
      "Extending to winter sport Player extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 678 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   678 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     257.83 ms /   692 tokens\n",
      "  2%|▏         | 15/900 [00:14<13:02,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from athlete athlete on link trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     622.85 ms /  1175 tokens\n",
      "  2%|▏         | 16/900 [00:16<14:36,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work work on link author\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 143 prefix-match hit, remaining 1099 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1099 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    37 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     592.24 ms /  1136 tokens\n",
      "  2%|▏         | 17/900 [00:17<16:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from career station career station on link career station\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 148 prefix-match hit, remaining 1129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     296.09 ms /  1142 tokens\n",
      "  2%|▏         | 18/900 [00:18<15:18,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to musical work extending_to True\n",
      "Downgraded from agent agent on link performer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     482.32 ms /  1168 tokens\n",
      "  2%|▏         | 19/900 [00:19<14:57,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to musical artist extending_to True\n",
      "Downgraded from Band Band on link Music Band\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 141 prefix-match hit, remaining 1410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1410 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     546.73 ms /  1441 tokens\n",
      "  2%|▏         | 20/900 [00:20<15:41,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to song extending_to False\n",
      "Extending to song extending_to True\n",
      "Extending to song extending_to True\n",
      "Extending to song extending_to True\n",
      "Extending to song extending_to True\n",
      "Extending to song extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 687 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   687 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.59 ms /   698 tokens\n",
      "  2%|▏         | 21/900 [00:21<14:02,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from place place on link residence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.36 ms /  1128 tokens\n",
      "  2%|▏         | 22/900 [00:22<14:34,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team sports team on link debut team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    34 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     558.75 ms /  1187 tokens\n",
      "  3%|▎         | 23/900 [00:23<15:12,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Australian rules football player extending_to True\n",
      "Extending to Australian rules football player extending_to True\n",
      "Extending to hockey team extending_to True\n",
      "Extending to Australian rules football player extending_to True\n",
      "Extending to Australian rules football player extending_to False\n",
      "Extending to Australian rules football player extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 685 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   685 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     247.46 ms /   699 tokens\n",
      "  3%|▎         | 24/900 [00:24<13:24,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to river extending_to False\n",
      "Downgraded from body of water river on link outflow\n",
      "Extending to river extending_to False\n",
      "Downgraded from bridge bridge on link crosses\n",
      "Extending to river extending_to False\n",
      "Downgraded from body of water lake on link outflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1835 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1835 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1158.90 ms /  1911 tokens\n",
      "  3%|▎         | 25/900 [00:25<17:14,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to True\n",
      "Downgraded from city city on link capital\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.32 ms /  1166 tokens\n",
      "  3%|▎         | 26/900 [00:26<16:13,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place populated place on link federal state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1158 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     326.61 ms /  1173 tokens\n",
      "  3%|▎         | 27/900 [00:27<15:41,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to album extending_to False\n",
      "Extending to album extending_to True\n",
      "Downgraded from single list single list on link list of singles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     729.51 ms /  1183 tokens\n",
      "  3%|▎         | 28/900 [00:28<15:59,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person function extending_to True\n",
      "Downgraded from person person on link person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1150 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.03 ms /  1172 tokens\n",
      "  3%|▎         | 29/900 [00:30<15:44,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to place extending_to False\n",
      "Downgraded from animal person on link birth place\n",
      "Extending to place extending_to True\n",
      "Downgraded from populated place settlement on link nearest city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    27 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     514.13 ms /  1514 tokens\n",
      "  3%|▎         | 30/900 [00:31<16:21,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work musical work on link author\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 143 prefix-match hit, remaining 1160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    33 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     563.73 ms /  1193 tokens\n",
      "  3%|▎         | 31/900 [00:32<16:37,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place populated place on link federal state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     527.68 ms /  1226 tokens\n",
      "  4%|▎         | 32/900 [00:33<16:54,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to office holder extending_to True\n",
      "Downgraded from political party political party on link other party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     621.31 ms /  1172 tokens\n",
      "  4%|▎         | 33/900 [00:34<16:58,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from educational institution school on link alma mater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1147 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     934.50 ms /  1212 tokens\n",
      "  4%|▍         | 34/900 [00:36<18:31,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 673 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   673 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     307.93 ms /   691 tokens\n",
      "  4%|▍         | 35/900 [00:37<16:09,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link opening theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     433.45 ms /  1156 tokens\n",
      "  4%|▍         | 36/900 [00:37<15:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to stream extending_to False\n",
      "Extending to stream extending_to True\n",
      "Downgraded from country country on link source country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1148 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     421.30 ms /  1171 tokens\n",
      "  4%|▍         | 37/900 [00:38<14:26,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to mountain extending_to False\n",
      "Downgraded from river river on link mouth mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     332.70 ms /  1172 tokens\n",
      "  4%|▍         | 38/900 [00:39<13:08,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to village extending_to False\n",
      "Extending to village extending_to True\n",
      "Extending to village extending_to False\n",
      "Extending to village extending_to True\n",
      "Extending to village extending_to False\n",
      "Extending to village extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 677 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   677 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.14 ms /   687 tokens\n",
      "  4%|▍         | 39/900 [00:40<12:22,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place region on link frazioni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1151 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     323.03 ms /  1166 tokens\n",
      "  4%|▍         | 40/900 [00:41<13:08,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to True\n",
      "Downgraded from musical artist musical artist on link music composer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1241 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    38 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     628.09 ms /  1279 tokens\n",
      "  5%|▍         | 41/900 [00:42<13:39,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to False\n",
      "Extending to settlement extending_to False\n",
      "Downgraded from agent educational institution on link home town\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1180 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.11 ms /  1200 tokens\n",
      "  5%|▍         | 42/900 [00:43<14:20,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place populated place on link federal state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     442.39 ms /  1219 tokens\n",
      "  5%|▍         | 43/900 [00:44<14:43,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 674 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   674 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.10 ms /   686 tokens\n",
      "  5%|▍         | 44/900 [00:45<13:07,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link opening theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1163 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1163 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    27 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     489.01 ms /  1190 tokens\n",
      "  5%|▌         | 45/900 [00:46<13:12,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to False\n",
      "Downgraded from agent organisation on link home town\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1131 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     896.39 ms /  1193 tokens\n",
      "  5%|▌         | 46/900 [00:47<15:13,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to False\n",
      "Downgraded from legislature legislature on link meeting city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.18 ms /  1285 tokens\n",
      "  5%|▌         | 47/900 [00:49<18:47,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 682 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   682 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     316.26 ms /   700 tokens\n",
      "  5%|▌         | 48/900 [00:50<16:07,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 671 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   671 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.89 ms /   681 tokens\n",
      "  5%|▌         | 49/900 [00:50<13:59,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to False\n",
      "Extending to animal extending_to True\n",
      "Downgraded from place place on link death place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    37 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     591.21 ms /  1150 tokens\n",
      "  6%|▌         | 50/900 [00:52<15:20,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work book on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     752.49 ms /  1180 tokens\n",
      "  6%|▌         | 51/900 [00:53<16:39,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to True\n",
      "Downgraded from person athlete on link trainer\n",
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team sports team on link manager club\n",
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team sports team on link debut team\n",
      "Extending to athlete extending_to False\n",
      "Downgraded from sports event sports event on link champion in double male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 2240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2240 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1438.55 ms /  2334 tokens\n",
      "  6%|▌         | 52/900 [00:55<21:13,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to military unit extending_to True\n",
      "Downgraded from person athlete on link notable commander\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     426.73 ms /  1183 tokens\n",
      "  6%|▌         | 53/900 [00:56<18:54,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to False\n",
      "Downgraded from language language on link spoken in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     377.12 ms /  1142 tokens\n",
      "  6%|▌         | 54/900 [00:57<17:37,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to artwork extending_to False\n",
      "Extending to artwork extending_to True\n",
      "Downgraded from museum museum on link museum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     274.51 ms /  1149 tokens\n",
      "  6%|▌         | 55/900 [00:58<15:17,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 682 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   682 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     463.30 ms /   712 tokens\n",
      "  6%|▌         | 56/900 [00:59<14:23,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    35 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     573.67 ms /  1179 tokens\n",
      "  6%|▋         | 57/900 [01:00<14:52,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from country country on link state of origin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 143 prefix-match hit, remaining 1142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     304.98 ms /  1156 tokens\n",
      "  6%|▋         | 58/900 [01:01<14:01,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work work on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 143 prefix-match hit, remaining 1114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1114 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     636.36 ms /  1155 tokens\n",
      "  7%|▋         | 59/900 [01:02<15:42,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to place extending_to True\n",
      "Downgraded from populated place village on link district\n",
      "Extending to place extending_to True\n",
      "Downgraded from person function person function on link political leader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1413 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1413 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     369.99 ms /  1429 tokens\n",
      "  7%|▋         | 60/900 [01:03<14:47,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to False\n",
      "Extending to animal extending_to False\n",
      "Extending to animal extending_to True\n",
      "Downgraded from place town on link death place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1143 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     480.10 ms /  1171 tokens\n",
      "  7%|▋         | 61/900 [01:04<15:03,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to military person extending_to True\n",
      "Downgraded from military unit military unit on link military unit\n",
      "Extending to military unit extending_to False\n",
      "Downgraded from military person military person on link military unit\n",
      "Extending to military person extending_to True\n",
      "Downgraded from military unit military unit on link military unit\n",
      "Extending to military person extending_to True\n",
      "Downgraded from military unit military unit on link military unit\n",
      "Extending to military person extending_to False\n",
      "Extending to military unit extending_to False\n",
      "Downgraded from military person military person on link military unit\n",
      "Extending to military person extending_to True\n",
      "Downgraded from military unit military unit on link military unit\n",
      "Extending to military unit extending_to True\n",
      "Downgraded from person person on link notable commander\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 3300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3300 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    32 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     835.68 ms /  3332 tokens\n",
      "  7%|▋         | 62/900 [01:06<18:20,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 657 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   657 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     283.98 ms /   673 tokens\n",
      "  7%|▋         | 63/900 [01:07<15:48,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from educational institution university on link alma mater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     692.48 ms /  1226 tokens\n",
      "  7%|▋         | 64/900 [01:08<16:47,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from movie movie on link film director\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 143 prefix-match hit, remaining 1125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    33 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     547.34 ms /  1158 tokens\n",
      "  7%|▋         | 65/900 [01:10<16:59,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to True\n",
      "Downgraded from place region on link death place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1173 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1318.89 ms /  1270 tokens\n",
      "  7%|▋         | 66/900 [01:12<20:06,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to person extending_to False\n",
      "Downgraded from work work on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     464.82 ms /  1148 tokens\n",
      "  7%|▋         | 67/900 [01:13<18:28,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to False\n",
      "Downgraded from agent agent on link home town\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     773.14 ms /  1181 tokens\n",
      "  8%|▊         | 68/900 [01:14<19:16,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to True\n",
      "Downgraded from place populated place on link death place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1169 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     442.10 ms /  1193 tokens\n",
      "  8%|▊         | 69/900 [01:15<18:03,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to True\n",
      "Downgraded from city city on link capital\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     340.33 ms /  1148 tokens\n",
      "  8%|▊         | 70/900 [01:16<16:44,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Organisation member extending_to False\n",
      "Downgraded from organisation organisation on link organisation member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     429.26 ms /  1184 tokens\n",
      "  8%|▊         | 71/900 [01:17<16:10,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from movie movie on link editing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1148 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     300.03 ms /  1161 tokens\n",
      "  8%|▊         | 72/900 [01:18<16:01,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to politician extending_to False\n",
      "Downgraded from political function political function on link politician\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1175 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.45 ms /  1197 tokens\n",
      "  8%|▊         | 73/900 [01:19<15:19,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to school extending_to True\n",
      "Downgraded from person person on link nobel laureates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1131 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     793.78 ms /  1184 tokens\n",
      "  8%|▊         | 74/900 [01:21<16:15,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 673 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   673 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     326.34 ms /   692 tokens\n",
      "  8%|▊         | 75/900 [01:22<14:22,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to True\n",
      "Downgraded from place populated place on link birth place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     368.60 ms /  1185 tokens\n",
      "  8%|▊         | 76/900 [01:23<14:16,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to True\n",
      "Downgraded from ethnic group ethnic group on link ethnic group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1276 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1276 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     350.35 ms /  1292 tokens\n",
      "  9%|▊         | 77/900 [01:24<14:06,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to river extending_to True\n",
      "Downgraded from mountain mountain on link mouth mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1143 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     416.29 ms /  1165 tokens\n",
      "  9%|▊         | 78/900 [01:24<13:21,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 664 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   664 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.57 ms /   675 tokens\n",
      "  9%|▉         | 79/900 [01:25<11:47,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to False\n",
      "Downgraded from sports event sports event on link champion in double female\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1158 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     458.31 ms /  1184 tokens\n",
      "  9%|▉         | 80/900 [01:26<12:52,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from athlete soccer player on link trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     427.09 ms /  1179 tokens\n",
      "  9%|▉         | 81/900 [01:27<13:10,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to village extending_to False\n",
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     823.70 ms /  1202 tokens\n",
      "  9%|▉         | 82/900 [01:29<15:28,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team hockey team on link manager club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     297.45 ms /  1173 tokens\n",
      "  9%|▉         | 83/900 [01:30<14:43,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place region on link federal state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1162 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1162 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     368.06 ms /  1180 tokens\n",
      "  9%|▉         | 84/900 [01:31<14:43,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to person extending_to False\n",
      "Downgraded from athlete soccer player on link trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1174 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     484.38 ms /  1202 tokens\n",
      "  9%|▉         | 85/900 [01:32<14:33,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to False\n",
      "Downgraded from sports event tournament on link champion in single male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     927.36 ms /  1231 tokens\n",
      " 10%|▉         | 86/900 [01:33<16:24,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 656 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   656 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     773.43 ms /   712 tokens\n",
      " 10%|▉         | 87/900 [01:34<16:15,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to book extending_to True\n",
      "Extending to soccer club extending_to True\n",
      "Downgraded from place settlement on link ground\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1152 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     372.83 ms /  1170 tokens\n",
      " 10%|▉         | 88/900 [01:35<15:24,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to musical work extending_to False\n",
      "Downgraded from military unit military unit on link march\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1177 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.32 ms /  1197 tokens\n",
      " 10%|▉         | 89/900 [01:36<14:38,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from career station career station on link career station\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     373.23 ms /  1178 tokens\n",
      " 10%|█         | 90/900 [01:37<13:57,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to person extending_to True\n",
      "Downgraded from country country on link nationality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1145 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     348.69 ms /  1162 tokens\n",
      " 10%|█         | 91/900 [01:38<13:23,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from country country on link nationality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1283 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     377.71 ms /    31 tokens\n",
      " 10%|█         | 92/900 [01:39<13:10,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team sports team on link former team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1155 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     425.14 ms /  1178 tokens\n",
      " 10%|█         | 93/900 [01:40<13:06,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     682.41 ms /  1190 tokens\n",
      " 10%|█         | 94/900 [01:41<14:24,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to movie extending_to False\n",
      "Downgraded from film festival film festival on link opening film\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     340.21 ms /  1215 tokens\n",
      " 11%|█         | 95/900 [01:42<13:59,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to False\n",
      "Extending to animal extending_to True\n",
      "Downgraded from place populated place on link death place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1169 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    32 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     541.39 ms /  1201 tokens\n",
      " 11%|█         | 96/900 [01:44<14:48,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place settlement on link frazioni\n",
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place populated place on link frazioni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1551 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1551 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    35 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     627.08 ms /  1586 tokens\n",
      " 11%|█         | 97/900 [01:45<15:49,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to company extending_to False\n",
      "Downgraded from record label record label on link distributing company\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1150 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.58 ms /  1171 tokens\n",
      " 11%|█         | 98/900 [01:46<14:33,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person function extending_to False\n",
      "Downgraded from organisation organisation on link leaderFunction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.55 ms /  1149 tokens\n",
      " 11%|█         | 99/900 [01:47<14:21,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to baseball player extending_to False\n",
      "Extending to baseball player extending_to False\n",
      "Extending to baseball player extending_to False\n",
      "Extending to baseball player extending_to True\n",
      "Extending to baseball player extending_to True\n",
      "Extending to baseball player extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 680 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   680 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     379.26 ms /   703 tokens\n",
      " 11%|█         | 100/900 [01:48<12:47,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to True\n",
      "Downgraded from city city on link capital\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1148 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     377.51 ms /  1167 tokens\n",
      " 11%|█         | 101/900 [01:49<13:11,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work work on link author\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     821.64 ms /  1168 tokens\n",
      " 11%|█▏        | 102/900 [01:50<15:11,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.85 ms /  1164 tokens\n",
      " 11%|█▏        | 103/900 [01:51<14:53,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link opening theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.84 ms /  1153 tokens\n",
      " 12%|█▏        | 104/900 [01:52<13:44,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from place place on link place of burial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    27 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     456.95 ms /  1134 tokens\n",
      " 12%|█▏        | 105/900 [01:53<13:59,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from country country on link state of origin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 196 prefix-match hit, remaining 1089 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1089 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     262.84 ms /  1100 tokens\n",
      " 12%|█▏        | 106/900 [01:54<12:52,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 682 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   682 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     608.16 ms /   726 tokens\n",
      " 12%|█▏        | 107/900 [01:55<12:59,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 758 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   758 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     939.76 ms /   829 tokens\n",
      " 12%|█▏        | 108/900 [01:56<14:21,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to True\n",
      "Downgraded from place place on link death place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1114 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     299.00 ms /  1128 tokens\n",
      " 12%|█▏        | 109/900 [01:57<13:23,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to musical work extending_to True\n",
      "Downgraded from agent organisation on link performer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1155 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     674.60 ms /  1200 tokens\n",
      " 12%|█▏        | 110/900 [01:58<14:32,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to Organisation member extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 674 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   674 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     234.62 ms /   687 tokens\n",
      " 12%|█▏        | 111/900 [01:59<12:43,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team sports team on link former team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1050.41 ms /  1181 tokens\n",
      " 12%|█▏        | 112/900 [02:01<15:18,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to False\n",
      "Downgraded from sports event sports event on link champion in single male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     376.51 ms /  1181 tokens\n",
      " 13%|█▎        | 113/900 [02:02<14:33,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work musical work on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    33 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     531.16 ms /  1193 tokens\n",
      " 13%|█▎        | 114/900 [02:03<14:27,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to winter sport Player extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 676 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   676 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     737.38 ms /   731 tokens\n",
      " 13%|█▎        | 115/900 [02:04<14:30,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 664 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   664 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     280.37 ms /   681 tokens\n",
      " 13%|█▎        | 116/900 [02:05<12:43,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work software on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     276.47 ms /  1141 tokens\n",
      " 13%|█▎        | 117/900 [02:06<12:38,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link ending theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     338.86 ms /  1150 tokens\n",
      " 13%|█▎        | 118/900 [02:06<11:50,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person function extending_to False\n",
      "Downgraded from person politician on link person function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.96 ms /  1164 tokens\n",
      " 13%|█▎        | 119/900 [02:07<12:34,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team sports team on link former team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1145 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.32 ms /  1166 tokens\n",
      " 13%|█▎        | 120/900 [02:08<12:14,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to basketball player extending_to True\n",
      "Extending to basketball player extending_to False\n",
      "Extending to basketball team extending_to False\n",
      "Extending to basketball player extending_to True\n",
      "Extending to basketball player extending_to True\n",
      "Extending to basketball team extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 678 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   678 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.98 ms /   690 tokens\n",
      " 13%|█▎        | 121/900 [02:09<10:35,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from movie movie on link film director\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1149 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     326.60 ms /  1165 tokens\n",
      " 14%|█▎        | 122/900 [02:10<10:38,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 664 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   664 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.99 ms /   675 tokens\n",
      " 14%|█▎        | 123/900 [02:10<09:46,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work album on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1141 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    36 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     566.48 ms /  1177 tokens\n",
      " 14%|█▍        | 124/900 [02:11<11:07,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 758 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   758 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.41 ms /   766 tokens\n",
      " 14%|█▍        | 125/900 [02:12<09:57,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Organisation member extending_to True\n",
      "Extending to Organisation member extending_to False\n",
      "Downgraded from organisation company on link organisation member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     507.71 ms /  1185 tokens\n",
      " 14%|█▍        | 126/900 [02:13<11:21,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.85 ms /  1189 tokens\n",
      " 14%|█▍        | 127/900 [02:14<11:33,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work movie on link auteur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1124 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.27 ms /  1145 tokens\n",
      " 14%|█▍        | 128/900 [02:15<11:48,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to route of transportation extending_to False\n",
      "Extending to route of transportation extending_to False\n",
      "Extending to station extending_to False\n",
      "Downgraded from route of transportation route of transportation on link route end\n",
      "Extending to station extending_to True\n",
      "Extending to route of transportation extending_to True\n",
      "Downgraded from station station on link route end\n",
      "Extending to station extending_to True\n",
      "Extending to station extending_to False\n",
      "Downgraded from route of transportation route of transportation on link route end\n",
      "Extending to station extending_to True\n",
      "Extending to station extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1741 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1741 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.36 ms /  1756 tokens\n",
      " 14%|█▍        | 129/900 [02:16<11:33,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to False\n",
      "Extending to animal extending_to True\n",
      "Downgraded from place place on link birth place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     327.39 ms /  1121 tokens\n",
      " 14%|█▍        | 130/900 [02:17<11:46,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from television show television show on link presenter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1177 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.80 ms /  1200 tokens\n",
      " 15%|█▍        | 131/900 [02:18<11:47,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to organisation extending_to False\n",
      "Downgraded from architectural structure infrastructure on link tenant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     339.70 ms /  1161 tokens\n",
      " 15%|█▍        | 132/900 [02:19<11:47,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to movie extending_to True\n",
      "Downgraded from person person on link editing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     601.22 ms /  1158 tokens\n",
      " 15%|█▍        | 133/900 [02:20<12:58,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link ending theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.88 ms /  1154 tokens\n",
      " 15%|█▍        | 134/900 [02:21<12:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to False\n",
      "Downgraded from agent sports club on link home town\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     509.67 ms /  1185 tokens\n",
      " 15%|█▌        | 135/900 [02:22<12:20,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to agent extending_to False\n",
      "Downgraded from musical work musical work on link performer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.43 ms /  1126 tokens\n",
      " 15%|█▌        | 136/900 [02:23<11:59,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from organisation organisation on link employer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1137 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     376.16 ms /  1157 tokens\n",
      " 15%|█▌        | 137/900 [02:24<11:56,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to mountain extending_to False\n",
      "Downgraded from river river on link mouth mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     363.10 ms /  1175 tokens\n",
      " 15%|█▌        | 138/900 [02:24<11:03,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to soccer player extending_to True\n",
      "Downgraded from sports team sports team on link trainer club\n",
      "Extending to soccer player extending_to True\n",
      "Downgraded from sports team sports team on link trainer club\n",
      "Extending to soccer player extending_to True\n",
      "Downgraded from sports team hockey team on link trainer club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1902 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1902 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.03 ms /  1916 tokens\n",
      " 15%|█▌        | 139/900 [02:25<11:22,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Sports team member extending_to False\n",
      "Extending to Sports team member extending_to True\n",
      "Extending to Sports team member extending_to True\n",
      "Extending to Sports team member extending_to False\n",
      "Extending to Sports team member extending_to False\n",
      "Extending to Sports team member extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 684 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   684 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     248.70 ms /   698 tokens\n",
      " 16%|█▌        | 140/900 [02:26<10:31,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to False\n",
      "Downgraded from sports event sports event on link champion in double female\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     423.21 ms /  1188 tokens\n",
      " 16%|█▌        | 141/900 [02:27<10:36,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to river extending_to False\n",
      "Downgraded from body of water river on link outflow\n",
      "Extending to river extending_to False\n",
      "Downgraded from place place on link river\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1443 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1443 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     743.96 ms /  1491 tokens\n",
      " 16%|█▌        | 142/900 [02:28<13:16,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to False\n",
      "Downgraded from agent organisation on link home town\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     698.68 ms /  1207 tokens\n",
      " 16%|█▌        | 143/900 [02:29<13:50,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to True\n",
      "Downgraded from city city on link capital\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1145 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.97 ms /  1168 tokens\n",
      " 16%|█▌        | 144/900 [02:30<12:56,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 671 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   671 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     375.58 ms /   696 tokens\n",
      " 16%|█▌        | 145/900 [02:31<12:01,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to False\n",
      "Extending to populated place extending_to False\n",
      "Downgraded from place park on link nearest city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1152 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     342.81 ms /  1168 tokens\n",
      " 16%|█▌        | 146/900 [02:32<12:05,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from work book on link author\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     326.70 ms /  1152 tokens\n",
      " 16%|█▋        | 147/900 [02:33<11:57,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to place extending_to False\n",
      "Downgraded from animal person on link death place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1093 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1093 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    43 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     645.69 ms /  1136 tokens\n",
      " 16%|█▋        | 148/900 [02:34<12:59,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to True\n",
      "Downgraded from place village on link birth place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     362.23 ms /  1154 tokens\n",
      " 17%|█▋        | 149/900 [02:35<12:36,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to musical artist extending_to False\n",
      "Downgraded from work work on link music composer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     338.17 ms /  1152 tokens\n",
      " 17%|█▋        | 150/900 [02:36<11:38,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to automobile engine extending_to True\n",
      "Extending to automobile engine extending_to True\n",
      "Extending to automobile engine extending_to False\n",
      "Extending to automobile engine extending_to False\n",
      "Extending to automobile engine extending_to True\n",
      "Extending to automobile extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 694 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   694 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     270.11 ms /   710 tokens\n",
      " 17%|█▋        | 151/900 [02:36<10:03,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link ending theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1287 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     318.13 ms /  1301 tokens\n",
      " 17%|█▋        | 152/900 [02:37<09:38,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to False\n",
      "Downgraded from organisation group on link headquarter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    38 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     591.61 ms /  1225 tokens\n",
      " 17%|█▋        | 153/900 [02:38<10:43,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team cycling team on link manager club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.03 ms /  1183 tokens\n",
      " 17%|█▋        | 154/900 [02:39<10:51,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to politician extending_to True\n",
      "Downgraded from person Organisation member on link prefect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.38 ms /  1205 tokens\n",
      " 17%|█▋        | 155/900 [02:40<10:59,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to agent extending_to False\n",
      "Downgraded from musical work single on link performer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     346.83 ms /  1120 tokens\n",
      " 17%|█▋        | 156/900 [02:41<10:54,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from career station career station on link career station\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     352.68 ms /  1156 tokens\n",
      " 17%|█▋        | 157/900 [02:42<11:14,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to True\n",
      "Downgraded from career station career station on link career station\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1168 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     339.94 ms /  1185 tokens\n",
      " 18%|█▊        | 158/900 [02:43<11:07,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.44 ms /  1189 tokens\n",
      " 18%|█▊        | 159/900 [02:44<11:11,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to settlement extending_to True\n",
      "Downgraded from populated place settlement on link federal state\n",
      "Extending to settlement extending_to False\n",
      "Downgraded from legislature legislature on link meeting city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1517 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1517 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     486.84 ms /  1543 tokens\n",
      " 18%|█▊        | 160/900 [02:45<11:39,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to False\n",
      "Downgraded from organisation organisation on link headquarter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1159 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     615.38 ms /  1199 tokens\n",
      " 18%|█▊        | 161/900 [02:46<12:19,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from Band Band on link former band member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1275 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.27 ms /  1300 tokens\n",
      " 18%|█▊        | 162/900 [02:47<12:10,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Organisation member extending_to False\n",
      "Downgraded from organisation Band on link organisation member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     510.11 ms /  1289 tokens\n",
      " 18%|█▊        | 163/900 [02:48<12:51,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team basketball team on link former team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1159 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     366.68 ms /  1178 tokens\n",
      " 18%|█▊        | 164/900 [02:49<12:13,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1162 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1162 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     773.18 ms /  1215 tokens\n",
      " 18%|█▊        | 165/900 [02:50<13:37,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to military unit extending_to False\n",
      "Downgraded from military person military person on link military unit\n",
      "Extending to military unit extending_to False\n",
      "Downgraded from military person military person on link military unit\n",
      "Extending to military unit extending_to True\n",
      "Downgraded from person Organisation member on link notable commander\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1911 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1911 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     578.21 ms /  1940 tokens\n",
      " 18%|█▊        | 166/900 [02:51<14:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to musical artist extending_to True\n",
      "Downgraded from Band Band on link Music Band\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1407 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1407 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     379.44 ms /  1425 tokens\n",
      " 19%|█▊        | 167/900 [02:52<13:23,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to agent extending_to False\n",
      "Downgraded from musical work song on link performer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     378.79 ms /  1205 tokens\n",
      " 19%|█▊        | 168/900 [02:53<12:33,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to person extending_to False\n",
      "Downgraded from sports team sports team on link current member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     612.39 ms /  1186 tokens\n",
      " 19%|█▉        | 169/900 [02:55<13:10,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to winter sport Player extending_to False\n",
      "Extending to winter sport Player extending_to False\n",
      "Extending to winter sport Player extending_to False\n",
      "Extending to winter sport Player extending_to True\n",
      "Extending to winter sport Player extending_to True\n",
      "Extending to winter sport Player extending_to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 678 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   678 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     317.98 ms /   698 tokens\n",
      " 19%|█▉        | 170/900 [02:55<11:25,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to person extending_to True\n",
      "Downgraded from country country on link state of origin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     470.33 ms /  1174 tokens\n",
      " 19%|█▉        | 171/900 [02:56<11:37,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to company extending_to True\n",
      "Extending to song extending_to True\n",
      "Extending to company extending_to False\n",
      "Downgraded from work work on link production company\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     481.12 ms /  1135 tokens\n",
      " 19%|█▉        | 172/900 [02:57<11:38,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to person extending_to False\n",
      "Downgraded from movie movie on link film director\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1149 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     434.71 ms /  1174 tokens\n",
      " 19%|█▉        | 173/900 [02:58<11:35,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 664 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   664 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     339.90 ms /   686 tokens\n",
      " 19%|█▉        | 174/900 [02:59<10:40,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to song extending_to False\n",
      "Extending to song extending_to False\n",
      "Extending to song extending_to True\n",
      "Extending to song extending_to False\n",
      "Extending to song extending_to True\n",
      "Extending to song extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 670 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   670 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    36 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.29 ms /   706 tokens\n",
      " 19%|█▉        | 175/900 [03:00<10:17,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to work extending_to False\n",
      "Downgraded from television show television show on link opening theme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     433.51 ms /  1164 tokens\n",
      " 20%|█▉        | 176/900 [03:01<10:40,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to True\n",
      "Downgraded from ethnic group ethnic group on link ethnic group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     468.66 ms /  1168 tokens\n",
      " 20%|█▉        | 177/900 [03:02<11:24,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to athlete extending_to False\n",
      "Downgraded from sports event tournament on link champion in single male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.68 ms /  1163 tokens\n",
      " 20%|█▉        | 178/900 [03:03<11:25,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to musical artist extending_to True\n",
      "Downgraded from Band Band on link Music Band\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1313 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     761.90 ms /  1363 tokens\n",
      " 20%|█▉        | 179/900 [03:04<12:34,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to village extending_to False\n",
      "Extending to ethnic group extending_to False\n",
      "Downgraded from populated place settlement on link ethnic group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     483.62 ms /  1175 tokens\n",
      " 20%|██        | 180/900 [03:05<11:58,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to animal extending_to False\n",
      "Extending to settlement extending_to False\n",
      "Downgraded from agent agent on link home town\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     765.95 ms /  1168 tokens\n",
      " 20%|██        | 181/900 [03:06<13:30,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to Organisation member extending_to True\n",
      "Extending to sports team extending_to False\n",
      "Downgraded from athlete athlete on link former team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1155 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     460.82 ms /  1181 tokens\n",
      " 20%|██        | 182/900 [03:07<12:55,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to politician extending_to True\n",
      "Downgraded from person athlete on link prefect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1148 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     353.91 ms /  1165 tokens\n",
      " 20%|██        | 183/900 [03:08<12:33,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to military person extending_to False\n",
      "Extending to military person extending_to True\n",
      "Downgraded from military unit military unit on link military unit\n",
      "Extending to military person extending_to False\n",
      "Extending to military person extending_to False\n",
      "Extending to military person extending_to False\n",
      "Extending to military unit extending_to True\n",
      "Downgraded from mean of transportation mean of transportation on link aircraft bomber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1518 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1518 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     471.98 ms /  1541 tokens\n",
      " 20%|██        | 184/900 [03:09<12:39,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to athlete extending_to True\n",
      "Downgraded from sports team cycling team on link former team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     514.68 ms /  1189 tokens\n",
      " 21%|██        | 185/900 [03:10<13:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to True\n",
      "Extending to career station extending_to False\n",
      "Extending to career station extending_to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 198 prefix-match hit, remaining 598 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   598 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.76 ms /   608 tokens\n",
      " 21%|██        | 186/900 [03:11<11:39,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to populated place extending_to False\n",
      "Downgraded from organisation company on link headquarter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    21 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     426.73 ms /  1174 tokens\n",
      " 21%|██        | 187/900 [03:13<13:35,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to musical work extending_to False\n",
      "Downgraded from military unit military unit on link march\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.56 ms /  1170 tokens\n",
      " 21%|██        | 188/900 [03:14<14:11,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending to office holder extending_to True\n",
      "Downgraded from political party political party on link other party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 139 prefix-match hit, remaining 1164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1043.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     510.72 ms /  1189 tokens\n",
      " 21%|██        | 189/900 [03:15<14:51,  1.25s/it]"
     ]
    }
   ],
   "source": [
    "n_examples = 300\n",
    "n_nodes = [3, 5, 10]\n",
    "resulting_examples = []\n",
    "progress = tqdm(total=n_examples * len(n_nodes))\n",
    "for n_node in n_nodes:\n",
    "    for i in range(n_examples):\n",
    "        \n",
    "        erl = choose_graph(seed=i, max_nodes=n_node)\n",
    "        response = llama_model.create_chat_completion(\n",
    "            # grammar=self.grammar_erl,\n",
    "            messages=messages\n",
    "            + [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": erl.model_dump_json(),\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=-1,\n",
    "            temperature=0.7,  # get wild :)\n",
    "        )\n",
    "        progress.update(1)\n",
    "        resulting_examples.append(\n",
    "            {\n",
    "                \"erl\": erl.model_dump_json(),\n",
    "                \"response\": response[\"choices\"][0][\"message\"][\"content\"],\n",
    "                \"n_nodes\": n_node,\n",
    "                \"seed\": i,\n",
    "            }\n",
    "        )\n",
    "        topic_man.engine.dispose()\n",
    "        \n",
    "resulting_examples_df = pd.DataFrame(resulting_examples)\n",
    "resulting_examples_df.to_csv(\"llama_examples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
