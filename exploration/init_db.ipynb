{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../backend\"))\n",
    "sys.path.append(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup for  BTO\n",
      "-- Loading and merging datasets\n",
      "0 datasets loaded\n"
     ]
    }
   ],
   "source": [
    "from rdflib.plugins.stores.sparqlstore import SPARQLStore\n",
    "from backend.ontology import Graph, OntologyManager, OntologyConfig\n",
    "from backend.datasetmatcher import DatasetManager\n",
    "from backend.explorative.explorative_support import GuidanceManager\n",
    "from backend.explorative.llm_query import LLMQuery\n",
    "from backend.explorative.topic_init import TopicInitator\n",
    "\n",
    "from backend.eval_config import (\n",
    "    DBPEDIA_CONFIGS,\n",
    "    OMA_CONFIGS,\n",
    "    UNIPROT_CONFIGS,\n",
    "    BTO_CONFIGS,\n",
    "    DNB_CONFIGS,\n",
    "    BTO_CONFIGS,\n",
    "    EvalConfig,\n",
    ")\n",
    "base_path = \"../data\"\n",
    "onto_path = f\"{base_path}/hero-ontology/hereditary_clinical.ttl\"\n",
    "\n",
    "\n",
    "# graph = Graph().parse(onto_path, format=\"turtle\")\n",
    "# graph.bind(\"bto\", \"http://www.semanticweb.org/ontologies/2020/3/bto#\")\n",
    "\n",
    "# store = SPARQLStore(\n",
    "#     \"http://localhost:7200/repositories/dpedia\",\n",
    "#     method=\"POST_FORM\",\n",
    "#     params={\"infer\": False, \"sameAs\": False},\n",
    "# )\n",
    "\n",
    "# store = SPARQLStore(\n",
    "#     \"http://localhost:3030/dbpedia/query\",\n",
    "#     method=\"POST_FORM\",\n",
    "#     params={\"infer\": False, \"sameAs\": False},\n",
    "# )\n",
    "setup: EvalConfig = BTO_CONFIGS[-1]\n",
    "print(\"Setup for \", setup.name)\n",
    "store = SPARQLStore(\n",
    "    setup.sparql_endpoint,\n",
    "    method=\"POST_FORM\",\n",
    "    params={\"infer\": False, \"sameAs\": False},\n",
    "    retries=10,\n",
    ")\n",
    "graph = Graph(store=store)\n",
    "\n",
    "config = OntologyConfig()\n",
    "\n",
    "ontology_manager = OntologyManager(config, graph)\n",
    "dataset_manager = DatasetManager(ontology_manager)\n",
    "dataset_manager.initialise(glob_path=\"data/datasets/ALS/**/*.csv\")\n",
    "\n",
    "# ontology_manager.load_full_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53b0ad130b816dae48f9b2782cf10506b88a27a95fd38e0fdfafabe84dfb916a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "guidance_man = GuidanceManager(\n",
    "    ontology_manager, conn_str=setup.conn_str, llm_model_id=setup.model_id\n",
    ")\n",
    "print(guidance_man.identifier)\n",
    "llm_query = LLMQuery(guidance_man)\n",
    "topic_init = TopicInitator(guidance_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving anonymous properties: 0it [00:00, ?it/s]\n",
      "Enriching bn1929:  27%|██▋       | 139/511 [00:19<00:07, 50.25it/s]                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn36\n",
      "Skipping _:bn36\n",
      "Skipping _:bn36\n",
      "Skipping _:bn128\n",
      "Skipping _:bn128\n",
      "Skipping _:bn128\n",
      "Skipping _:bn158\n",
      "Skipping _:bn158\n",
      "Skipping _:bn158\n",
      "Skipping _:bn181\n",
      "Skipping _:bn181\n",
      "Skipping _:bn181\n",
      "Skipping _:bn207\n",
      "Skipping _:bn207\n",
      "Skipping _:bn207\n",
      "Skipping _:bn233\n",
      "Skipping _:bn233\n",
      "Skipping _:bn233\n",
      "Skipping _:bn249\n",
      "Skipping _:bn249\n",
      "Skipping _:bn249\n",
      "Skipping _:bn263\n",
      "Skipping _:bn263\n",
      "Skipping _:bn263\n",
      "Skipping _:bn313\n",
      "Skipping _:bn313\n",
      "Skipping _:bn313\n",
      "Skipping _:bn512\n",
      "Skipping _:bn512\n",
      "Skipping _:bn512\n",
      "Skipping _:bn531\n",
      "Skipping _:bn531\n",
      "Skipping _:bn531\n",
      "Skipping _:bn664\n",
      "Skipping _:bn664\n",
      "Skipping _:bn664\n",
      "Skipping _:bn665\n",
      "Skipping _:bn665\n",
      "Skipping _:bn665\n",
      "Skipping _:bn669\n",
      "Skipping _:bn669\n",
      "Skipping _:bn669\n",
      "Skipping _:bn676\n",
      "Skipping _:bn676\n",
      "Skipping _:bn676\n",
      "Skipping _:bn740\n",
      "Skipping _:bn740\n",
      "Skipping _:bn740\n",
      "Skipping _:bn747\n",
      "Skipping _:bn747\n",
      "Skipping _:bn747\n",
      "Skipping _:bn886\n",
      "Skipping _:bn886\n",
      "Skipping _:bn886\n",
      "Skipping _:bn930\n",
      "Skipping _:bn930\n",
      "Skipping _:bn930\n",
      "Skipping _:bn965\n",
      "Skipping _:bn965\n",
      "Skipping _:bn965\n",
      "Skipping _:bn1135\n",
      "Skipping _:bn1135\n",
      "Skipping _:bn1135\n",
      "Skipping _:bn1155\n",
      "Skipping _:bn1155\n",
      "Skipping _:bn1155\n",
      "Skipping _:bn1158\n",
      "Skipping _:bn1158\n",
      "Skipping _:bn1158\n",
      "Skipping _:bn1194\n",
      "Skipping _:bn1194\n",
      "Skipping _:bn1194\n",
      "Skipping _:bn1209\n",
      "Skipping _:bn1209\n",
      "Skipping _:bn1209\n",
      "Skipping _:bn1250\n",
      "Skipping _:bn1250\n",
      "Skipping _:bn1250\n",
      "Skipping _:bn1371\n",
      "Skipping _:bn1371\n",
      "Skipping _:bn1371\n",
      "Skipping _:bn1461\n",
      "Skipping _:bn1461\n",
      "Skipping _:bn1461\n",
      "Skipping _:bn1485\n",
      "Skipping _:bn1485\n",
      "Skipping _:bn1485\n",
      "Skipping _:bn1522\n",
      "Skipping _:bn1522\n",
      "Skipping _:bn1522\n",
      "Skipping _:bn1637\n",
      "Skipping _:bn1637\n",
      "Skipping _:bn1637\n",
      "Skipping _:bn1673\n",
      "Skipping _:bn1673\n",
      "Skipping _:bn1673\n",
      "Skipping _:bn1696\n",
      "Skipping _:bn1696\n",
      "Skipping _:bn1696\n",
      "Skipping _:bn1871\n",
      "Skipping _:bn1871\n",
      "Skipping _:bn1871\n",
      "Skipping _:bn1900\n",
      "Skipping _:bn1900\n",
      "Skipping _:bn1900\n",
      "Skipping _:bn1901\n",
      "Skipping _:bn1901\n",
      "Skipping _:bn1901\n",
      "Skipping _:bn1905\n",
      "Skipping _:bn1905\n",
      "Skipping _:bn1905\n",
      "Skipping _:bn1929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn3176:  34%|███▎      | 172/511 [00:19<00:03, 93.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn1929\n",
      "Skipping _:bn1929\n",
      "Skipping _:bn1966\n",
      "Skipping _:bn1966\n",
      "Skipping _:bn1966\n",
      "Skipping _:bn2070\n",
      "Skipping _:bn2070\n",
      "Skipping _:bn2070\n",
      "Skipping _:bn2075\n",
      "Skipping _:bn2075\n",
      "Skipping _:bn2075\n",
      "Skipping _:bn2122\n",
      "Skipping _:bn2122\n",
      "Skipping _:bn2122\n",
      "Skipping _:bn2186\n",
      "Skipping _:bn2186\n",
      "Skipping _:bn2186\n",
      "Skipping _:bn2187\n",
      "Skipping _:bn2187\n",
      "Skipping _:bn2187\n",
      "Skipping _:bn2208\n",
      "Skipping _:bn2208\n",
      "Skipping _:bn2208\n",
      "Skipping _:bn2240\n",
      "Skipping _:bn2240\n",
      "Skipping _:bn2240\n",
      "Skipping _:bn2251\n",
      "Skipping _:bn2251\n",
      "Skipping _:bn2251\n",
      "Skipping _:bn2254\n",
      "Skipping _:bn2254\n",
      "Skipping _:bn2254\n",
      "Skipping _:bn2277\n",
      "Skipping _:bn2277\n",
      "Skipping _:bn2277\n",
      "Skipping _:bn2289\n",
      "Skipping _:bn2289\n",
      "Skipping _:bn2289\n",
      "Skipping _:bn2328\n",
      "Skipping _:bn2328\n",
      "Skipping _:bn2328\n",
      "Skipping _:bn2353\n",
      "Skipping _:bn2353\n",
      "Skipping _:bn2353\n",
      "Skipping _:bn2378\n",
      "Skipping _:bn2378\n",
      "Skipping _:bn2378\n",
      "Skipping _:bn2392\n",
      "Skipping _:bn2392\n",
      "Skipping _:bn2392\n",
      "Skipping _:bn2401\n",
      "Skipping _:bn2401\n",
      "Skipping _:bn2401\n",
      "Skipping _:bn2407\n",
      "Skipping _:bn2407\n",
      "Skipping _:bn2407\n",
      "Skipping _:bn2452\n",
      "Skipping _:bn2452\n",
      "Skipping _:bn2452\n",
      "Skipping _:bn2523\n",
      "Skipping _:bn2523\n",
      "Skipping _:bn2523\n",
      "Skipping _:bn2656\n",
      "Skipping _:bn2656\n",
      "Skipping _:bn2656\n",
      "Skipping _:bn2704\n",
      "Skipping _:bn2704\n",
      "Skipping _:bn2704\n",
      "Skipping _:bn2771\n",
      "Skipping _:bn2771\n",
      "Skipping _:bn2771\n",
      "Skipping _:bn2891\n",
      "Skipping _:bn2891\n",
      "Skipping _:bn2891\n",
      "Skipping _:bn2938\n",
      "Skipping _:bn2938\n",
      "Skipping _:bn2938\n",
      "Skipping _:bn2949\n",
      "Skipping _:bn2949\n",
      "Skipping _:bn2949\n",
      "Skipping _:bn2963\n",
      "Skipping _:bn2963\n",
      "Skipping _:bn2963\n",
      "Skipping _:bn2998\n",
      "Skipping _:bn2998\n",
      "Skipping _:bn2998\n",
      "Skipping _:bn3027\n",
      "Skipping _:bn3027\n",
      "Skipping _:bn3027\n",
      "Skipping _:bn3028\n",
      "Skipping _:bn3028\n",
      "Skipping _:bn3028\n",
      "Skipping _:bn3142\n",
      "Skipping _:bn3142\n",
      "Skipping _:bn3142\n",
      "Skipping _:bn3153\n",
      "Skipping _:bn3153\n",
      "Skipping _:bn3153\n",
      "Skipping _:bn3166\n",
      "Skipping _:bn3166\n",
      "Skipping _:bn3166\n",
      "Skipping _:bn3176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn5304:  41%|████▏     | 211/511 [00:19<00:02, 137.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn3176\n",
      "Skipping _:bn3176\n",
      "Skipping _:bn3185\n",
      "Skipping _:bn3185\n",
      "Skipping _:bn3185\n",
      "Skipping _:bn3283\n",
      "Skipping _:bn3283\n",
      "Skipping _:bn3283\n",
      "Skipping _:bn3321\n",
      "Skipping _:bn3321\n",
      "Skipping _:bn3321\n",
      "Skipping _:bn3345\n",
      "Skipping _:bn3345\n",
      "Skipping _:bn3345\n",
      "Skipping _:bn3460\n",
      "Skipping _:bn3460\n",
      "Skipping _:bn3460\n",
      "Skipping _:bn3476\n",
      "Skipping _:bn3476\n",
      "Skipping _:bn3476\n",
      "Skipping _:bn3576\n",
      "Skipping _:bn3576\n",
      "Skipping _:bn3576\n",
      "Skipping _:bn3658\n",
      "Skipping _:bn3658\n",
      "Skipping _:bn3658\n",
      "Skipping _:bn3790\n",
      "Skipping _:bn3790\n",
      "Skipping _:bn3790\n",
      "Skipping _:bn3871\n",
      "Skipping _:bn3871\n",
      "Skipping _:bn3871\n",
      "Skipping _:bn3977\n",
      "Skipping _:bn3977\n",
      "Skipping _:bn3977\n",
      "Skipping _:bn4060\n",
      "Skipping _:bn4060\n",
      "Skipping _:bn4060\n",
      "Skipping _:bn4077\n",
      "Skipping _:bn4077\n",
      "Skipping _:bn4077\n",
      "Skipping _:bn4096\n",
      "Skipping _:bn4096\n",
      "Skipping _:bn4096\n",
      "Skipping _:bn4111\n",
      "Skipping _:bn4111\n",
      "Skipping _:bn4111\n",
      "Skipping _:bn4154\n",
      "Skipping _:bn4154\n",
      "Skipping _:bn4154\n",
      "Skipping _:bn4203\n",
      "Skipping _:bn4203\n",
      "Skipping _:bn4203\n",
      "Skipping _:bn4557\n",
      "Skipping _:bn4557\n",
      "Skipping _:bn4557\n",
      "Skipping _:bn4635\n",
      "Skipping _:bn4635\n",
      "Skipping _:bn4635\n",
      "Skipping _:bn4688\n",
      "Skipping _:bn4688\n",
      "Skipping _:bn4688\n",
      "Skipping _:bn4736\n",
      "Skipping _:bn4736\n",
      "Skipping _:bn4736\n",
      "Skipping _:bn4749\n",
      "Skipping _:bn4749\n",
      "Skipping _:bn4749\n",
      "Skipping _:bn4842\n",
      "Skipping _:bn4842\n",
      "Skipping _:bn4842\n",
      "Skipping _:bn4846\n",
      "Skipping _:bn4846\n",
      "Skipping _:bn4846\n",
      "Skipping _:bn4885\n",
      "Skipping _:bn4885\n",
      "Skipping _:bn4885\n",
      "Skipping _:bn4900\n",
      "Skipping _:bn4900\n",
      "Skipping _:bn4900\n",
      "Skipping _:bn4927\n",
      "Skipping _:bn4927\n",
      "Skipping _:bn4927\n",
      "Skipping _:bn4947\n",
      "Skipping _:bn4947\n",
      "Skipping _:bn4947\n",
      "Skipping _:bn4974\n",
      "Skipping _:bn4974\n",
      "Skipping _:bn4974\n",
      "Skipping _:bn5034\n",
      "Skipping _:bn5034\n",
      "Skipping _:bn5034\n",
      "Skipping _:bn5060\n",
      "Skipping _:bn5060\n",
      "Skipping _:bn5060\n",
      "Skipping _:bn5070\n",
      "Skipping _:bn5070\n",
      "Skipping _:bn5070\n",
      "Skipping _:bn5082\n",
      "Skipping _:bn5082\n",
      "Skipping _:bn5082\n",
      "Skipping _:bn5083\n",
      "Skipping _:bn5083\n",
      "Skipping _:bn5083\n",
      "Skipping _:bn5136\n",
      "Skipping _:bn5136\n",
      "Skipping _:bn5136\n",
      "Skipping _:bn5155\n",
      "Skipping _:bn5155\n",
      "Skipping _:bn5155\n",
      "Skipping _:bn5220\n",
      "Skipping _:bn5220\n",
      "Skipping _:bn5220\n",
      "Skipping _:bn5303\n",
      "Skipping _:bn5303\n",
      "Skipping _:bn5303\n",
      "Skipping _:bn5304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn6988:  49%|████▊     | 248/511 [00:19<00:01, 148.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn5304\n",
      "Skipping _:bn5304\n",
      "Skipping _:bn5408\n",
      "Skipping _:bn5408\n",
      "Skipping _:bn5408\n",
      "Skipping _:bn5456\n",
      "Skipping _:bn5456\n",
      "Skipping _:bn5456\n",
      "Skipping _:bn5500\n",
      "Skipping _:bn5500\n",
      "Skipping _:bn5500\n",
      "Skipping _:bn5605\n",
      "Skipping _:bn5605\n",
      "Skipping _:bn5605\n",
      "Skipping _:bn5764\n",
      "Skipping _:bn5764\n",
      "Skipping _:bn5764\n",
      "Skipping _:bn5765\n",
      "Skipping _:bn5765\n",
      "Skipping _:bn5765\n",
      "Skipping _:bn5906\n",
      "Skipping _:bn5906\n",
      "Skipping _:bn5906\n",
      "Skipping _:bn5915\n",
      "Skipping _:bn5915\n",
      "Skipping _:bn5915\n",
      "Skipping _:bn5941\n",
      "Skipping _:bn5941\n",
      "Skipping _:bn5941\n",
      "Skipping _:bn6038\n",
      "Skipping _:bn6038\n",
      "Skipping _:bn6038\n",
      "Skipping _:bn6134\n",
      "Skipping _:bn6134\n",
      "Skipping _:bn6134\n",
      "Skipping _:bn6181\n",
      "Skipping _:bn6181\n",
      "Skipping _:bn6181\n",
      "Skipping _:bn6184\n",
      "Skipping _:bn6184\n",
      "Skipping _:bn6184\n",
      "Skipping _:bn6188\n",
      "Skipping _:bn6188\n",
      "Skipping _:bn6188\n",
      "Skipping _:bn6250\n",
      "Skipping _:bn6250\n",
      "Skipping _:bn6250\n",
      "Skipping _:bn6257\n",
      "Skipping _:bn6257\n",
      "Skipping _:bn6257\n",
      "Skipping _:bn6462\n",
      "Skipping _:bn6462\n",
      "Skipping _:bn6462\n",
      "Skipping _:bn6493\n",
      "Skipping _:bn6493\n",
      "Skipping _:bn6493\n",
      "Skipping _:bn6507\n",
      "Skipping _:bn6507\n",
      "Skipping _:bn6507\n",
      "Skipping _:bn6565\n",
      "Skipping _:bn6565\n",
      "Skipping _:bn6565\n",
      "Skipping _:bn6610\n",
      "Skipping _:bn6610\n",
      "Skipping _:bn6610\n",
      "Skipping _:bn6649\n",
      "Skipping _:bn6649\n",
      "Skipping _:bn6649\n",
      "Skipping _:bn6653\n",
      "Skipping _:bn6653\n",
      "Skipping _:bn6653\n",
      "Skipping _:bn6728\n",
      "Skipping _:bn6728\n",
      "Skipping _:bn6728\n",
      "Skipping _:bn6754\n",
      "Skipping _:bn6754\n",
      "Skipping _:bn6754\n",
      "Skipping _:bn6837\n",
      "Skipping _:bn6837\n",
      "Skipping _:bn6837\n",
      "Skipping _:bn6872\n",
      "Skipping _:bn6872\n",
      "Skipping _:bn6872\n",
      "Skipping _:bn6943\n",
      "Skipping _:bn6943\n",
      "Skipping _:bn6943\n",
      "Skipping _:bn6964\n",
      "Skipping _:bn6964\n",
      "Skipping _:bn6964\n",
      "Skipping _:bn6973\n",
      "Skipping _:bn6973\n",
      "Skipping _:bn6973\n",
      "Skipping _:bn6988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn8702:  56%|█████▌    | 286/511 [00:19<00:01, 165.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn6988\n",
      "Skipping _:bn6988\n",
      "Skipping _:bn7022\n",
      "Skipping _:bn7022\n",
      "Skipping _:bn7022\n",
      "Skipping _:bn7305\n",
      "Skipping _:bn7305\n",
      "Skipping _:bn7305\n",
      "Skipping _:bn7368\n",
      "Skipping _:bn7368\n",
      "Skipping _:bn7368\n",
      "Skipping _:bn7373\n",
      "Skipping _:bn7373\n",
      "Skipping _:bn7373\n",
      "Skipping _:bn7471\n",
      "Skipping _:bn7471\n",
      "Skipping _:bn7471\n",
      "Skipping _:bn7499\n",
      "Skipping _:bn7499\n",
      "Skipping _:bn7499\n",
      "Skipping _:bn7501\n",
      "Skipping _:bn7501\n",
      "Skipping _:bn7501\n",
      "Skipping _:bn7545\n",
      "Skipping _:bn7545\n",
      "Skipping _:bn7545\n",
      "Skipping _:bn7570\n",
      "Skipping _:bn7570\n",
      "Skipping _:bn7570\n",
      "Skipping _:bn7699\n",
      "Skipping _:bn7699\n",
      "Skipping _:bn7699\n",
      "Skipping _:bn7722\n",
      "Skipping _:bn7722\n",
      "Skipping _:bn7722\n",
      "Skipping _:bn7730\n",
      "Skipping _:bn7730\n",
      "Skipping _:bn7730\n",
      "Skipping _:bn7764\n",
      "Skipping _:bn7764\n",
      "Skipping _:bn7764\n",
      "Skipping _:bn7803\n",
      "Skipping _:bn7803\n",
      "Skipping _:bn7803\n",
      "Skipping _:bn7812\n",
      "Skipping _:bn7812\n",
      "Skipping _:bn7812\n",
      "Skipping _:bn7861\n",
      "Skipping _:bn7861\n",
      "Skipping _:bn7861\n",
      "Skipping _:bn7871\n",
      "Skipping _:bn7871\n",
      "Skipping _:bn7871\n",
      "Skipping _:bn7895\n",
      "Skipping _:bn7895\n",
      "Skipping _:bn7895\n",
      "Skipping _:bn7901\n",
      "Skipping _:bn7901\n",
      "Skipping _:bn7901\n",
      "Skipping _:bn7909\n",
      "Skipping _:bn7909\n",
      "Skipping _:bn7909\n",
      "Skipping _:bn7947\n",
      "Skipping _:bn7947\n",
      "Skipping _:bn7947\n",
      "Skipping _:bn8020\n",
      "Skipping _:bn8020\n",
      "Skipping _:bn8020\n",
      "Skipping _:bn8044\n",
      "Skipping _:bn8044\n",
      "Skipping _:bn8044\n",
      "Skipping _:bn8055\n",
      "Skipping _:bn8055\n",
      "Skipping _:bn8055\n",
      "Skipping _:bn8128\n",
      "Skipping _:bn8128\n",
      "Skipping _:bn8128\n",
      "Skipping _:bn8163\n",
      "Skipping _:bn8163\n",
      "Skipping _:bn8163\n",
      "Skipping _:bn8183\n",
      "Skipping _:bn8183\n",
      "Skipping _:bn8183\n",
      "Skipping _:bn8236\n",
      "Skipping _:bn8236\n",
      "Skipping _:bn8236\n",
      "Skipping _:bn8284\n",
      "Skipping _:bn8284\n",
      "Skipping _:bn8284\n",
      "Skipping _:bn8311\n",
      "Skipping _:bn8311\n",
      "Skipping _:bn8311\n",
      "Skipping _:bn8386\n",
      "Skipping _:bn8386\n",
      "Skipping _:bn8386\n",
      "Skipping _:bn8395\n",
      "Skipping _:bn8395\n",
      "Skipping _:bn8395\n",
      "Skipping _:bn8397\n",
      "Skipping _:bn8397\n",
      "Skipping _:bn8397\n",
      "Skipping _:bn8399\n",
      "Skipping _:bn8399\n",
      "Skipping _:bn8399\n",
      "Skipping _:bn8402\n",
      "Skipping _:bn8402\n",
      "Skipping _:bn8402\n",
      "Skipping _:bn8434\n",
      "Skipping _:bn8434\n",
      "Skipping _:bn8434\n",
      "Skipping _:bn8608\n",
      "Skipping _:bn8608\n",
      "Skipping _:bn8608\n",
      "Skipping _:bn8702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn10491:  63%|██████▎   | 324/511 [00:20<00:01, 163.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn8702\n",
      "Skipping _:bn8702\n",
      "Skipping _:bn8760\n",
      "Skipping _:bn8760\n",
      "Skipping _:bn8760\n",
      "Skipping _:bn8766\n",
      "Skipping _:bn8766\n",
      "Skipping _:bn8766\n",
      "Skipping _:bn8788\n",
      "Skipping _:bn8788\n",
      "Skipping _:bn8788\n",
      "Skipping _:bn8803\n",
      "Skipping _:bn8803\n",
      "Skipping _:bn8803\n",
      "Skipping _:bn8841\n",
      "Skipping _:bn8841\n",
      "Skipping _:bn8841\n",
      "Skipping _:bn8860\n",
      "Skipping _:bn8860\n",
      "Skipping _:bn8860\n",
      "Skipping _:bn9075\n",
      "Skipping _:bn9075\n",
      "Skipping _:bn9075\n",
      "Skipping _:bn9143\n",
      "Skipping _:bn9143\n",
      "Skipping _:bn9143\n",
      "Skipping _:bn9149\n",
      "Skipping _:bn9149\n",
      "Skipping _:bn9149\n",
      "Skipping _:bn9167\n",
      "Skipping _:bn9167\n",
      "Skipping _:bn9167\n",
      "Skipping _:bn9176\n",
      "Skipping _:bn9176\n",
      "Skipping _:bn9176\n",
      "Skipping _:bn9238\n",
      "Skipping _:bn9238\n",
      "Skipping _:bn9238\n",
      "Skipping _:bn9249\n",
      "Skipping _:bn9249\n",
      "Skipping _:bn9249\n",
      "Skipping _:bn9353\n",
      "Skipping _:bn9353\n",
      "Skipping _:bn9353\n",
      "Skipping _:bn9357\n",
      "Skipping _:bn9357\n",
      "Skipping _:bn9357\n",
      "Skipping _:bn9427\n",
      "Skipping _:bn9427\n",
      "Skipping _:bn9427\n",
      "Skipping _:bn9444\n",
      "Skipping _:bn9444\n",
      "Skipping _:bn9444\n",
      "Skipping _:bn9530\n",
      "Skipping _:bn9530\n",
      "Skipping _:bn9530\n",
      "Skipping _:bn9572\n",
      "Skipping _:bn9572\n",
      "Skipping _:bn9572\n",
      "Skipping _:bn9655\n",
      "Skipping _:bn9655\n",
      "Skipping _:bn9655\n",
      "Skipping _:bn9754\n",
      "Skipping _:bn9754\n",
      "Skipping _:bn9754\n",
      "Skipping _:bn9769\n",
      "Skipping _:bn9769\n",
      "Skipping _:bn9769\n",
      "Skipping _:bn9846\n",
      "Skipping _:bn9846\n",
      "Skipping _:bn9846\n",
      "Skipping _:bn9891\n",
      "Skipping _:bn9891\n",
      "Skipping _:bn9891\n",
      "Skipping _:bn9946\n",
      "Skipping _:bn9946\n",
      "Skipping _:bn9946\n",
      "Skipping _:bn9957\n",
      "Skipping _:bn9957\n",
      "Skipping _:bn9957\n",
      "Skipping _:bn9994\n",
      "Skipping _:bn9994\n",
      "Skipping _:bn9994\n",
      "Skipping _:bn10229\n",
      "Skipping _:bn10229\n",
      "Skipping _:bn10229\n",
      "Skipping _:bn10325\n",
      "Skipping _:bn10325\n",
      "Skipping _:bn10325\n",
      "Skipping _:bn10430\n",
      "Skipping _:bn10430\n",
      "Skipping _:bn10430\n",
      "Skipping _:bn10432\n",
      "Skipping _:bn10432\n",
      "Skipping _:bn10432\n",
      "Skipping _:bn10464\n",
      "Skipping _:bn10464\n",
      "Skipping _:bn10464\n",
      "Skipping _:bn10478\n",
      "Skipping _:bn10478\n",
      "Skipping _:bn10478\n",
      "Skipping _:bn10491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn12193:  71%|███████   | 364/511 [00:20<00:00, 178.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn10491\n",
      "Skipping _:bn10491\n",
      "Skipping _:bn10522\n",
      "Skipping _:bn10522\n",
      "Skipping _:bn10522\n",
      "Skipping _:bn10525\n",
      "Skipping _:bn10525\n",
      "Skipping _:bn10525\n",
      "Skipping _:bn10626\n",
      "Skipping _:bn10626\n",
      "Skipping _:bn10626\n",
      "Skipping _:bn10798\n",
      "Skipping _:bn10798\n",
      "Skipping _:bn10798\n",
      "Skipping _:bn10809\n",
      "Skipping _:bn10809\n",
      "Skipping _:bn10809\n",
      "Skipping _:bn10811\n",
      "Skipping _:bn10811\n",
      "Skipping _:bn10811\n",
      "Skipping _:bn10908\n",
      "Skipping _:bn10908\n",
      "Skipping _:bn10908\n",
      "Skipping _:bn10909\n",
      "Skipping _:bn10909\n",
      "Skipping _:bn10909\n",
      "Skipping _:bn10999\n",
      "Skipping _:bn10999\n",
      "Skipping _:bn10999\n",
      "Skipping _:bn11056\n",
      "Skipping _:bn11056\n",
      "Skipping _:bn11056\n",
      "Skipping _:bn11078\n",
      "Skipping _:bn11078\n",
      "Skipping _:bn11078\n",
      "Skipping _:bn11109\n",
      "Skipping _:bn11109\n",
      "Skipping _:bn11109\n",
      "Skipping _:bn11117\n",
      "Skipping _:bn11117\n",
      "Skipping _:bn11117\n",
      "Skipping _:bn11132\n",
      "Skipping _:bn11132\n",
      "Skipping _:bn11132\n",
      "Skipping _:bn11138\n",
      "Skipping _:bn11138\n",
      "Skipping _:bn11138\n",
      "Skipping _:bn11140\n",
      "Skipping _:bn11140\n",
      "Skipping _:bn11140\n",
      "Skipping _:bn11224\n",
      "Skipping _:bn11224\n",
      "Skipping _:bn11224\n",
      "Skipping _:bn11263\n",
      "Skipping _:bn11263\n",
      "Skipping _:bn11263\n",
      "Skipping _:bn11276\n",
      "Skipping _:bn11276\n",
      "Skipping _:bn11276\n",
      "Skipping _:bn11290\n",
      "Skipping _:bn11290\n",
      "Skipping _:bn11290\n",
      "Skipping _:bn11327\n",
      "Skipping _:bn11327\n",
      "Skipping _:bn11327\n",
      "Skipping _:bn11349\n",
      "Skipping _:bn11349\n",
      "Skipping _:bn11349\n",
      "Skipping _:bn11367\n",
      "Skipping _:bn11367\n",
      "Skipping _:bn11367\n",
      "Skipping _:bn11385\n",
      "Skipping _:bn11385\n",
      "Skipping _:bn11385\n",
      "Skipping _:bn11484\n",
      "Skipping _:bn11484\n",
      "Skipping _:bn11484\n",
      "Skipping _:bn11655\n",
      "Skipping _:bn11655\n",
      "Skipping _:bn11655\n",
      "Skipping _:bn11754\n",
      "Skipping _:bn11754\n",
      "Skipping _:bn11754\n",
      "Skipping _:bn11770\n",
      "Skipping _:bn11770\n",
      "Skipping _:bn11770\n",
      "Skipping _:bn11857\n",
      "Skipping _:bn11857\n",
      "Skipping _:bn11857\n",
      "Skipping _:bn11911\n",
      "Skipping _:bn11911\n",
      "Skipping _:bn11911\n",
      "Skipping _:bn11915\n",
      "Skipping _:bn11915\n",
      "Skipping _:bn11915\n",
      "Skipping _:bn11969\n",
      "Skipping _:bn11969\n",
      "Skipping _:bn11969\n",
      "Skipping _:bn12005\n",
      "Skipping _:bn12005\n",
      "Skipping _:bn12005\n",
      "Skipping _:bn12015\n",
      "Skipping _:bn12015\n",
      "Skipping _:bn12015\n",
      "Skipping _:bn12026\n",
      "Skipping _:bn12026\n",
      "Skipping _:bn12026\n",
      "Skipping _:bn12069\n",
      "Skipping _:bn12069\n",
      "Skipping _:bn12069\n",
      "Skipping _:bn12083\n",
      "Skipping _:bn12083\n",
      "Skipping _:bn12083\n",
      "Skipping _:bn12193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn13475:  79%|███████▉  | 403/511 [00:20<00:00, 173.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn12193\n",
      "Skipping _:bn12193\n",
      "Skipping _:bn12202\n",
      "Skipping _:bn12202\n",
      "Skipping _:bn12202\n",
      "Skipping _:bn12222\n",
      "Skipping _:bn12222\n",
      "Skipping _:bn12222\n",
      "Skipping _:bn12224\n",
      "Skipping _:bn12224\n",
      "Skipping _:bn12224\n",
      "Skipping _:bn12244\n",
      "Skipping _:bn12244\n",
      "Skipping _:bn12244\n",
      "Skipping _:bn12266\n",
      "Skipping _:bn12266\n",
      "Skipping _:bn12266\n",
      "Skipping _:bn12270\n",
      "Skipping _:bn12270\n",
      "Skipping _:bn12270\n",
      "Skipping _:bn12272\n",
      "Skipping _:bn12272\n",
      "Skipping _:bn12272\n",
      "Skipping _:bn12294\n",
      "Skipping _:bn12294\n",
      "Skipping _:bn12294\n",
      "Skipping _:bn12332\n",
      "Skipping _:bn12332\n",
      "Skipping _:bn12332\n",
      "Skipping _:bn12334\n",
      "Skipping _:bn12334\n",
      "Skipping _:bn12334\n",
      "Skipping _:bn12339\n",
      "Skipping _:bn12339\n",
      "Skipping _:bn12339\n",
      "Skipping _:bn12348\n",
      "Skipping _:bn12348\n",
      "Skipping _:bn12348\n",
      "Skipping _:bn12357\n",
      "Skipping _:bn12357\n",
      "Skipping _:bn12357\n",
      "Skipping _:bn12396\n",
      "Skipping _:bn12396\n",
      "Skipping _:bn12396\n",
      "Skipping _:bn12446\n",
      "Skipping _:bn12446\n",
      "Skipping _:bn12446\n",
      "Skipping _:bn12458\n",
      "Skipping _:bn12458\n",
      "Skipping _:bn12458\n",
      "Skipping _:bn12632\n",
      "Skipping _:bn12632\n",
      "Skipping _:bn12632\n",
      "Skipping _:bn12648\n",
      "Skipping _:bn12648\n",
      "Skipping _:bn12648\n",
      "Skipping _:bn12654\n",
      "Skipping _:bn12654\n",
      "Skipping _:bn12654\n",
      "Skipping _:bn12669\n",
      "Skipping _:bn12669\n",
      "Skipping _:bn12669\n",
      "Skipping _:bn12719\n",
      "Skipping _:bn12719\n",
      "Skipping _:bn12719\n",
      "Skipping _:bn12751\n",
      "Skipping _:bn12751\n",
      "Skipping _:bn12751\n",
      "Skipping _:bn12773\n",
      "Skipping _:bn12773\n",
      "Skipping _:bn12773\n",
      "Skipping _:bn12790\n",
      "Skipping _:bn12790\n",
      "Skipping _:bn12790\n",
      "Skipping _:bn12903\n",
      "Skipping _:bn12903\n",
      "Skipping _:bn12903\n",
      "Skipping _:bn12921\n",
      "Skipping _:bn12921\n",
      "Skipping _:bn12921\n",
      "Skipping _:bn12967\n",
      "Skipping _:bn12967\n",
      "Skipping _:bn12967\n",
      "Skipping _:bn13032\n",
      "Skipping _:bn13032\n",
      "Skipping _:bn13032\n",
      "Skipping _:bn13090\n",
      "Skipping _:bn13090\n",
      "Skipping _:bn13090\n",
      "Skipping _:bn13228\n",
      "Skipping _:bn13228\n",
      "Skipping _:bn13228\n",
      "Skipping _:bn13238\n",
      "Skipping _:bn13238\n",
      "Skipping _:bn13238\n",
      "Skipping _:bn13261\n",
      "Skipping _:bn13261\n",
      "Skipping _:bn13261\n",
      "Skipping _:bn13276\n",
      "Skipping _:bn13276\n",
      "Skipping _:bn13276\n",
      "Skipping _:bn13301\n",
      "Skipping _:bn13301\n",
      "Skipping _:bn13301\n",
      "Skipping _:bn13411\n",
      "Skipping _:bn13411\n",
      "Skipping _:bn13411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn15164:  86%|████████▋ | 441/511 [00:20<00:00, 176.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn13475\n",
      "Skipping _:bn13475\n",
      "Skipping _:bn13475\n",
      "Skipping _:bn13594\n",
      "Skipping _:bn13594\n",
      "Skipping _:bn13594\n",
      "Skipping _:bn13596\n",
      "Skipping _:bn13596\n",
      "Skipping _:bn13596\n",
      "Skipping _:bn13600\n",
      "Skipping _:bn13600\n",
      "Skipping _:bn13600\n",
      "Skipping _:bn13643\n",
      "Skipping _:bn13643\n",
      "Skipping _:bn13643\n",
      "Skipping _:bn13693\n",
      "Skipping _:bn13693\n",
      "Skipping _:bn13693\n",
      "Skipping _:bn13867\n",
      "Skipping _:bn13867\n",
      "Skipping _:bn13867\n",
      "Skipping _:bn13895\n",
      "Skipping _:bn13895\n",
      "Skipping _:bn13895\n",
      "Skipping _:bn13947\n",
      "Skipping _:bn13947\n",
      "Skipping _:bn13947\n",
      "Skipping _:bn14022\n",
      "Skipping _:bn14022\n",
      "Skipping _:bn14022\n",
      "Skipping _:bn14027\n",
      "Skipping _:bn14027\n",
      "Skipping _:bn14027\n",
      "Skipping _:bn14059\n",
      "Skipping _:bn14059\n",
      "Skipping _:bn14059\n",
      "Skipping _:bn14075\n",
      "Skipping _:bn14075\n",
      "Skipping _:bn14075\n",
      "Skipping _:bn14092\n",
      "Skipping _:bn14092\n",
      "Skipping _:bn14092\n",
      "Skipping _:bn14156\n",
      "Skipping _:bn14156\n",
      "Skipping _:bn14156\n",
      "Skipping _:bn14185\n",
      "Skipping _:bn14185\n",
      "Skipping _:bn14185\n",
      "Skipping _:bn14186\n",
      "Skipping _:bn14186\n",
      "Skipping _:bn14186\n",
      "Skipping _:bn14293\n",
      "Skipping _:bn14293\n",
      "Skipping _:bn14293\n",
      "Skipping _:bn14332\n",
      "Skipping _:bn14332\n",
      "Skipping _:bn14332\n",
      "Skipping _:bn14367\n",
      "Skipping _:bn14367\n",
      "Skipping _:bn14367\n",
      "Skipping _:bn14530\n",
      "Skipping _:bn14530\n",
      "Skipping _:bn14530\n",
      "Skipping _:bn14532\n",
      "Skipping _:bn14532\n",
      "Skipping _:bn14532\n",
      "Skipping _:bn14570\n",
      "Skipping _:bn14570\n",
      "Skipping _:bn14570\n",
      "Skipping _:bn14685\n",
      "Skipping _:bn14685\n",
      "Skipping _:bn14685\n",
      "Skipping _:bn14699\n",
      "Skipping _:bn14699\n",
      "Skipping _:bn14699\n",
      "Skipping _:bn14787\n",
      "Skipping _:bn14787\n",
      "Skipping _:bn14787\n",
      "Skipping _:bn14854\n",
      "Skipping _:bn14854\n",
      "Skipping _:bn14854\n",
      "Skipping _:bn14885\n",
      "Skipping _:bn14885\n",
      "Skipping _:bn14885\n",
      "Skipping _:bn14926\n",
      "Skipping _:bn14926\n",
      "Skipping _:bn14926\n",
      "Skipping _:bn14936\n",
      "Skipping _:bn14936\n",
      "Skipping _:bn14936\n",
      "Skipping _:bn14990\n",
      "Skipping _:bn14990\n",
      "Skipping _:bn14990\n",
      "Skipping _:bn15058\n",
      "Skipping _:bn15058\n",
      "Skipping _:bn15058\n",
      "Skipping _:bn15063\n",
      "Skipping _:bn15063\n",
      "Skipping _:bn15063\n",
      "Skipping _:bn15101\n",
      "Skipping _:bn15101\n",
      "Skipping _:bn15101\n",
      "Skipping _:bn15105\n",
      "Skipping _:bn15105\n",
      "Skipping _:bn15105\n",
      "Skipping _:bn15145\n",
      "Skipping _:bn15145\n",
      "Skipping _:bn15145\n",
      "Skipping _:bn15164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn16230:  90%|████████▉ | 459/511 [00:20<00:00, 165.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn15164\n",
      "Skipping _:bn15164\n",
      "Skipping _:bn15172\n",
      "Skipping _:bn15172\n",
      "Skipping _:bn15172\n",
      "Skipping _:bn15190\n",
      "Skipping _:bn15190\n",
      "Skipping _:bn15190\n",
      "Skipping _:bn15215\n",
      "Skipping _:bn15215\n",
      "Skipping _:bn15215\n",
      "Skipping _:bn15309\n",
      "Skipping _:bn15309\n",
      "Skipping _:bn15309\n",
      "Skipping _:bn15314\n",
      "Skipping _:bn15314\n",
      "Skipping _:bn15314\n",
      "Skipping _:bn15323\n",
      "Skipping _:bn15323\n",
      "Skipping _:bn15323\n",
      "Skipping _:bn15337\n",
      "Skipping _:bn15337\n",
      "Skipping _:bn15337\n",
      "Skipping _:bn15409\n",
      "Skipping _:bn15409\n",
      "Skipping _:bn15409\n",
      "Skipping _:bn15440\n",
      "Skipping _:bn15440\n",
      "Skipping _:bn15440\n",
      "Skipping _:bn15496\n",
      "Skipping _:bn15496\n",
      "Skipping _:bn15496\n",
      "Skipping _:bn15571\n",
      "Skipping _:bn15571\n",
      "Skipping _:bn15571\n",
      "Skipping _:bn15578\n",
      "Skipping _:bn15578\n",
      "Skipping _:bn15578\n",
      "Skipping _:bn15595\n",
      "Skipping _:bn15595\n",
      "Skipping _:bn15595\n",
      "Skipping _:bn15607\n",
      "Skipping _:bn15607\n",
      "Skipping _:bn15607\n",
      "Skipping _:bn15656\n",
      "Skipping _:bn15656\n",
      "Skipping _:bn15656\n",
      "Skipping _:bn15681\n",
      "Skipping _:bn15681\n",
      "Skipping _:bn15681\n",
      "Skipping _:bn15688\n",
      "Skipping _:bn15688\n",
      "Skipping _:bn15688\n",
      "Skipping _:bn15834\n",
      "Skipping _:bn15834\n",
      "Skipping _:bn15834\n",
      "Skipping _:bn15863\n",
      "Skipping _:bn15863\n",
      "Skipping _:bn15863\n",
      "Skipping _:bn15911\n",
      "Skipping _:bn15911\n",
      "Skipping _:bn15911\n",
      "Skipping _:bn15923\n",
      "Skipping _:bn15923\n",
      "Skipping _:bn15923\n",
      "Skipping _:bn15943\n",
      "Skipping _:bn15943\n",
      "Skipping _:bn15943\n",
      "Skipping _:bn15984\n",
      "Skipping _:bn15984\n",
      "Skipping _:bn15984\n",
      "Skipping _:bn15992\n",
      "Skipping _:bn15992\n",
      "Skipping _:bn15992\n",
      "Skipping _:bn16006\n",
      "Skipping _:bn16006\n",
      "Skipping _:bn16006\n",
      "Skipping _:bn16077\n",
      "Skipping _:bn16077\n",
      "Skipping _:bn16077\n",
      "Skipping _:bn16105\n",
      "Skipping _:bn16105\n",
      "Skipping _:bn16105\n",
      "Skipping _:bn16106\n",
      "Skipping _:bn16106\n",
      "Skipping _:bn16106\n",
      "Skipping _:bn16113\n",
      "Skipping _:bn16113\n",
      "Skipping _:bn16113\n",
      "Skipping _:bn16198\n",
      "Skipping _:bn16198\n",
      "Skipping _:bn16198\n",
      "Skipping _:bn16203\n",
      "Skipping _:bn16203\n",
      "Skipping _:bn16203\n",
      "Skipping _:bn16230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching bn17728: 100%|██████████| 511/511 [00:21<00:00, 24.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn16230\n",
      "Skipping _:bn16230\n",
      "Skipping _:bn16254\n",
      "Skipping _:bn16254\n",
      "Skipping _:bn16254\n",
      "Skipping _:bn16277\n",
      "Skipping _:bn16277\n",
      "Skipping _:bn16277\n",
      "Skipping _:bn16286\n",
      "Skipping _:bn16286\n",
      "Skipping _:bn16286\n",
      "Skipping _:bn16298\n",
      "Skipping _:bn16298\n",
      "Skipping _:bn16298\n",
      "Skipping _:bn16337\n",
      "Skipping _:bn16337\n",
      "Skipping _:bn16337\n",
      "Skipping _:bn16358\n",
      "Skipping _:bn16358\n",
      "Skipping _:bn16358\n",
      "Skipping _:bn16371\n",
      "Skipping _:bn16371\n",
      "Skipping _:bn16371\n",
      "Skipping _:bn16476\n",
      "Skipping _:bn16476\n",
      "Skipping _:bn16476\n",
      "Skipping _:bn16496\n",
      "Skipping _:bn16496\n",
      "Skipping _:bn16496\n",
      "Skipping _:bn16502\n",
      "Skipping _:bn16502\n",
      "Skipping _:bn16502\n",
      "Skipping _:bn16527\n",
      "Skipping _:bn16527\n",
      "Skipping _:bn16527\n",
      "Skipping _:bn16602\n",
      "Skipping _:bn16602\n",
      "Skipping _:bn16602\n",
      "Skipping _:bn16776\n",
      "Skipping _:bn16776\n",
      "Skipping _:bn16776\n",
      "Skipping _:bn16791\n",
      "Skipping _:bn16791\n",
      "Skipping _:bn16791\n",
      "Skipping _:bn16877\n",
      "Skipping _:bn16877\n",
      "Skipping _:bn16877\n",
      "Skipping _:bn16894\n",
      "Skipping _:bn16894\n",
      "Skipping _:bn16894\n",
      "Skipping _:bn16931\n",
      "Skipping _:bn16931\n",
      "Skipping _:bn16931\n",
      "Skipping _:bn16953\n",
      "Skipping _:bn16953\n",
      "Skipping _:bn16953\n",
      "Skipping _:bn16973\n",
      "Skipping _:bn16973\n",
      "Skipping _:bn16973\n",
      "Skipping _:bn17075\n",
      "Skipping _:bn17075\n",
      "Skipping _:bn17075\n",
      "Skipping _:bn17084\n",
      "Skipping _:bn17084\n",
      "Skipping _:bn17084\n",
      "Skipping _:bn17093\n",
      "Skipping _:bn17093\n",
      "Skipping _:bn17093\n",
      "Skipping _:bn17112\n",
      "Skipping _:bn17112\n",
      "Skipping _:bn17112\n",
      "Skipping _:bn17195\n",
      "Skipping _:bn17195\n",
      "Skipping _:bn17195\n",
      "Skipping _:bn17210\n",
      "Skipping _:bn17210\n",
      "Skipping _:bn17210\n",
      "Skipping _:bn17282\n",
      "Skipping _:bn17282\n",
      "Skipping _:bn17282\n",
      "Skipping _:bn17297\n",
      "Skipping _:bn17297\n",
      "Skipping _:bn17297\n",
      "Skipping _:bn17307\n",
      "Skipping _:bn17307\n",
      "Skipping _:bn17307\n",
      "Skipping _:bn17407\n",
      "Skipping _:bn17407\n",
      "Skipping _:bn17407\n",
      "Skipping _:bn17550\n",
      "Skipping _:bn17550\n",
      "Skipping _:bn17550\n",
      "Skipping _:bn17569\n",
      "Skipping _:bn17569\n",
      "Skipping _:bn17569\n",
      "Skipping _:bn17605\n",
      "Skipping _:bn17605\n",
      "Skipping _:bn17605\n",
      "Skipping _:bn17629\n",
      "Skipping _:bn17629\n",
      "Skipping _:bn17629\n",
      "Skipping _:bn17647\n",
      "Skipping _:bn17647\n",
      "Skipping _:bn17647\n",
      "Skipping _:bn17728\n",
      "Skipping _:bn17728\n",
      "Skipping _:bn17728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving classes:   0%|          | 0/511 [00:00<?, ?it/s]Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: {'new.pooler.dense.weight', 'new.pooler.dense.bias'}\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "_:bn17728 with 128 named individuals: 100%|██████████| 511/511 [17:53<00:00,  2.10s/it]                                                          \n",
      "Saving relations:   1%|          | 6/511 [00:00<00:28, 17.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:   2%|▏         | 9/511 [00:00<00:27, 18.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn469\n",
      "Skipping _:bn779\n",
      "Skipping _:bn1577\n",
      "Skipping _:bn12\n",
      "Skipping _:bn1131\n",
      "Skipping _:bn3528\n",
      "Skipping _:bn552\n",
      "Skipping _:bn324\n",
      "Skipping _:bn719\n",
      "Skipping _:bn651\n",
      "Skipping _:bn644\n",
      "Skipping _:bn551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:   5%|▍         | 24/511 [00:01<00:17, 27.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn2596\n",
      "Skipping _:bn562\n",
      "Skipping _:bn173\n",
      "Skipping _:bn18\n",
      "Skipping _:bn1282\n",
      "Skipping _:bn298\n",
      "Skipping _:bn372\n",
      "Skipping _:bn2547\n",
      "Skipping _:bn1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:   7%|▋         | 36/511 [00:01<00:24, 19.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn4482\n",
      "Skipping _:bn2669\n",
      "Skipping _:bn15\n",
      "Skipping _:bn4553\n",
      "Skipping _:bn1225\n",
      "Skipping _:bn546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:   9%|▉         | 45/511 [00:02<00:20, 23.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn368\n",
      "Skipping _:bn1061\n",
      "Skipping _:bn1151\n",
      "Skipping _:bn69\n",
      "Skipping _:bn66\n",
      "Skipping _:bn1734\n",
      "Skipping _:bn687\n",
      "Skipping _:bn578\n",
      "Skipping _:bn688\n",
      "Skipping _:bn1085\n",
      "Skipping _:bn936\n",
      "Skipping _:bn3916\n",
      "Skipping _:bn2149\n",
      "Skipping _:bn955\n",
      "Skipping _:bn124\n",
      "Skipping _:bn1378\n",
      "Skipping _:bn3060\n",
      "Skipping _:bn365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  12%|█▏        | 63/511 [00:02<00:11, 38.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn1172\n",
      "Skipping _:bn1518\n",
      "Skipping _:bn1879\n",
      "Skipping _:bn1188\n",
      "Skipping _:bn2190\n",
      "Skipping _:bn732\n",
      "Skipping _:bn640\n",
      "Skipping _:bn1622\n",
      "Skipping _:bn756\n",
      "Skipping _:bn219\n",
      "Skipping _:bn1992\n",
      "Skipping _:bn556\n",
      "Skipping _:bn227\n",
      "Skipping _:bn101\n",
      "Skipping _:bn3265\n",
      "Skipping _:bn1519\n",
      "Skipping _:bn818\n",
      "Skipping _:bn1223\n",
      "Skipping _:bn303\n",
      "Skipping _:bn1534\n",
      "Skipping _:bn1289\n",
      "Skipping _:bn248\n",
      "Skipping _:bn301\n",
      "Skipping _:bn1022\n",
      "Skipping _:bn1748\n",
      "Skipping _:bn749\n",
      "Skipping _:bn3953\n",
      "Skipping _:bn5041\n",
      "Skipping _:bn708\n",
      "Skipping _:bn1754\n",
      "Skipping _:bn460\n",
      "Skipping _:bn1831\n",
      "Skipping _:bn597\n",
      "Skipping _:bn848\n",
      "Skipping _:bn5330\n",
      "Skipping _:bn1287\n",
      "Skipping _:bn1218\n",
      "Skipping _:bn4051\n",
      "Skipping _:bn830\n",
      "Skipping _:bn10\n",
      "Skipping _:bn1335\n",
      "Skipping _:bn539\n",
      "Skipping _:bn299\n",
      "Skipping _:bn3529\n",
      "Skipping _:bn3587\n",
      "Skipping _:bn1843\n",
      "Skipping _:bn2411\n",
      "Skipping _:bn947\n",
      "Skipping _:bn1021\n",
      "Skipping _:bn1887\n",
      "Skipping _:bn550\n",
      "Skipping _:bn192\n",
      "Skipping _:bn61\n",
      "Skipping _:bn1120\n",
      "Skipping _:bn391\n",
      "Skipping _:bn106\n",
      "Skipping _:bn284\n",
      "Skipping _:bn1414\n",
      "Skipping _:bn2217\n",
      "Skipping _:bn1651\n",
      "Skipping _:bn598\n",
      "Skipping _:bn426\n",
      "Skipping _:bn1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  13%|█▎        | 68/511 [00:03<00:17, 24.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn4605\n",
      "Skipping _:bn2951\n",
      "Skipping _:bn182\n",
      "Skipping _:bn1253\n",
      "Skipping _:bn367\n",
      "Skipping _:bn21\n",
      "Skipping _:bn220\n",
      "Skipping _:bn1554\n",
      "Skipping _:bn1276\n",
      "Skipping _:bn3570\n",
      "Skipping _:bn463\n",
      "Skipping _:bn639\n",
      "Skipping _:bn80\n",
      "Skipping _:bn761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  16%|█▌        | 81/511 [00:03<00:15, 27.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn440\n",
      "Skipping _:bn1291\n",
      "Skipping _:bn120\n",
      "Skipping _:bn1909\n",
      "Skipping _:bn1304\n",
      "Skipping _:bn402\n",
      "Skipping _:bn228\n",
      "Skipping _:bn338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  17%|█▋        | 88/511 [00:03<00:15, 27.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn250\n",
      "Skipping _:bn28\n",
      "Skipping _:bn1480\n",
      "Skipping _:bn2702\n",
      "Skipping _:bn118\n",
      "Skipping _:bn974\n",
      "Skipping _:bn2132\n",
      "Skipping _:bn949\n",
      "Skipping _:bn30\n",
      "Skipping _:bn13\n",
      "Skipping _:bn826\n",
      "Skipping _:bn529\n",
      "Skipping _:bn141\n",
      "Skipping _:bn2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  18%|█▊        | 92/511 [00:04<00:19, 21.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn3030\n",
      "Skipping _:bn1157\n",
      "Skipping _:bn995\n",
      "Skipping _:bn321\n",
      "Skipping _:bn1624\n",
      "Skipping _:bn43\n",
      "Skipping _:bn1701\n",
      "Skipping _:bn33\n",
      "Skipping _:bn1369\n",
      "Skipping _:bn852\n",
      "Skipping _:bn295\n",
      "Skipping _:bn1083\n",
      "Skipping _:bn777\n",
      "Skipping _:bn137\n",
      "Skipping _:bn1161\n",
      "Skipping _:bn829\n",
      "Skipping _:bn388\n",
      "Skipping _:bn103\n",
      "Skipping _:bn574\n",
      "Skipping _:bn4024\n",
      "Skipping _:bn1559\n",
      "Skipping _:bn482\n",
      "Skipping _:bn54\n",
      "Skipping _:bn76\n",
      "Skipping _:bn1375\n",
      "Skipping _:bn795\n",
      "Skipping _:bn335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  19%|█▉        | 99/511 [00:04<00:23, 17.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn64\n",
      "Skipping _:bn1453\n",
      "Skipping _:bn721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations: 100%|██████████| 511/511 [00:04<00:00, 104.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn276\n",
      "Skipping _:bn110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixing props links: 100%|██████████| 628/628 [00:01<00:00, 617.24it/s]\n",
      "100%|██████████| 9/9 [00:06<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM model NousResearch/Hermes-3-Llama-3.2-3B-GGUF None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M2 Pro) - 19748 MiB free\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 255 tensors from /Users/benedikt/.cache/huggingface/hub/models--NousResearch--Hermes-3-Llama-3.2-3B-GGUF/snapshots/3cd927095d8cbab12c743f932aa63b6f7bbfa141/./Hermes-3-Llama-3.2-3B.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hermes 3 Llama 3.2 3b Base Fft Chatml...\n",
      "llama_model_loader: - kv   3:                            general.version str              = 2024-11-24\n",
      "llama_model_loader: - kv   4:                       general.organization str              = Nous Research Core\n",
      "llama_model_loader: - kv   5:                           general.finetune str              = base-fft-chatml-rerun-8xh100-e4-4k-steps\n",
      "llama_model_loader: - kv   6:                           general.basename str              = Hermes-3-Llama-3.2\n",
      "llama_model_loader: - kv   7:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   8:                            general.license str              = llama3\n",
      "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Meta Llama 3.2 3B\n",
      "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Met...\n",
      "llama_model_loader: - kv  13:                               general.tags arr[str,12]      = [\"Llama-3\", \"instruct\", \"finetune\", \"...\n",
      "llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  15:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  16:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  17:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  18:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  19:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  20:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  21:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  22:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  23:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  24:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  25:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  26:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  27:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 128039\n",
      "llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  37:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q8_0:  197 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7994 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 3.18 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Hermes 3 Llama 3.2 3b Base Fft Chatml Rerun 8xh100 E4 2024 11 24 4k Steps\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128039 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 128039 '<|im_end|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOG token        = 128039 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  3255.91 MiB, ( 5352.33 / 21845.34)\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors: Metal_Mapped model buffer size =  3255.91 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   399.23 MiB\n",
      ".................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 10016\n",
      "llama_new_context_with_model: n_ctx_per_seq = 10016\n",
      "llama_new_context_with_model: n_batch       = 2048\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (10016) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x345871730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x3ad7dbbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x12a6330d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x12a654630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x12a648c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x12a6525c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x12a63bee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x12a62d960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x360ca0d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x12a660770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x3b2ffe900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x34588a8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x12a642f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x12a65ee30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x3b2f16b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x12a625420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x12a623270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x12a6553e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x362508ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x345884590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x12a641a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x362509260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x12a66e290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x3b0cb97f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x3ad7dbf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x12a657730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a661640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x12a658b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x36250a710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x362509b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a664470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x12a651c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x12a65e600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x36250bf80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a67c8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a688df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a67e600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a640860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x36250b0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x36250da20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a690860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a67fbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x36250d2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x36250ea60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a674340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a655ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a659090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x36250f540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x3625104b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a6317b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a624bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a6722f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x3ad7edb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x362512140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x12a67f290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x3ac6671c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x34588c420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x3ac6a81d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x362511d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x362512bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x3625132c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x362513520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x3ac696690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x362514900 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x3ac6abe60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x362513780 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x3625142c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x362515980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x3625153d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x362516320 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x362517b00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x3625185a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x3b1dcaf30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x3ac69a2f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x362517450 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x3ac69b5e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x3ac69c1d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x3ac69c730 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x362518fa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x3ac69cb70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x362519e80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x3625197a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x36251a920 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x36251c000 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x36251b0e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x3ac69d960 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x36251d630 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x36251cf10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x34588d110 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x36251e1d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x36251f0f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x36251fb50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x362093c40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x3575c8800 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x3b359a7f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x358c3daf0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x3b4912890 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x362098830 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x36209a5b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x321211330 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x3a94db4d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x36209a0b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x3b4917c20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x36208aef0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x3b49f1930 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x3b30f8f60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x360c9dbe0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x3b30f77c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x3b4988750 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x3b49bf4a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x3621f1630 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x3b30db0b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x3b4913b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x3b30f13f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x3b30fb960 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x3b4919770 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x3b49fc1f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x35da3e300 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x3b49d7450 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x3b49fc450 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x362086680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x36208d770 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x362404080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x362404720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x362404980 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x3620937b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x362405960 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x36205bb60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x362406710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x360c9f3e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x3b08fdfa0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x362406d50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x362056610 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x3b30f0150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x362407080 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x362407790 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x362407f10 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x362408610 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x362408c20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x362409740 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x3b3036570 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x32120cb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x321211c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x36240a550 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x32120ddb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x36240ac40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x32120eac0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x32120f490 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x35da91c00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x32120e370 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x36240c5a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x35da92340 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x35da92a20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x35da93a20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x35da933a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x36240d9c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x35da92c80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x35da8e620 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x36240e760 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x36240cb90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x3ac69e0e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x36205df10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x36209e250 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x36240d160 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x36240f1c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x36240f6b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x36208fd70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x3624100b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x3620903a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x36209cc90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x36209d4f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x362411e40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x36209c010 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x362412ec0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x362092600 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x362413610 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x362412590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x3620994a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x362413e10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x362092860 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x362414580 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x362414e40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x3624154a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x362415c30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x362416630 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x362416960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x3620965f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x362417040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x36208e580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x362088fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x362417790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x362418040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x3624186f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x362419ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x36241a2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x362096140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x362097bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x36241adb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x36241bc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x36241b520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x362094a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x362095410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x36241c450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x36208f210 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x362090990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x36241e080 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x362074770 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x36241d8e0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x36208b6d0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x36208c600 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x36208aad0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x3620783f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x3620768f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x36208ca90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x36241e7e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x362054160 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x362088080 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x3620883e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x362088aa0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x36241f900 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x36207fbf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x3620774f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x362049880 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x36204f1f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x36241f510 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x362420e10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x36204a370 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x362420890 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x362083ac0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x362080450 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x3624218d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x362421b30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x36204d680 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x362424460 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x362424900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x3624253d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x362425bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x362426300 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x362426560 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x362427aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x362426bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x36206c140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x36251e980 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x36251ebe0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x3ac666c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10a629800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x345890510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10a6445d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x3ac69eb30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10a644830 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x3b34ec890 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x3b34ecaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x3b34ed2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10a68f430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10a68fea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x3b34edce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x3b34ecdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x3b34e9330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10a6908e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x3b34e9590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10a690f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x3b34e9f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x3458a8db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x10a6915f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x3b34eae90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x3b34ebf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x3b34e7030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x3b34e7570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x10a691960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x3b34e79f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10a6924b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10a692c40 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1095.50 MiB\n",
      "llama_new_context_with_model: KV self size  = 1095.50 MiB, K (f16):  547.75 MiB, V (f16):  547.75 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   513.07 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    25.57 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128039', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '8', 'llama.vocab_size': '128256', 'llama.embedding_length': '3072', 'general.base_model.0.name': 'Meta Llama 3.2 3B', 'general.size_label': '3B', 'llama.block_count': '28', 'general.base_model.0.organization': 'Meta Llama', 'llama.attention.value_length': '128', 'llama.attention.head_count': '24', 'llama.attention.key_length': '128', 'general.base_model.count': '1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Meta-Llama-3.2-3B', 'general.license': 'llama3', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'Hermes-3-Llama-3.2', 'llama.context_length': '131072', 'general.file_type': '7', 'general.finetune': 'base-fft-chatml-rerun-8xh100-e4-4k-steps', 'general.organization': 'Nous Research Core', 'tokenizer.ggml.bos_token_id': '128000', 'general.type': 'model', 'general.version': '2024-11-24', 'llama.rope.freq_base': '500000.000000', 'general.name': 'Hermes 3 Llama 3.2 3b Base Fft Chatml Rerun 8xh100 E4 2024 11 24 4k Steps'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "Building documents:   2%|▏         | 11/510 [00:04<01:36,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn469\n",
      "Skipping _:bn779\n",
      "Skipping _:bn1577\n",
      "Skipping _:bn12\n",
      "Skipping _:bn1131\n",
      "Skipping _:bn3528\n",
      "Skipping _:bn552\n",
      "Skipping _:bn324\n",
      "Skipping _:bn719\n",
      "Skipping _:bn651\n",
      "Skipping _:bn644\n",
      "Skipping _:bn551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   3%|▎         | 15/510 [00:05<01:47,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   3%|▎         | 17/510 [00:05<01:26,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn562\n",
      "Skipping _:bn173\n",
      "Skipping _:bn18\n",
      "Skipping _:bn1282\n",
      "Skipping _:bn298\n",
      "Skipping _:bn372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   5%|▍         | 24/510 [00:06<00:44, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn2547\n",
      "Skipping _:bn1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   7%|▋         | 35/510 [00:11<02:31,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn4482\n",
      "Skipping _:bn2669\n",
      "Skipping _:bn15\n",
      "Skipping _:bn4553\n",
      "Skipping _:bn1225\n",
      "Skipping _:bn546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   7%|▋         | 36/510 [00:11<02:21,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn368\n",
      "Skipping _:bn1061\n",
      "Skipping _:bn1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   9%|▊         | 44/510 [00:12<00:59,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn69\n",
      "Skipping _:bn66\n",
      "Skipping _:bn1734\n",
      "Skipping _:bn687\n",
      "Skipping _:bn578\n",
      "Skipping _:bn688\n",
      "Skipping _:bn1085\n",
      "Skipping _:bn936\n",
      "Skipping _:bn3916\n",
      "Skipping _:bn2149\n",
      "Skipping _:bn955\n",
      "Skipping _:bn124\n",
      "Skipping _:bn1378\n",
      "Skipping _:bn3060\n",
      "Skipping _:bn365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  12%|█▏        | 62/510 [00:14<00:41, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn1172\n",
      "Skipping _:bn1518\n",
      "Skipping _:bn1879\n",
      "Skipping _:bn1188\n",
      "Skipping _:bn2190\n",
      "Skipping _:bn732\n",
      "Skipping _:bn640\n",
      "Skipping _:bn1622\n",
      "Skipping _:bn756\n",
      "Skipping _:bn219\n",
      "Skipping _:bn1992\n",
      "Skipping _:bn556\n",
      "Skipping _:bn227\n",
      "Skipping _:bn101\n",
      "Skipping _:bn3265\n",
      "Skipping _:bn1519\n",
      "Skipping _:bn818\n",
      "Skipping _:bn1223\n",
      "Skipping _:bn303\n",
      "Skipping _:bn1534\n",
      "Skipping _:bn1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  13%|█▎        | 65/510 [00:14<00:31, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn248\n",
      "Skipping _:bn301\n",
      "Skipping _:bn1022\n",
      "Skipping _:bn1748\n",
      "Skipping _:bn749\n",
      "Skipping _:bn3953\n",
      "Skipping _:bn5041\n",
      "Skipping _:bn708\n",
      "Skipping _:bn1754\n",
      "Skipping _:bn460\n",
      "Skipping _:bn1831\n",
      "Skipping _:bn597\n",
      "Skipping _:bn848\n",
      "Skipping _:bn5330\n",
      "Skipping _:bn1287\n",
      "Skipping _:bn1218\n",
      "Skipping _:bn4051\n",
      "Skipping _:bn830\n",
      "Skipping _:bn10\n",
      "Skipping _:bn1335\n",
      "Skipping _:bn539\n",
      "Skipping _:bn299\n",
      "Skipping _:bn3529\n",
      "Skipping _:bn3587\n",
      "Skipping _:bn1843\n",
      "Skipping _:bn2411\n",
      "Skipping _:bn947\n",
      "Skipping _:bn1021\n",
      "Skipping _:bn1887\n",
      "Skipping _:bn550\n",
      "Skipping _:bn192\n",
      "Skipping _:bn61\n",
      "Skipping _:bn1120\n",
      "Skipping _:bn391\n",
      "Skipping _:bn106\n",
      "Skipping _:bn284\n",
      "Skipping _:bn1414\n",
      "Skipping _:bn2217\n",
      "Skipping _:bn1651\n",
      "Skipping _:bn598\n",
      "Skipping _:bn426\n",
      "Skipping _:bn1074\n",
      "Skipping _:bn4605\n",
      "Skipping _:bn2951\n",
      "Skipping _:bn182\n",
      "Skipping _:bn1253\n",
      "Skipping _:bn367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  14%|█▍        | 71/510 [00:15<00:41, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn21\n",
      "Skipping _:bn220\n",
      "Skipping _:bn1554\n",
      "ignoring <https://w3id.org/brainteaser/ontology/schema/MuscleTone>\n",
      "Skipping _:bn1276\n",
      "Skipping _:bn3570\n",
      "Skipping _:bn463\n",
      "Skipping _:bn639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  15%|█▍        | 74/510 [00:16<01:29,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn80\n",
      "Skipping _:bn761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  15%|█▌        | 78/510 [00:20<04:05,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  16%|█▋        | 83/510 [00:21<02:15,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  17%|█▋        | 87/510 [00:21<01:33,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn120\n",
      "Skipping _:bn1909\n",
      "Skipping _:bn1304\n",
      "Skipping _:bn402\n",
      "Skipping _:bn228\n",
      "Skipping _:bn338\n",
      "Skipping _:bn250\n",
      "Skipping _:bn28\n",
      "Skipping _:bn1480\n",
      "Skipping _:bn2702\n",
      "Skipping _:bn118\n",
      "Skipping _:bn974\n",
      "Skipping _:bn2132\n",
      "Skipping _:bn949\n",
      "Skipping _:bn30\n",
      "Skipping _:bn13\n",
      "Skipping _:bn826\n",
      "Skipping _:bn529\n",
      "Skipping _:bn141\n",
      "Skipping _:bn2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  18%|█▊        | 91/510 [00:22<01:31,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn3030\n",
      "Skipping _:bn1157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  18%|█▊        | 92/510 [00:22<01:41,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn995\n",
      "Skipping _:bn321\n",
      "Skipping _:bn1624\n",
      "Skipping _:bn43\n",
      "Skipping _:bn1701\n",
      "Skipping _:bn33\n",
      "Skipping _:bn1369\n",
      "Skipping _:bn852\n",
      "Skipping _:bn295\n",
      "Skipping _:bn1083\n",
      "Skipping _:bn777\n",
      "Skipping _:bn137\n",
      "Skipping _:bn1161\n",
      "Skipping _:bn829\n",
      "Skipping _:bn388\n",
      "Skipping _:bn103\n",
      "Skipping _:bn574\n",
      "Skipping _:bn4024\n",
      "Skipping _:bn1559\n",
      "Skipping _:bn482\n",
      "Skipping _:bn54\n",
      "Skipping _:bn76\n",
      "Skipping _:bn1375\n",
      "Skipping _:bn795\n",
      "Skipping _:bn335\n",
      "Skipping _:bn64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  19%|█▉        | 96/510 [00:23<01:10,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn1453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  19%|█▉        | 97/510 [00:23<01:06,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  20%|██        | 102/510 [00:24<01:59,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  22%|██▏       | 111/510 [00:25<00:40,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents: 100%|██████████| 510/510 [17:03<00:00,  2.01s/it]\n",
      "2025-06-11 18:44:02,307 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434b4c4e7a164324a3c8078f00ea87af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 18:45:17,631 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-11 18:45:17,631 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2025-06-11 18:45:28,593 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-11 18:45:28,594 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "To disable this warning, you can either:\n",
      "To disable this warning, you can either:\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-06-11 18:45:30,747 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-11 18:45:30,751 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "  0%|          | 0/208 [00:00<?, ?it/s]llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1525.19 ms /   476 tokens\n",
      "  0%|          | 1/208 [00:01<05:17,  1.53s/it]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     516.12 ms /   212 tokens\n",
      "  1%|          | 2/208 [00:02<03:12,  1.07it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     538.20 ms /   228 tokens\n",
      "  1%|▏         | 3/208 [00:02<02:34,  1.32it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.72 ms /   203 tokens\n",
      "  2%|▏         | 4/208 [00:03<02:08,  1.59it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.81 ms /   208 tokens\n",
      "  2%|▏         | 5/208 [00:03<01:53,  1.79it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     431.85 ms /   211 tokens\n",
      "  3%|▎         | 6/208 [00:03<01:44,  1.94it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     460.90 ms /   206 tokens\n",
      "  3%|▎         | 7/208 [00:04<01:40,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     568.40 ms /   224 tokens\n",
      "  4%|▍         | 8/208 [00:04<01:44,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.52 ms /   205 tokens\n",
      "  4%|▍         | 9/208 [00:05<01:38,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     487.35 ms /   216 tokens\n",
      "  5%|▍         | 10/208 [00:05<01:37,  2.03it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.14 ms /   205 tokens\n",
      "  5%|▌         | 11/208 [00:06<01:31,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.75 ms /   208 tokens\n",
      "  6%|▌         | 12/208 [00:06<01:29,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.78 ms /   206 tokens\n",
      "  6%|▋         | 13/208 [00:07<01:26,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     431.30 ms /   207 tokens\n",
      "  7%|▋         | 14/208 [00:07<01:25,  2.28it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.42 ms /   201 tokens\n",
      "  7%|▋         | 15/208 [00:07<01:22,  2.33it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.57 ms /   205 tokens\n",
      "  8%|▊         | 16/208 [00:08<01:22,  2.32it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.49 ms /   202 tokens\n",
      "  8%|▊         | 17/208 [00:08<01:20,  2.36it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     377.24 ms /   198 tokens\n",
      "  9%|▊         | 18/208 [00:09<01:17,  2.44it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     484.28 ms /   215 tokens\n",
      "  9%|▉         | 19/208 [00:09<01:21,  2.31it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.90 ms /   202 tokens\n",
      " 10%|▉         | 20/208 [00:10<01:23,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     458.33 ms /   199 tokens\n",
      " 10%|█         | 21/208 [00:10<01:23,  2.24it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     458.77 ms /   214 tokens\n",
      " 11%|█         | 22/208 [00:11<01:23,  2.22it/s]Llama.generate: 267 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     423.44 ms /   192 tokens\n",
      " 11%|█         | 23/208 [00:11<01:22,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     380.45 ms /   199 tokens\n",
      " 12%|█▏        | 24/208 [00:11<01:18,  2.35it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.86 ms /   206 tokens\n",
      " 12%|█▏        | 25/208 [00:12<01:18,  2.34it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     434.53 ms /   211 tokens\n",
      " 12%|█▎        | 26/208 [00:12<01:18,  2.32it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     351.65 ms /   196 tokens\n",
      " 13%|█▎        | 27/208 [00:13<01:13,  2.46it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.48 ms /   201 tokens\n",
      " 13%|█▎        | 28/208 [00:13<01:13,  2.46it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.16 ms /   201 tokens\n",
      " 14%|█▍        | 29/208 [00:13<01:12,  2.45it/s]Llama.generate: 267 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     542.77 ms /   172 tokens\n",
      " 14%|█▍        | 30/208 [00:14<01:19,  2.23it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     354.52 ms /   199 tokens\n",
      " 15%|█▍        | 31/208 [00:14<01:14,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.65 ms /   198 tokens\n",
      " 15%|█▌        | 32/208 [00:15<01:13,  2.40it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     434.10 ms /   207 tokens\n",
      " 16%|█▌        | 33/208 [00:15<01:13,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.80 ms /   214 tokens\n",
      " 16%|█▋        | 34/208 [00:16<01:15,  2.30it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     378.14 ms /   199 tokens\n",
      " 17%|█▋        | 35/208 [00:16<01:12,  2.39it/s]Llama.generate: 268 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     378.92 ms /   198 tokens\n",
      " 17%|█▋        | 36/208 [00:16<01:09,  2.46it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     485.34 ms /   213 tokens\n",
      " 18%|█▊        | 37/208 [00:17<01:13,  2.32it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.07 ms /   210 tokens\n",
      " 18%|█▊        | 38/208 [00:17<01:14,  2.27it/s]Llama.generate: 267 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     661.41 ms /   242 tokens\n",
      " 19%|█▉        | 39/208 [00:18<01:25,  1.97it/s]Llama.generate: 267 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     592.50 ms /   226 tokens\n",
      " 19%|█▉        | 40/208 [00:19<01:29,  1.88it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     435.86 ms /   213 tokens\n",
      " 20%|█▉        | 41/208 [00:19<01:24,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     536.71 ms /   216 tokens\n",
      " 20%|██        | 42/208 [00:20<01:25,  1.94it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     535.26 ms /   221 tokens\n",
      " 21%|██        | 43/208 [00:20<01:26,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     429.46 ms /   205 tokens\n",
      " 21%|██        | 44/208 [00:20<01:21,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     431.33 ms /   206 tokens\n",
      " 22%|██▏       | 45/208 [00:21<01:17,  2.10it/s]Llama.generate: 267 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   157 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     359.46 ms /   162 tokens\n",
      " 22%|██▏       | 46/208 [00:21<01:11,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     568.37 ms /   232 tokens\n",
      " 23%|██▎       | 47/208 [00:22<01:17,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.90 ms /   206 tokens\n",
      " 23%|██▎       | 48/208 [00:22<01:14,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     377.40 ms /   200 tokens\n",
      " 24%|██▎       | 49/208 [00:23<01:10,  2.27it/s]Llama.generate: 267 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   345 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     817.90 ms /   358 tokens\n",
      " 24%|██▍       | 50/208 [00:23<01:27,  1.80it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     381.91 ms /   203 tokens\n",
      " 25%|██▍       | 51/208 [00:24<01:19,  1.99it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.91 ms /   200 tokens\n",
      " 25%|██▌       | 52/208 [00:24<01:13,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   170 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     493.79 ms /   179 tokens\n",
      " 25%|██▌       | 53/208 [00:25<01:14,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.30 ms /   207 tokens\n",
      " 26%|██▌       | 54/208 [00:25<01:11,  2.15it/s]Llama.generate: 267 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.81 ms /   195 tokens\n",
      " 26%|██▋       | 55/208 [00:26<01:13,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   258 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     777.42 ms /   273 tokens\n",
      " 27%|██▋       | 56/208 [00:27<01:26,  1.75it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     434.11 ms /   210 tokens\n",
      " 27%|██▋       | 57/208 [00:27<01:20,  1.88it/s]Llama.generate: 267 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.31 ms /   233 tokens\n",
      " 28%|██▊       | 58/208 [00:28<01:21,  1.85it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.67 ms /   212 tokens\n",
      " 28%|██▊       | 59/208 [00:28<01:15,  1.96it/s]Llama.generate: 267 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     656.74 ms /   248 tokens\n",
      " 29%|██▉       | 60/208 [00:29<01:22,  1.80it/s]Llama.generate: 267 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   252 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     556.30 ms /   260 tokens\n",
      " 29%|██▉       | 61/208 [00:29<01:21,  1.80it/s]Llama.generate: 267 prefix-match hit, remaining 152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   152 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     354.11 ms /   157 tokens\n",
      " 30%|██▉       | 62/208 [00:30<01:12,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.14 ms /   201 tokens\n",
      " 30%|███       | 63/208 [00:30<01:09,  2.10it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     458.77 ms /   215 tokens\n",
      " 31%|███       | 64/208 [00:30<01:07,  2.12it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.85 ms /   208 tokens\n",
      " 31%|███▏      | 65/208 [00:31<01:05,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.37 ms /   204 tokens\n",
      " 32%|███▏      | 66/208 [00:31<01:03,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.04 ms /   208 tokens\n",
      " 32%|███▏      | 67/208 [00:32<01:01,  2.30it/s]Llama.generate: 267 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     594.55 ms /   234 tokens\n",
      " 33%|███▎      | 68/208 [00:32<01:07,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.90 ms /   221 tokens\n",
      " 33%|███▎      | 69/208 [00:33<01:06,  2.10it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.53 ms /   208 tokens\n",
      " 34%|███▎      | 70/208 [00:33<01:02,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   225 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     621.90 ms /   235 tokens\n",
      " 34%|███▍      | 71/208 [00:34<01:09,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   282 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     560.57 ms /   288 tokens\n",
      " 35%|███▍      | 72/208 [00:34<01:11,  1.91it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.84 ms /   211 tokens\n",
      " 35%|███▌      | 73/208 [00:35<01:07,  1.99it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     431.92 ms /   208 tokens\n",
      " 36%|███▌      | 74/208 [00:35<01:04,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     580.08 ms /   242 tokens\n",
      " 36%|███▌      | 75/208 [00:36<01:08,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.33 ms /   215 tokens\n",
      " 37%|███▋      | 76/208 [00:36<01:04,  2.05it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     433.33 ms /   212 tokens\n",
      " 37%|███▋      | 77/208 [00:37<01:01,  2.12it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     379.12 ms /   203 tokens\n",
      " 38%|███▊      | 78/208 [00:37<00:57,  2.25it/s]Llama.generate: 268 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     381.06 ms /   199 tokens\n",
      " 38%|███▊      | 79/208 [00:37<00:54,  2.35it/s]Llama.generate: 267 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     514.53 ms /   237 tokens\n",
      " 38%|███▊      | 80/208 [00:38<00:58,  2.21it/s]Llama.generate: 267 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     421.26 ms /   214 tokens\n",
      " 39%|███▉      | 81/208 [00:38<00:56,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     369.55 ms /   202 tokens\n",
      " 39%|███▉      | 82/208 [00:39<00:53,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.21 ms /   209 tokens\n",
      " 40%|███▉      | 83/208 [00:39<00:53,  2.32it/s]Llama.generate: 270 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     605.94 ms /   229 tokens\n",
      " 40%|████      | 84/208 [00:40<00:59,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     488.13 ms /   220 tokens\n",
      " 41%|████      | 85/208 [00:40<00:59,  2.06it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     457.97 ms /   213 tokens\n",
      " 41%|████▏     | 86/208 [00:41<00:58,  2.09it/s]Llama.generate: 267 prefix-match hit, remaining 257 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   257 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     542.91 ms /   263 tokens\n",
      " 42%|████▏     | 87/208 [00:41<01:00,  2.01it/s]Llama.generate: 267 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   235 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     474.11 ms /   240 tokens\n",
      " 42%|████▏     | 88/208 [00:42<00:58,  2.04it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.72 ms /   215 tokens\n",
      " 43%|████▎     | 89/208 [00:42<00:57,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     333.09 ms /   194 tokens\n",
      " 43%|████▎     | 90/208 [00:43<00:51,  2.28it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.21 ms /   198 tokens\n",
      " 44%|████▍     | 91/208 [00:43<00:50,  2.33it/s]Llama.generate: 267 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     537.37 ms /   222 tokens\n",
      " 44%|████▍     | 92/208 [00:43<00:53,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     433.12 ms /   212 tokens\n",
      " 45%|████▍     | 93/208 [00:44<00:52,  2.20it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.79 ms /   213 tokens\n",
      " 45%|████▌     | 94/208 [00:44<00:51,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     416.62 ms /   203 tokens\n",
      " 46%|████▌     | 95/208 [00:45<00:50,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     376.23 ms /   198 tokens\n",
      " 46%|████▌     | 96/208 [00:45<00:47,  2.35it/s]Llama.generate: 268 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.15 ms /   197 tokens\n",
      " 47%|████▋     | 97/208 [00:46<00:46,  2.39it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     433.44 ms /   206 tokens\n",
      " 47%|████▋     | 98/208 [00:46<00:46,  2.36it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     600.92 ms /   228 tokens\n",
      " 48%|████▊     | 99/208 [00:47<00:51,  2.10it/s]Llama.generate: 267 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     589.92 ms /   234 tokens\n",
      " 48%|████▊     | 100/208 [00:47<00:55,  1.96it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     429.75 ms /   207 tokens\n",
      " 49%|████▊     | 101/208 [00:48<00:52,  2.05it/s]Llama.generate: 267 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     316.84 ms /   194 tokens\n",
      " 49%|████▉     | 102/208 [00:48<00:46,  2.29it/s]Llama.generate: 267 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     553.14 ms /   219 tokens\n",
      " 50%|████▉     | 103/208 [00:49<00:49,  2.12it/s]Llama.generate: 267 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     566.07 ms /   233 tokens\n",
      " 50%|█████     | 104/208 [00:49<00:52,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     314.99 ms /   191 tokens\n",
      " 50%|█████     | 105/208 [00:49<00:45,  2.24it/s]Llama.generate: 267 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     506.57 ms /   218 tokens\n",
      " 51%|█████     | 106/208 [00:50<00:47,  2.15it/s]Llama.generate: 267 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     563.86 ms /   234 tokens\n",
      " 51%|█████▏    | 107/208 [00:50<00:49,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.65 ms /   216 tokens\n",
      " 52%|█████▏    | 108/208 [00:51<00:46,  2.13it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     609.78 ms /   230 tokens\n",
      " 52%|█████▏    | 109/208 [00:52<00:50,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   159 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     381.35 ms /   165 tokens\n",
      " 53%|█████▎    | 110/208 [00:52<00:46,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     509.08 ms /   214 tokens\n",
      " 53%|█████▎    | 111/208 [00:52<00:47,  2.06it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.87 ms /   208 tokens\n",
      " 54%|█████▍    | 112/208 [00:53<00:45,  2.13it/s]Llama.generate: 267 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.09 ms /   193 tokens\n",
      " 54%|█████▍    | 113/208 [00:53<00:42,  2.24it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.38 ms /   199 tokens\n",
      " 55%|█████▍    | 114/208 [00:54<00:40,  2.30it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.14 ms /   207 tokens\n",
      " 55%|█████▌    | 115/208 [00:54<00:40,  2.30it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.83 ms /   198 tokens\n",
      " 56%|█████▌    | 116/208 [00:54<00:39,  2.35it/s]Llama.generate: 267 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.77 ms /   228 tokens\n",
      " 56%|█████▋    | 117/208 [00:55<00:42,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     431.09 ms /   209 tokens\n",
      " 57%|█████▋    | 118/208 [00:55<00:41,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     367.34 ms /   196 tokens\n",
      " 57%|█████▋    | 119/208 [00:56<00:38,  2.32it/s]Llama.generate: 274 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     368.32 ms /   192 tokens\n",
      " 58%|█████▊    | 120/208 [00:56<00:36,  2.42it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     567.48 ms /   227 tokens\n",
      " 58%|█████▊    | 121/208 [00:57<00:39,  2.18it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     456.53 ms /   218 tokens\n",
      " 59%|█████▊    | 122/208 [00:57<00:39,  2.18it/s]Llama.generate: 278 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     482.29 ms /   205 tokens\n",
      " 59%|█████▉    | 123/208 [00:58<00:39,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     315.37 ms /   187 tokens\n",
      " 60%|█████▉    | 124/208 [00:58<00:35,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.39 ms /   198 tokens\n",
      " 60%|██████    | 125/208 [00:58<00:34,  2.40it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.79 ms /   201 tokens\n",
      " 61%|██████    | 126/208 [00:59<00:34,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     598.84 ms /   233 tokens\n",
      " 61%|██████    | 127/208 [00:59<00:38,  2.10it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     521.19 ms /   216 tokens\n",
      " 62%|██████▏   | 128/208 [01:00<00:39,  2.04it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.53 ms /   205 tokens\n",
      " 62%|██████▏   | 129/208 [01:00<00:37,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.29 ms /   190 tokens\n",
      " 62%|██████▎   | 130/208 [01:01<00:35,  2.22it/s]Llama.generate: 267 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     518.46 ms /   218 tokens\n",
      " 63%|██████▎   | 131/208 [01:01<00:36,  2.12it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.14 ms /   200 tokens\n",
      " 63%|██████▎   | 132/208 [01:02<00:34,  2.21it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.02 ms /   217 tokens\n",
      " 64%|██████▍   | 133/208 [01:02<00:35,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     486.07 ms /   211 tokens\n",
      " 64%|██████▍   | 134/208 [01:03<00:35,  2.09it/s]Llama.generate: 270 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     546.73 ms /   222 tokens\n",
      " 65%|██████▍   | 135/208 [01:03<00:36,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.99 ms /   202 tokens\n",
      " 65%|██████▌   | 136/208 [01:04<00:34,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.91 ms /   200 tokens\n",
      " 66%|██████▌   | 137/208 [01:04<00:32,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     342.62 ms /   194 tokens\n",
      " 66%|██████▋   | 138/208 [01:05<00:29,  2.35it/s]Llama.generate: 267 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     320.21 ms /   187 tokens\n",
      " 67%|██████▋   | 139/208 [01:05<00:27,  2.54it/s]Llama.generate: 267 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     352.93 ms /   194 tokens\n",
      " 67%|██████▋   | 140/208 [01:05<00:25,  2.62it/s]Llama.generate: 267 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     579.25 ms /   230 tokens\n",
      " 68%|██████▊   | 141/208 [01:06<00:29,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.62 ms /   206 tokens\n",
      " 68%|██████▊   | 142/208 [01:06<00:29,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     635.73 ms /   244 tokens\n",
      " 69%|██████▉   | 143/208 [01:07<00:32,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     600.93 ms /   233 tokens\n",
      " 69%|██████▉   | 144/208 [01:07<00:33,  1.88it/s]Llama.generate: 267 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     319.68 ms /   186 tokens\n",
      " 70%|██████▉   | 145/208 [01:08<00:29,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     372.23 ms /   195 tokens\n",
      " 70%|███████   | 146/208 [01:08<00:27,  2.27it/s]Llama.generate: 267 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     518.75 ms /   218 tokens\n",
      " 71%|███████   | 147/208 [01:09<00:28,  2.15it/s]Llama.generate: 267 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     665.76 ms /   243 tokens\n",
      " 71%|███████   | 148/208 [01:09<00:31,  1.90it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.40 ms /   205 tokens\n",
      " 72%|███████▏  | 149/208 [01:10<00:29,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     341.60 ms /   193 tokens\n",
      " 72%|███████▏  | 150/208 [01:10<00:26,  2.21it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     490.98 ms /   211 tokens\n",
      " 73%|███████▎  | 151/208 [01:11<00:26,  2.15it/s]Llama.generate: 267 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   309 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     700.82 ms /   319 tokens\n",
      " 73%|███████▎  | 152/208 [01:11<00:30,  1.86it/s]Llama.generate: 267 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.55 ms /   218 tokens\n",
      " 74%|███████▎  | 153/208 [01:12<00:29,  1.88it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     489.37 ms /   210 tokens\n",
      " 74%|███████▍  | 154/208 [01:12<00:28,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     490.72 ms /   207 tokens\n",
      " 75%|███████▍  | 155/208 [01:13<00:27,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.06 ms /   197 tokens\n",
      " 75%|███████▌  | 156/208 [01:13<00:25,  2.07it/s]Llama.generate: 275 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     371.97 ms /   187 tokens\n",
      " 75%|███████▌  | 157/208 [01:14<00:22,  2.22it/s]Llama.generate: 267 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     342.17 ms /   188 tokens\n",
      " 76%|███████▌  | 158/208 [01:14<00:20,  2.39it/s]Llama.generate: 267 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     371.69 ms /   193 tokens\n",
      " 76%|███████▋  | 159/208 [01:14<00:19,  2.47it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.28 ms /   202 tokens\n",
      " 77%|███████▋  | 160/208 [01:15<00:19,  2.41it/s]Llama.generate: 267 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.16 ms /   218 tokens\n",
      " 77%|███████▋  | 161/208 [01:15<00:19,  2.36it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     493.78 ms /   208 tokens\n",
      " 78%|███████▊  | 162/208 [01:16<00:20,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     343.63 ms /   191 tokens\n",
      " 78%|███████▊  | 163/208 [01:16<00:18,  2.41it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     573.19 ms /   222 tokens\n",
      " 79%|███████▉  | 164/208 [01:17<00:20,  2.16it/s]Llama.generate: 267 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.10 ms /   195 tokens\n",
      " 79%|███████▉  | 165/208 [01:17<00:19,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     437.56 ms /   200 tokens\n",
      " 80%|███████▉  | 166/208 [01:17<00:18,  2.26it/s]Llama.generate: 276 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.40 ms /   191 tokens\n",
      " 80%|████████  | 167/208 [01:18<00:17,  2.32it/s]Llama.generate: 267 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     372.17 ms /   193 tokens\n",
      " 81%|████████  | 168/208 [01:18<00:16,  2.42it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.98 ms /   199 tokens\n",
      " 81%|████████▏ | 169/208 [01:19<00:16,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     507.76 ms /   232 tokens\n",
      " 82%|████████▏ | 170/208 [01:19<00:17,  2.23it/s]Llama.generate: 267 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     435.49 ms /   198 tokens\n",
      " 82%|████████▏ | 171/208 [01:20<00:16,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     370.76 ms /   187 tokens\n",
      " 83%|████████▎ | 172/208 [01:20<00:15,  2.36it/s]Llama.generate: 267 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     486.29 ms /   259 tokens\n",
      " 83%|████████▎ | 173/208 [01:20<00:15,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     344.94 ms /   186 tokens\n",
      " 84%|████████▎ | 174/208 [01:21<00:14,  2.42it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     515.20 ms /   202 tokens\n",
      " 84%|████████▍ | 175/208 [01:21<00:14,  2.25it/s]Llama.generate: 267 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     339.71 ms /   182 tokens\n",
      " 85%|████████▍ | 176/208 [01:22<00:13,  2.42it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     490.81 ms /   202 tokens\n",
      " 85%|████████▌ | 177/208 [01:22<00:13,  2.29it/s]Llama.generate: 277 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     423.28 ms /   190 tokens\n",
      " 86%|████████▌ | 178/208 [01:23<00:13,  2.31it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     463.37 ms /   209 tokens\n",
      " 86%|████████▌ | 179/208 [01:23<00:12,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     566.54 ms /   219 tokens\n",
      " 87%|████████▋ | 180/208 [01:24<00:13,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     476.73 ms /   187 tokens\n",
      " 87%|████████▋ | 181/208 [01:24<00:12,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 343 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   343 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     585.27 ms /   347 tokens\n",
      " 88%|████████▊ | 182/208 [01:25<00:13,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     571.52 ms /   216 tokens\n",
      " 88%|████████▊ | 183/208 [01:25<00:13,  1.88it/s]Llama.generate: 267 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.37 ms /   192 tokens\n",
      " 88%|████████▊ | 184/208 [01:26<00:11,  2.03it/s]Llama.generate: 267 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     370.45 ms /   185 tokens\n",
      " 89%|████████▉ | 185/208 [01:26<00:10,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     543.62 ms /   206 tokens\n",
      " 89%|████████▉ | 186/208 [01:27<00:10,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     511.94 ms /   215 tokens\n",
      " 90%|████████▉ | 187/208 [01:27<00:10,  2.03it/s]Llama.generate: 267 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   242 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     483.90 ms /   247 tokens\n",
      " 90%|█████████ | 188/208 [01:28<00:09,  2.04it/s]Llama.generate: 267 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   174 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     343.03 ms /   177 tokens\n",
      " 91%|█████████ | 189/208 [01:28<00:08,  2.24it/s]Llama.generate: 267 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   180 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     369.44 ms /   184 tokens\n",
      " 91%|█████████▏| 190/208 [01:28<00:07,  2.36it/s]Llama.generate: 267 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.86 ms /   189 tokens\n",
      " 92%|█████████▏| 191/208 [01:29<00:07,  2.40it/s]Llama.generate: 267 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   182 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.53 ms /   187 tokens\n",
      " 92%|█████████▏| 192/208 [01:29<00:06,  2.43it/s]Llama.generate: 267 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.78 ms /   198 tokens\n",
      " 93%|█████████▎| 193/208 [01:30<00:06,  2.36it/s]Llama.generate: 267 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   246 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     596.46 ms /   255 tokens\n",
      " 93%|█████████▎| 194/208 [01:30<00:06,  2.10it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     601.28 ms /   211 tokens\n",
      " 94%|█████████▍| 195/208 [01:31<00:06,  1.94it/s]Llama.generate: 267 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   274 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     499.89 ms /   278 tokens\n",
      " 94%|█████████▍| 196/208 [01:31<00:06,  1.96it/s]Llama.generate: 267 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     346.12 ms /   184 tokens\n",
      " 95%|█████████▍| 197/208 [01:32<00:05,  2.16it/s]Llama.generate: 267 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     458.92 ms /   243 tokens\n",
      " 95%|█████████▌| 198/208 [01:32<00:04,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     336.49 ms /   148 tokens\n",
      " 96%|█████████▌| 199/208 [01:32<00:03,  2.35it/s]Llama.generate: 267 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   273 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     691.33 ms /   284 tokens\n",
      " 96%|█████████▌| 200/208 [01:33<00:04,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     608.41 ms /   267 tokens\n",
      " 97%|█████████▋| 201/208 [01:34<00:03,  1.86it/s]Llama.generate: 267 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   147 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.07 ms /   153 tokens\n",
      " 97%|█████████▋| 202/208 [01:34<00:02,  2.03it/s]Llama.generate: 267 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   225 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     751.09 ms /   240 tokens\n",
      " 98%|█████████▊| 203/208 [01:35<00:02,  1.75it/s]Llama.generate: 267 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.83 ms /   184 tokens\n",
      " 98%|█████████▊| 204/208 [01:35<00:02,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   168 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.90 ms /   173 tokens\n",
      " 99%|█████████▊| 205/208 [01:36<00:01,  2.06it/s]Llama.generate: 267 prefix-match hit, remaining 264 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   264 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     633.80 ms /   273 tokens\n",
      " 99%|█████████▉| 206/208 [01:36<00:01,  1.89it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.16 ms /   199 tokens\n",
      "100%|█████████▉| 207/208 [01:37<00:00,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   175 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.00 ms /   182 tokens\n",
      "100%|██████████| 208/208 [01:37<00:00,  2.13it/s]\n",
      "2025-06-11 18:47:08,772 - BERTopic - Representation - Completed ✓\n",
      "  0%|          | 0/206 [00:00<?, ?it/s]Llama.generate: 267 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     669.79 ms /   242 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "  0%|          | 1/206 [00:00<02:19,  1.47it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.30 ms /   217 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "  1%|          | 2/206 [00:01<02:00,  1.69it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.88 ms /   208 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "  1%|▏         | 3/206 [00:01<01:46,  1.91it/s]Llama.generate: 267 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   229 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     667.82 ms /   241 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "  2%|▏         | 4/206 [00:02<01:57,  1.71it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.20 ms /   208 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "  2%|▏         | 5/206 [00:02<01:47,  1.87it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     570.57 ms /   227 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "  3%|▎         | 6/206 [00:03<01:50,  1.81it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.13 ms /   203 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "  3%|▎         | 7/206 [00:03<01:42,  1.93it/s]Llama.generate: 267 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     481.44 ms /   231 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "  4%|▍         | 8/206 [00:04<01:40,  1.96it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     517.70 ms /   217 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "  4%|▍         | 9/206 [00:04<01:41,  1.94it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     571.54 ms /   227 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "  5%|▍         | 10/206 [00:05<01:44,  1.87it/s]Llama.generate: 280 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     623.30 ms /   217 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "  5%|▌         | 11/206 [00:06<01:50,  1.77it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     446.06 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "  6%|▌         | 12/206 [00:06<01:43,  1.88it/s]Llama.generate: 276 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.87 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "  6%|▋         | 13/206 [00:06<01:38,  1.97it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     602.81 ms /   228 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "  7%|▋         | 14/206 [00:07<01:43,  1.85it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.77 ms /   212 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "  7%|▋         | 15/206 [00:08<01:38,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     672.28 ms /   238 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "  8%|▊         | 16/206 [00:08<01:47,  1.77it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.80 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "  8%|▊         | 17/206 [00:09<01:39,  1.89it/s]Llama.generate: 276 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.61 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "  9%|▊         | 18/206 [00:09<01:34,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   229 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     671.59 ms /   241 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "  9%|▉         | 19/206 [00:10<01:44,  1.79it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     550.35 ms /   217 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      " 10%|▉         | 20/206 [00:10<01:43,  1.79it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     470.61 ms /   218 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      " 10%|█         | 21/206 [00:11<01:39,  1.87it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     553.23 ms /   216 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      " 11%|█         | 22/206 [00:11<01:40,  1.84it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     444.25 ms /   207 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      " 11%|█         | 23/206 [00:12<01:34,  1.93it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.22 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      " 12%|█▏        | 24/206 [00:12<01:30,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     437.18 ms /   209 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      " 12%|█▏        | 25/206 [00:13<01:27,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     384.13 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      " 13%|█▎        | 26/206 [00:13<01:21,  2.20it/s]Llama.generate: 267 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   229 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     668.74 ms /   241 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      " 13%|█▎        | 27/206 [00:14<01:33,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     546.24 ms /   217 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      " 14%|█▎        | 28/206 [00:14<01:34,  1.88it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     603.62 ms /   230 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      " 14%|█▍        | 29/206 [00:15<01:38,  1.80it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.08 ms /   208 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 15%|█▍        | 30/206 [00:15<01:32,  1.91it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     384.41 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      " 15%|█▌        | 31/206 [00:16<01:24,  2.06it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.35 ms /   215 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      " 16%|█▌        | 32/206 [00:16<01:26,  2.01it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     521.32 ms /   216 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      " 16%|█▌        | 33/206 [00:17<01:27,  1.97it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     468.29 ms /   215 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
      " 17%|█▋        | 34/206 [00:17<01:25,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.01 ms /   211 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 17%|█▋        | 35/206 [00:18<01:22,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.10 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      " 17%|█▋        | 36/206 [00:18<01:17,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     594.95 ms /   228 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      " 18%|█▊        | 37/206 [00:19<01:24,  1.99it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.90 ms /   203 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      " 18%|█▊        | 38/206 [00:19<01:18,  2.13it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.40 ms /   205 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      " 19%|█▉        | 39/206 [00:20<01:17,  2.15it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.57 ms /   213 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 19%|█▉        | 40/206 [00:20<01:16,  2.18it/s]Llama.generate: 267 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     594.56 ms /   226 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      " 20%|█▉        | 41/206 [00:21<01:23,  1.99it/s]Llama.generate: 267 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     516.36 ms /   224 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      " 20%|██        | 42/206 [00:21<01:23,  1.96it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     437.91 ms /   209 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 21%|██        | 43/206 [00:22<01:19,  2.04it/s]Llama.generate: 267 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     597.98 ms /   226 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      " 21%|██▏       | 44/206 [00:22<01:25,  1.90it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     382.53 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      " 22%|██▏       | 45/206 [00:23<01:18,  2.06it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.38 ms /   205 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 22%|██▏       | 46/206 [00:23<01:15,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     524.93 ms /   216 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      " 23%|██▎       | 47/206 [00:24<01:18,  2.03it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     382.99 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      " 23%|██▎       | 48/206 [00:24<01:13,  2.16it/s]Llama.generate: 267 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     592.57 ms /   227 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      " 24%|██▍       | 49/206 [00:25<01:19,  1.98it/s]Llama.generate: 314 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   169 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     529.61 ms /   179 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      " 24%|██▍       | 50/206 [00:25<01:20,  1.94it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     517.73 ms /   214 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      " 25%|██▍       | 51/206 [00:26<01:20,  1.93it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     464.29 ms /   214 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      " 25%|██▌       | 52/206 [00:26<01:17,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.85 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      " 26%|██▌       | 53/206 [00:27<01:13,  2.09it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.80 ms /   215 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      " 26%|██▌       | 54/206 [00:27<01:15,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.15 ms /   204 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 27%|██▋       | 55/206 [00:28<01:12,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.50 ms /   213 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      " 27%|██▋       | 56/206 [00:28<01:14,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     577.48 ms /   229 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      " 28%|██▊       | 57/206 [00:29<01:17,  1.91it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.67 ms /   209 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 28%|██▊       | 58/206 [00:29<01:14,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     468.54 ms /   213 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
      " 29%|██▊       | 59/206 [00:30<01:12,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     572.82 ms /   231 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      " 29%|██▉       | 60/206 [00:30<01:15,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.30 ms /   211 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 30%|██▉       | 61/206 [00:31<01:12,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     493.76 ms /   212 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      " 30%|███       | 62/206 [00:31<01:12,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.30 ms /   204 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 31%|███       | 63/206 [00:32<01:09,  2.06it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.96 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      " 31%|███       | 64/206 [00:32<01:07,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     382.69 ms /   198 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      " 32%|███▏      | 65/206 [00:32<01:03,  2.23it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     471.41 ms /   213 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      " 32%|███▏      | 66/206 [00:33<01:04,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.16 ms /   215 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      " 33%|███▎      | 67/206 [00:33<01:06,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     383.18 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      " 33%|███▎      | 68/206 [00:34<01:02,  2.20it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.23 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 33%|███▎      | 69/206 [00:34<01:01,  2.21it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     465.06 ms /   212 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      " 34%|███▍      | 70/206 [00:35<01:02,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     572.90 ms /   230 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      " 34%|███▍      | 71/206 [00:35<01:07,  2.01it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     464.65 ms /   215 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      " 35%|███▍      | 72/206 [00:36<01:05,  2.04it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.52 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 35%|███▌      | 73/206 [00:36<01:03,  2.09it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.23 ms /   197 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      " 36%|███▌      | 74/206 [00:37<00:59,  2.20it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     493.37 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      " 36%|███▋      | 75/206 [00:37<01:01,  2.13it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.97 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      " 37%|███▋      | 76/206 [00:38<00:59,  2.20it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     380.97 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      " 37%|███▋      | 77/206 [00:38<00:56,  2.30it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.18 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      " 38%|███▊      | 78/206 [00:38<00:55,  2.32it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     577.66 ms /   229 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      " 38%|███▊      | 79/206 [00:39<01:00,  2.09it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.82 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      " 39%|███▉      | 80/206 [00:39<00:57,  2.18it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     383.57 ms /   198 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      " 39%|███▉      | 81/206 [00:40<00:54,  2.27it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.98 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 40%|███▉      | 82/206 [00:40<00:54,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     549.32 ms /   224 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      " 40%|████      | 83/206 [00:41<00:58,  2.09it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.21 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      " 41%|████      | 84/206 [00:41<00:56,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.05 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      " 41%|████▏     | 85/206 [00:42<00:54,  2.23it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     544.40 ms /   209 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      " 42%|████▏     | 86/206 [00:42<00:57,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.19 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      " 42%|████▏     | 87/206 [00:43<00:54,  2.20it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     577.29 ms /   229 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      " 43%|████▎     | 88/206 [00:43<00:58,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     495.70 ms /   219 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      " 43%|████▎     | 89/206 [00:44<00:58,  2.01it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.90 ms /   204 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      " 44%|████▎     | 90/206 [00:44<00:55,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.04 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      " 44%|████▍     | 91/206 [00:45<00:52,  2.18it/s]Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.19 ms /   204 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      " 45%|████▍     | 92/206 [00:45<00:52,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     570.43 ms /   228 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      " 45%|████▌     | 93/206 [00:46<00:55,  2.02it/s]Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.67 ms /   204 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 46%|████▌     | 94/206 [00:46<00:53,  2.08it/s]Llama.generate: 299 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     475.09 ms /   175 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      " 46%|████▌     | 95/206 [00:47<00:53,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.76 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      " 47%|████▋     | 96/206 [00:47<00:50,  2.16it/s]Llama.generate: 267 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     570.07 ms /   225 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      " 47%|████▋     | 97/206 [00:48<00:54,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     517.24 ms /   213 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      " 48%|████▊     | 98/206 [00:48<00:54,  1.97it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.34 ms /   203 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 48%|████▊     | 99/206 [00:49<00:52,  2.04it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.73 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      " 49%|████▊     | 100/206 [00:49<00:49,  2.13it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     433.18 ms /   209 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      " 49%|████▉     | 101/206 [00:49<00:48,  2.17it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.13 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      " 50%|████▉     | 102/206 [00:50<00:46,  2.22it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.30 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      " 50%|█████     | 103/206 [00:50<00:44,  2.30it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     384.51 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      " 50%|█████     | 104/206 [00:51<00:43,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.79 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      " 51%|█████     | 105/206 [00:51<00:42,  2.36it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     493.81 ms /   219 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      " 51%|█████▏    | 106/206 [00:52<00:44,  2.23it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     362.63 ms /   197 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      " 52%|█████▏    | 107/206 [00:52<00:42,  2.35it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.94 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      " 52%|█████▏    | 108/206 [00:52<00:41,  2.39it/s]Llama.generate: 267 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.52 ms /   222 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      " 53%|█████▎    | 109/206 [00:53<00:45,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     583.91 ms /   223 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      " 53%|█████▎    | 110/206 [00:53<00:48,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.59 ms /   198 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      " 54%|█████▍    | 111/206 [00:54<00:44,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     444.59 ms /   205 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      " 54%|█████▍    | 112/206 [00:54<00:43,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     362.57 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      " 55%|█████▍    | 113/206 [00:55<00:40,  2.28it/s]Llama.generate: 267 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.57 ms /   196 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      " 55%|█████▌    | 114/206 [00:55<00:39,  2.33it/s]Llama.generate: 267 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     525.52 ms /   222 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      " 56%|█████▌    | 115/206 [00:56<00:41,  2.17it/s]Llama.generate: 297 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     489.56 ms /   191 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      " 56%|█████▋    | 116/206 [00:56<00:42,  2.11it/s]Llama.generate: 297 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     481.60 ms /   189 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      " 57%|█████▋    | 117/206 [00:57<00:42,  2.09it/s]Llama.generate: 297 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.83 ms /   191 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      " 57%|█████▋    | 118/206 [00:57<00:41,  2.11it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     437.79 ms /   211 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      " 58%|█████▊    | 119/206 [00:58<00:40,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.88 ms /   198 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      " 58%|█████▊    | 120/206 [00:58<00:38,  2.24it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.12 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      " 59%|█████▊    | 121/206 [00:58<00:37,  2.28it/s]Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     492.96 ms /   215 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      " 59%|█████▉    | 122/206 [00:59<00:38,  2.18it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.42 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 60%|█████▉    | 123/206 [00:59<00:37,  2.19it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     361.74 ms /   197 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      " 60%|██████    | 124/206 [01:00<00:35,  2.32it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     443.40 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      " 61%|██████    | 125/206 [01:00<00:35,  2.28it/s]Llama.generate: 267 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     443.40 ms /   211 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      " 61%|██████    | 126/206 [01:01<00:35,  2.26it/s]Llama.generate: 289 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.04 ms /   188 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      " 62%|██████▏   | 127/206 [01:01<00:34,  2.30it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.50 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 62%|██████▏   | 128/206 [01:01<00:34,  2.27it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.21 ms /   209 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      " 63%|██████▎   | 129/206 [01:02<00:35,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.69 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 63%|██████▎   | 130/206 [01:02<00:35,  2.16it/s]Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.84 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      " 64%|██████▎   | 131/206 [01:03<00:33,  2.26it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.37 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      " 64%|██████▍   | 132/206 [01:03<00:33,  2.24it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.35 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      " 65%|██████▍   | 133/206 [01:04<00:31,  2.31it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.45 ms /   198 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      " 65%|██████▌   | 134/206 [01:04<00:31,  2.32it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.37 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      " 66%|██████▌   | 135/206 [01:05<00:29,  2.37it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.42 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      " 66%|██████▌   | 136/206 [01:05<00:29,  2.36it/s]Llama.generate: 273 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     373.62 ms /   193 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
      " 67%|██████▋   | 137/206 [01:05<00:28,  2.43it/s]Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.30 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      " 67%|██████▋   | 138/206 [01:06<00:27,  2.46it/s]Llama.generate: 267 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.87 ms /   198 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      " 67%|██████▋   | 139/206 [01:06<00:27,  2.43it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     598.73 ms /   230 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      " 68%|██████▊   | 140/206 [01:07<00:31,  2.12it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.42 ms /   200 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      " 68%|██████▊   | 141/206 [01:07<00:29,  2.18it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     443.89 ms /   203 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      " 69%|██████▉   | 142/206 [01:08<00:29,  2.18it/s]Llama.generate: 267 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   251 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     487.83 ms /   256 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      " 69%|██████▉   | 143/206 [01:08<00:29,  2.13it/s]Llama.generate: 267 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.37 ms /   195 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      " 70%|██████▉   | 144/206 [01:09<00:28,  2.14it/s]Llama.generate: 267 prefix-match hit, remaining 343 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   343 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     857.42 ms /   357 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      " 70%|███████   | 145/206 [01:09<00:35,  1.71it/s]Llama.generate: 267 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   150 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     467.85 ms /   159 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      " 71%|███████   | 146/206 [01:10<00:33,  1.81it/s]Llama.generate: 267 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.09 ms /   229 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 71%|███████▏  | 147/206 [01:10<00:30,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.22 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      " 72%|███████▏  | 148/206 [01:11<00:30,  1.90it/s]Llama.generate: 267 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     693.22 ms /   249 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      " 72%|███████▏  | 149/206 [01:12<00:32,  1.73it/s]Llama.generate: 267 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   252 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     487.72 ms /   257 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      " 73%|███████▎  | 150/206 [01:12<00:31,  1.81it/s]Llama.generate: 400 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     478.28 ms /   117 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      " 73%|███████▎  | 151/206 [01:13<00:29,  1.87it/s]Llama.generate: 267 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.47 ms /   193 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      " 74%|███████▍  | 152/206 [01:13<00:28,  1.89it/s]Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     442.06 ms /   207 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      " 74%|███████▍  | 153/206 [01:14<00:26,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     478.54 ms /   233 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      " 75%|███████▍  | 154/206 [01:14<00:26,  2.00it/s]Llama.generate: 267 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   241 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     510.52 ms /   247 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      " 75%|███████▌  | 155/206 [01:15<00:25,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   173 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     424.19 ms /   179 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      " 76%|███████▌  | 156/206 [01:15<00:24,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   317 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     574.59 ms /   322 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      " 76%|███████▌  | 157/206 [01:16<00:25,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.53 ms /   207 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      " 77%|███████▋  | 158/206 [01:16<00:23,  2.05it/s]Llama.generate: 267 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     633.56 ms /   268 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      " 77%|███████▋  | 159/206 [01:17<00:25,  1.87it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.07 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      " 78%|███████▊  | 160/206 [01:17<00:22,  2.03it/s]Llama.generate: 267 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     579.21 ms /   220 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      " 78%|███████▊  | 161/206 [01:18<00:23,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   345 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     771.85 ms /   356 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      " 79%|███████▊  | 162/206 [01:18<00:26,  1.67it/s]Llama.generate: 267 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     466.15 ms /   219 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      " 79%|███████▉  | 163/206 [01:19<00:24,  1.78it/s]Llama.generate: 267 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     585.00 ms /   179 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      " 80%|███████▉  | 164/206 [01:20<00:24,  1.75it/s]Llama.generate: 267 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     546.24 ms /   265 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      " 80%|████████  | 165/206 [01:20<00:23,  1.76it/s]Llama.generate: 301 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     521.81 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      " 81%|████████  | 166/206 [01:21<00:22,  1.80it/s]Llama.generate: 267 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   157 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     362.55 ms /   162 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      " 81%|████████  | 167/206 [01:21<00:19,  1.99it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     416.23 ms /   209 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      " 82%|████████▏ | 168/206 [01:21<00:18,  2.08it/s]Llama.generate: 267 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   180 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     476.83 ms /   188 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      " 82%|████████▏ | 169/206 [01:22<00:17,  2.07it/s]Llama.generate: 279 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.44 ms /   168 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 83%|████████▎ | 170/206 [01:22<00:17,  2.12it/s]Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     579.98 ms /   176 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      " 83%|████████▎ | 171/206 [01:23<00:17,  1.97it/s]Llama.generate: 268 prefix-match hit, remaining 171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   171 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     609.29 ms /   184 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      " 83%|████████▎ | 172/206 [01:24<00:18,  1.85it/s]Llama.generate: 267 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     557.94 ms /   234 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      " 84%|████████▍ | 173/206 [01:24<00:18,  1.82it/s]Llama.generate: 267 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.41 ms /   203 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      " 84%|████████▍ | 174/206 [01:25<00:16,  1.93it/s]Llama.generate: 267 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   234 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     483.78 ms /   239 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      " 85%|████████▍ | 175/206 [01:25<00:15,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     467.26 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      " 85%|████████▌ | 176/206 [01:26<00:15,  1.98it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.72 ms /   225 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      " 86%|████████▌ | 177/206 [01:26<00:14,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   170 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     554.63 ms /   181 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      " 86%|████████▋ | 178/206 [01:27<00:14,  1.89it/s]Llama.generate: 267 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   253 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     648.68 ms /   264 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      " 87%|████████▋ | 179/206 [01:27<00:15,  1.76it/s]Llama.generate: 267 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   175 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     476.39 ms /   183 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      " 87%|████████▋ | 180/206 [01:28<00:14,  1.84it/s]Llama.generate: 267 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.84 ms /   190 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      " 88%|████████▊ | 181/206 [01:28<00:12,  1.93it/s]Llama.generate: 267 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     478.85 ms /   198 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      " 88%|████████▊ | 182/206 [01:29<00:12,  1.96it/s]Llama.generate: 267 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   235 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     617.48 ms /   245 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      " 89%|████████▉ | 183/206 [01:29<00:12,  1.83it/s]Llama.generate: 267 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   174 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     370.74 ms /   178 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      " 89%|████████▉ | 184/206 [01:30<00:10,  2.01it/s]Llama.generate: 267 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   251 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.67 ms /   259 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      " 90%|████████▉ | 185/206 [01:30<00:10,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   174 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.07 ms /   181 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      " 90%|█████████ | 186/206 [01:31<00:10,  1.98it/s]Llama.generate: 285 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     417.52 ms /   163 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      " 91%|█████████ | 187/206 [01:31<00:09,  2.07it/s]Llama.generate: 267 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     600.98 ms /   228 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      " 91%|█████████▏| 188/206 [01:32<00:09,  1.92it/s]Llama.generate: 267 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   175 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     340.67 ms /   178 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      " 92%|█████████▏| 189/206 [01:32<00:08,  2.12it/s]Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.87 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      " 92%|█████████▏| 190/206 [01:33<00:07,  2.23it/s]Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     598.67 ms /   223 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      " 93%|█████████▎| 191/206 [01:33<00:07,  2.01it/s]Llama.generate: 267 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     533.11 ms /   238 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      " 93%|█████████▎| 192/206 [01:34<00:07,  1.95it/s]Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     511.07 ms /   211 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      " 94%|█████████▎| 193/206 [01:34<00:06,  1.94it/s]Llama.generate: 291 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   169 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     422.73 ms /   175 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      " 94%|█████████▍| 194/206 [01:35<00:05,  2.04it/s]Llama.generate: 267 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   229 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     589.35 ms /   238 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      " 95%|█████████▍| 195/206 [01:35<00:05,  1.91it/s]Llama.generate: 285 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     461.45 ms /   218 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
      " 95%|█████████▌| 196/206 [01:36<00:05,  1.96it/s]Llama.generate: 283 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     435.52 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      " 96%|█████████▌| 197/206 [01:36<00:04,  2.03it/s]Llama.generate: 283 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     466.58 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      " 96%|█████████▌| 198/206 [01:37<00:03,  2.05it/s]Llama.generate: 285 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     543.07 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      " 97%|█████████▋| 199/206 [01:37<00:03,  1.96it/s]Llama.generate: 267 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.54 ms /   192 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      " 97%|█████████▋| 200/206 [01:38<00:02,  2.09it/s]Llama.generate: 267 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     466.53 ms /   225 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      " 98%|█████████▊| 201/206 [01:38<00:02,  2.09it/s]Llama.generate: 285 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.69 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      " 98%|█████████▊| 202/206 [01:39<00:01,  2.12it/s]Llama.generate: 285 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.37 ms /   205 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      " 99%|█████████▊| 203/206 [01:39<00:01,  2.14it/s]Llama.generate: 285 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.78 ms /   203 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      " 99%|█████████▉| 204/206 [01:40<00:00,  2.20it/s]Llama.generate: 285 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.34 ms /   207 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|█████████▉| 205/206 [01:40<00:00,  2.20it/s]Llama.generate: 285 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.65 ms /   204 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|██████████| 206/206 [01:40<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_init.initate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     822.16 ms /   215 tokens\n",
      " 10%|█         | 1/10 [00:00<00:08,  1.01it/s]Llama.generate: 156 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     423.14 ms /    68 tokens\n",
      " 20%|██        | 2/10 [00:01<00:06,  1.32it/s]Llama.generate: 156 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.50 ms /    75 tokens\n",
      " 30%|███       | 3/10 [00:02<00:04,  1.45it/s]Llama.generate: 156 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     327.68 ms /    61 tokens\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.65it/s]Llama.generate: 160 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     237.68 ms /    63 tokens\n",
      " 50%|█████     | 5/10 [00:03<00:02,  1.88it/s]Llama.generate: 156 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     569.96 ms /    82 tokens\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.62it/s]Llama.generate: 156 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     298.46 ms /    62 tokens\n",
      " 70%|███████   | 7/10 [00:04<00:01,  1.79it/s]Llama.generate: 156 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     490.56 ms /    74 tokens\n",
      " 80%|████████  | 8/10 [00:04<00:01,  1.67it/s]Llama.generate: 161 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     697.17 ms /   108 tokens\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.43it/s]Llama.generate: 161 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1314.29 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     543.84 ms /    71 tokens\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a patient undergoing a protocol event',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Patient', relation='undergo', target='Protocol Event', link=SubjectLink(link_id=404, label='undergo', from_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/undergo>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099), instance_count=50251))], entities=[EnrichedEntity(identifier='Patient', type='Patient', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094)), EnrichedEntity(identifier='Protocol Event', type='Protocol Event', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099))], message='Found Relations and Entities')),\n",
       " ('The Protocol Event has registered an activity related to Smoking.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Protocol Event', relation='hasRegisteredActivity', target='Smoking', link=SubjectLink(link_id=227, label='hasRegisteredActivity', from_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/Smoking>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/hasRegisteredActivity>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Smoking>', label='Smoking', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Smoking', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Activity>', label='Activity')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=1737), instance_count=95395))], entities=[EnrichedEntity(identifier='Protocol Event', type='Protocol Event', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099)), EnrichedEntity(identifier='Smoking', type='Smoking', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Smoking>', label='Smoking', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Smoking', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Activity>', label='Activity')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=1737))], message='Found Relations and Entities')),\n",
       " ('Before Onset has registered activity related to physical activity data.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Before Onset', relation='hasRegisteredActivity', target='Physical Activity Data', link=SubjectLink(link_id=227, label='hasRegisteredActivity', from_id='<https://w3id.org/brainteaser/ontology/schema/BeforeOnset>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/PhysicalActivityData>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/hasRegisteredActivity>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/BeforeOnset>', label='Before Onset', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Before Onset', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=2074), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/PhysicalActivityData>', label='Physical Activity Data', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Physical Activity Data', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=21343), instance_count=95395))], entities=[EnrichedEntity(identifier='Before Onset', type='Before Onset', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/BeforeOnset>', label='Before Onset', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Before Onset', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=2074)), EnrichedEntity(identifier='Physical Activity Data', type='Physical Activity Data', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/PhysicalActivityData>', label='Physical Activity Data', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Physical Activity Data', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=21343))], message='Found Relations and Entities')),\n",
       " ('a patient experiencing the onset of a condition',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Patient', relation='undergo', target='Onset', link=SubjectLink(link_id=404, label='undergo', from_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/Onset>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/undergo>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Onset>', label='Onset', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Onset', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3207), instance_count=50251))], entities=[EnrichedEntity(identifier='Patient', type='Patient', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094)), EnrichedEntity(identifier='Onset', type='Onset', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Onset>', label='Onset', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Onset', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3207))], message='Found Relations and Entities')),\n",
       " ('the ethnicity of a patient',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Patient', relation='hasEthnicity', target='Group (social concept)', link=SubjectLink(link_id=399, label='hasEthnicity', from_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/Group>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/hasEthnicity>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Group>', label='Group (social concept)', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Group (social concept)', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=4), instance_count=2977))], entities=[EnrichedEntity(identifier='Patient', type='Patient', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094)), EnrichedEntity(identifier='Group (social concept)', type='Group (social concept)', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Group>', label='Group (social concept)', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Group (social concept)', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=4))], message='Found Relations and Entities')),\n",
       " ('The Protocol Event has registered an activity related to the Beat-To-Beat Data.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Protocol Event', relation='hasRegisteredActivity', target='Beat-To-Beat Data', link=SubjectLink(link_id=227, label='hasRegisteredActivity', from_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/BeatToBeatData>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/hasRegisteredActivity>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/BeatToBeatData>', label='Beat-To-Beat Data', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Beat-To-Beat Data', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20390), instance_count=95395))], entities=[EnrichedEntity(identifier='Protocol Event', type='Protocol Event', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099)), EnrichedEntity(identifier='Beat-To-Beat Data', type='Beat-To-Beat Data', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/BeatToBeatData>', label='Beat-To-Beat Data', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Beat-To-Beat Data', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20390))], message='Found Relations and Entities')),\n",
       " ('a patient who has undergone genetic testing',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Patient', relation='tested', target='Genetic Testing', link=SubjectLink(link_id=403, label='tested', from_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/GeneticTesting>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/tested>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/GeneticTesting>', label='Genetic Testing', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Genetic Testing', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=5094), instance_count=5094))], entities=[EnrichedEntity(identifier='Patient', type='Patient', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/Patient>', label='Patient', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Patient', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<http://www.w3.org/2000/10/swap/pim/contact#Person>', label='Person')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3094)), EnrichedEntity(identifier='Genetic Testing', type='Genetic Testing', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/GeneticTesting>', label='Genetic Testing', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Genetic Testing', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=5094))], message='Found Relations and Entities')),\n",
       " ('The Protocol Event is associated with a registered activity of measuring heart rate.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Protocol Event', relation='hasRegisteredActivity', target='Heart Rate Measurement', link=SubjectLink(link_id=227, label='hasRegisteredActivity', from_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/HeartRateMeasurement>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/hasRegisteredActivity>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/HeartRateMeasurement>', label='Heart Rate Measurement', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Heart Rate Measurement', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20390), instance_count=95395))], entities=[EnrichedEntity(identifier='Protocol Event', type='Protocol Event', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099)), EnrichedEntity(identifier='Heart Rate Measurement', type='Heart Rate Measurement', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/HeartRateMeasurement>', label='Heart Rate Measurement', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Heart Rate Measurement', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20390))], message='Found Relations and Entities')),\n",
       " ('the ALSFRS is a part of a protocol event related to Amyotrophic Lateral Sclerosis',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Protocol Event', relation='consists', target='Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS)', link=SubjectLink(link_id=225, label='consists', from_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/ALSFRS>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/consists>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ALSFRS>', label='Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS)', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS)', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Questionnaire>', label='Questionnaire')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=5766), instance_count=25434))], entities=[EnrichedEntity(identifier='Protocol Event', type='Protocol Event', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099)), EnrichedEntity(identifier='Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS)', type='Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS)', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ALSFRS>', label='Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS)', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS)', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Questionnaire>', label='Questionnaire')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=5766))], message='Found Relations and Entities')),\n",
       " ('the Protocol Event has registered Physical Activity Data which represents the activity of physical movement.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Protocol Event', relation='hasRegisteredActivity', target='Physical Activity Data', link=SubjectLink(link_id=227, label='hasRegisteredActivity', from_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/PhysicalActivityData>', to_proptype=None, property_id='<https://w3id.org/brainteaser/ontology/schema/hasRegisteredActivity>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/PhysicalActivityData>', label='Physical Activity Data', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Physical Activity Data', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=21343), instance_count=95395))], entities=[EnrichedEntity(identifier='Protocol Event', type='Protocol Event', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/ProtocolEvent>', label='Protocol Event', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Protocol Event', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/Event>', label='Event')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=20099)), EnrichedEntity(identifier='Physical Activity Data', type='Physical Activity Data', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/PhysicalActivityData>', label='Physical Activity Data', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Physical Activity Data', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/WearableDataMeasurement>', label='Wearable Data Measurement')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=21343))], message='Found Relations and Entities'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_query.initiate(n_queries=10, force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
