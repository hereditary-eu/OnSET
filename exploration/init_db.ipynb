{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../backend\"))\n",
    "sys.path.append(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib.plugins.stores.sparqlstore import SPARQLStore\n",
    "from backend.ontology import Graph, OntologyManager, OntologyConfig\n",
    "from backend.datasetmatcher import DatasetManager\n",
    "from backend.explorative.explorative_support import GuidanceManager\n",
    "from backend.explorative.llm_query import LLMQuery\n",
    "from backend.explorative.topic_init import TopicInitator\n",
    "\n",
    "from backend.eval_config import (\n",
    "    DBPEDIA_CONFIGS,\n",
    "    OMA_CONFIGS,\n",
    "    UNIPROT_CONFIGS,\n",
    "    BTO_CONFIGS,\n",
    "    DNB_CONFIGS,\n",
    "    OLYMPICS_CONFIGS,\n",
    "    EvalConfig,\n",
    ")\n",
    "base_path = \"../data\"\n",
    "onto_path = f\"{base_path}/hero-ontology/hereditary_clinical.ttl\"\n",
    "\n",
    "\n",
    "# graph = Graph().parse(onto_path, format=\"turtle\")\n",
    "# graph.bind(\"bto\", \"http://www.semanticweb.org/ontologies/2020/3/bto#\")\n",
    "\n",
    "# store = SPARQLStore(\n",
    "#     \"http://localhost:7200/repositories/dpedia\",\n",
    "#     method=\"POST_FORM\",\n",
    "#     params={\"infer\": False, \"sameAs\": False},\n",
    "# )\n",
    "\n",
    "# store = SPARQLStore(\n",
    "#     \"http://localhost:3030/dbpedia/query\",\n",
    "#     method=\"POST_FORM\",\n",
    "#     params={\"infer\": False, \"sameAs\": False},\n",
    "# )\n",
    "setup: EvalConfig = BTO_CONFIGS[-1]\n",
    "print(\"Setup for \", setup.name)\n",
    "store = SPARQLStore(\n",
    "    setup.sparql_endpoint,\n",
    "    method=\"POST_FORM\",\n",
    "    params={\"infer\": False, \"sameAs\": False},\n",
    "    retries=10,\n",
    ")\n",
    "graph = Graph(store=store)\n",
    "\n",
    "config = OntologyConfig()\n",
    "\n",
    "ontology_manager = OntologyManager(config, graph)\n",
    "dataset_manager = DatasetManager(ontology_manager)\n",
    "dataset_manager.initialise(glob_path=\"data/datasets/ALS/**/*.csv\")\n",
    "\n",
    "# ontology_manager.load_full_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "guidance_man = GuidanceManager(\n",
    "    ontology_manager, conn_str=setup.conn_str, llm_model_id=setup.model_id\n",
    ")\n",
    "print(guidance_man.identifier)\n",
    "llm_query = LLMQuery(guidance_man)\n",
    "topic_init = TopicInitator(guidance_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_init.initate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  29%|██▉       | 68/234 [00:21<01:55,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring <https://w3id.org/brainteaser/ontology/schema/MuscleTone>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents: 100%|██████████| 234/234 [02:59<00:00,  1.31it/s]\n",
      "2025-06-05 10:21:10,148 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbb79743e2641a1bf6e4cbc5ead2bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:21:13,497 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-05 10:21:13,498 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-05 10:21:15,347 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-05 10:21:15,347 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-05 10:21:15,421 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-05 10:21:15,424 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]Llama.generate: 267 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      74.06 ms /   219 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 163 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   163 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      77.53 ms /   174 tokens\n",
      "  2%|▏         | 2/90 [00:00<00:06, 12.91it/s]Llama.generate: 267 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   157 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.17 ms /   161 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   151 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      57.10 ms /   159 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   155 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      73.78 ms /   166 tokens\n",
      "  6%|▌         | 5/90 [00:00<00:05, 15.77it/s]Llama.generate: 267 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.56 ms /   144 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.44 ms /   149 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.18 ms /   135 tokens\n",
      "  9%|▉         | 8/90 [00:00<00:04, 18.33it/s]Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.24 ms /   139 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      41.28 ms /   120 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   145 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      80.03 ms /   157 tokens\n",
      " 12%|█▏        | 11/90 [00:00<00:04, 18.24it/s]Llama.generate: 267 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.37 ms /   143 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      45.86 ms /   141 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   176 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      53.37 ms /   183 tokens\n",
      " 16%|█▌        | 14/90 [00:00<00:03, 19.15it/s]Llama.generate: 267 prefix-match hit, remaining 152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   152 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      79.64 ms /   164 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.27 ms /   139 tokens\n",
      " 18%|█▊        | 16/90 [00:00<00:04, 18.31it/s]Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.29 ms /   139 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      69.50 ms /   177 tokens\n",
      " 20%|██        | 18/90 [00:01<00:03, 18.17it/s]Llama.generate: 267 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   159 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      74.16 ms /   170 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      68.52 ms /   163 tokens\n",
      " 22%|██▏       | 20/90 [00:01<00:04, 16.67it/s]Llama.generate: 267 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      34.64 ms /   133 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.25 ms /   140 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      45.89 ms /   145 tokens\n",
      " 26%|██▌       | 23/90 [00:01<00:03, 18.84it/s]Llama.generate: 267 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   131 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.21 ms /   136 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.22 ms /   137 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.22 ms /   126 tokens\n",
      " 29%|██▉       | 26/90 [00:01<00:03, 20.62it/s]Llama.generate: 267 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   131 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.38 ms /   138 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.81 ms /   126 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.28 ms /   129 tokens\n",
      " 32%|███▏      | 29/90 [00:01<00:02, 21.31it/s]Llama.generate: 267 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      46.45 ms /   131 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.42 ms /   147 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.23 ms /   135 tokens\n",
      " 36%|███▌      | 32/90 [00:01<00:02, 21.24it/s]Llama.generate: 267 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.43 ms /   129 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      91.30 ms /   129 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      30.05 ms /   119 tokens\n",
      " 39%|███▉      | 35/90 [00:01<00:02, 20.36it/s]Llama.generate: 267 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.20 ms /   124 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      34.81 ms /   115 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      29.61 ms /   123 tokens\n",
      " 42%|████▏     | 38/90 [00:01<00:02, 22.43it/s]Llama.generate: 267 prefix-match hit, remaining 143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   143 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      52.01 ms /   150 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      34.63 ms /   133 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.24 ms /   139 tokens\n",
      " 46%|████▌     | 41/90 [00:02<00:02, 22.58it/s]Llama.generate: 267 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      57.59 ms /   150 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.27 ms /   125 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      68.05 ms /   144 tokens\n",
      " 49%|████▉     | 44/90 [00:02<00:02, 21.04it/s]Llama.generate: 267 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.59 ms /   119 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      95.87 ms /   145 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.85 ms /   132 tokens\n",
      " 52%|█████▏    | 47/90 [00:02<00:02, 19.62it/s]Llama.generate: 267 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      30.04 ms /   118 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   141 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.43 ms /   148 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.20 ms /   125 tokens\n",
      " 56%|█████▌    | 50/90 [00:02<00:01, 20.96it/s]Llama.generate: 267 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.26 ms /   129 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   344 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      90.06 ms /   356 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   173 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      47.81 ms /   179 tokens\n",
      " 59%|█████▉    | 53/90 [00:02<00:01, 19.55it/s]Llama.generate: 267 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.29 ms /   135 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.32 ms /   130 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   246 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      61.81 ms /   254 tokens\n",
      " 62%|██████▏   | 56/90 [00:02<00:01, 20.03it/s]Llama.generate: 267 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      66.04 ms /   233 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   252 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      61.46 ms /   260 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   157 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      46.87 ms /   163 tokens\n",
      " 66%|██████▌   | 59/90 [00:03<00:01, 18.92it/s]Llama.generate: 267 prefix-match hit, remaining 277 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   277 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      64.33 ms /   285 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   253 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      50.54 ms /   259 tokens\n",
      " 68%|██████▊   | 61/90 [00:03<00:01, 18.45it/s]Llama.generate: 267 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   238 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      43.46 ms /   243 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   149 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      41.31 ms /   154 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   158 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      57.23 ms /   166 tokens\n",
      " 71%|███████   | 64/90 [00:03<00:01, 19.07it/s]Llama.generate: 267 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   313 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      66.29 ms /   321 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      59.95 ms /   204 tokens\n",
      " 73%|███████▎  | 66/90 [00:03<00:01, 18.07it/s]Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   176 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      53.49 ms /   183 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      69.83 ms /   195 tokens\n",
      " 76%|███████▌  | 68/90 [00:03<00:01, 17.47it/s]Llama.generate: 267 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      60.18 ms /   187 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      52.77 ms /   173 tokens\n",
      " 78%|███████▊  | 70/90 [00:03<00:01, 17.40it/s]Llama.generate: 267 prefix-match hit, remaining 327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   327 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      88.63 ms /   339 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      66.25 ms /   248 tokens\n",
      " 80%|████████  | 72/90 [00:03<00:01, 15.79it/s]Llama.generate: 267 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      44.82 ms /   216 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      42.83 ms /   189 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   242 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      44.61 ms /   247 tokens\n",
      " 83%|████████▎ | 75/90 [00:03<00:00, 17.62it/s]Llama.generate: 267 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   246 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.34 ms /   252 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      63.03 ms /   148 tokens\n",
      " 86%|████████▌ | 77/90 [00:04<00:00, 17.47it/s]Llama.generate: 267 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      30.65 ms /   184 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      65.81 ms /   241 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 271 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   271 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      47.26 ms /   276 tokens\n",
      " 89%|████████▉ | 80/90 [00:04<00:00, 18.38it/s]Llama.generate: 267 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   267 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     103.11 ms /   282 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      37.28 ms /   226 tokens\n",
      " 91%|█████████ | 82/90 [00:04<00:00, 17.01it/s]Llama.generate: 267 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      63.34 ms /   267 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      30.05 ms /   147 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   176 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      47.81 ms /   182 tokens\n",
      " 94%|█████████▍| 85/90 [00:04<00:00, 18.14it/s]Llama.generate: 267 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      36.71 ms /   183 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   150 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.05 ms /   154 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      37.09 ms /   199 tokens\n",
      " 98%|█████████▊| 88/90 [00:04<00:00, 20.29it/s]Llama.generate: 267 prefix-match hit, remaining 264 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   264 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      69.56 ms /   273 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   170 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      42.33 ms /   175 tokens\n",
      "100%|██████████| 90/90 [00:04<00:00, 19.00it/s]\n",
      "2025-06-05 10:21:20,331 - BERTopic - Representation - Completed ✓\n",
      "  0%|          | 0/88 [00:00<?, ?it/s]Llama.generate: 267 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      37.05 ms /   119 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.16it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.51 ms /   125 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.29it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   143 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      52.36 ms /   150 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.63it/s]\n",
      "  3%|▎         | 3/88 [00:00<00:04, 20.73it/s]Llama.generate: 267 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      41.05 ms /   132 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.59it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.43 ms /   139 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.97it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      34.95 ms /   148 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.65it/s]\n",
      "  7%|▋         | 6/88 [00:00<00:03, 21.46it/s]Llama.generate: 267 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      34.91 ms /   134 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.79it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.39 ms /   139 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.88it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.36 ms /   135 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.13it/s]\n",
      " 10%|█         | 9/88 [00:00<00:03, 21.78it/s]Llama.generate: 267 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.38 ms /   138 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.01it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.52 ms /   138 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.78it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   149 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      85.43 ms /   162 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
      " 14%|█▎        | 12/88 [00:00<00:03, 19.06it/s]Llama.generate: 267 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.93 ms /   125 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.74it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   141 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.60 ms /   148 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.97it/s]\n",
      " 16%|█▌        | 14/88 [00:00<00:03, 18.99it/s]Llama.generate: 267 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.41 ms /   127 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.33it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      34.84 ms /   140 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.65it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      74.28 ms /   165 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.23it/s]\n",
      " 19%|█▉        | 17/88 [00:00<00:03, 18.68it/s]Llama.generate: 267 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.41 ms /   139 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.93it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      74.28 ms /   167 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.23it/s]\n",
      " 22%|██▏       | 19/88 [00:01<00:03, 17.73it/s]Llama.generate: 267 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.97 ms /   126 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.66it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   176 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      65.01 ms /   185 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.09it/s]\n",
      " 24%|██▍       | 21/88 [00:01<00:03, 17.42it/s]Llama.generate: 267 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.54 ms /   140 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.91it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      58.46 ms /   174 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.69it/s]\n",
      " 26%|██▌       | 23/88 [00:01<00:03, 17.47it/s]Llama.generate: 267 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   131 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.49 ms /   138 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.93it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      30.53 ms /   169 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.31it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   255 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      62.42 ms /   263 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.60it/s]\n",
      " 30%|██▉       | 26/88 [00:01<00:03, 17.77it/s]Llama.generate: 267 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      72.60 ms /   217 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.52it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      57.73 ms /   146 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.99it/s]\n",
      " 32%|███▏      | 28/88 [00:01<00:03, 16.53it/s]Llama.generate: 267 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   345 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      84.42 ms /   356 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   176 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      54.29 ms /   183 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.89it/s]\n",
      " 34%|███▍      | 30/88 [00:01<00:03, 15.44it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      31.85 ms /   208 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.99it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   242 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      83.83 ms /   254 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      " 36%|███▋      | 32/88 [00:01<00:03, 15.45it/s]Llama.generate: 267 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      42.84 ms /   219 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.60it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      29.96 ms /   133 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.02it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      51.09 ms /   260 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.04it/s]\n",
      " 40%|███▉      | 35/88 [00:01<00:03, 17.11it/s]Llama.generate: 267 prefix-match hit, remaining 293 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   293 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      48.78 ms /   298 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.92it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   147 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      30.86 ms /   150 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.01it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      71.47 ms /   240 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.67it/s]\n",
      " 43%|████▎     | 38/88 [00:02<00:02, 17.24it/s]Llama.generate: 267 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      87.28 ms /   205 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   269 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      47.94 ms /   274 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.24it/s]\n",
      " 45%|████▌     | 40/88 [00:02<00:02, 16.11it/s]Llama.generate: 267 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      41.89 ms /   128 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.15it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      60.10 ms /   232 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.25it/s]\n",
      " 48%|████▊     | 42/88 [00:02<00:02, 16.38it/s]Llama.generate: 267 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      42.62 ms /   189 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.74it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      41.79 ms /   171 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.15it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      45.74 ms /   259 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.26it/s]\n",
      " 51%|█████     | 45/88 [00:02<00:02, 17.46it/s]Llama.generate: 267 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      41.56 ms /   165 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.33it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   344 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      84.39 ms /   355 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      " 53%|█████▎    | 47/88 [00:02<00:02, 16.47it/s]Llama.generate: 267 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      65.19 ms /   225 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.94it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      63.73 ms /   267 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.40it/s]\n",
      " 56%|█████▌    | 49/88 [00:02<00:02, 15.75it/s]Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   149 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      46.97 ms /   155 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.73it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   249 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      62.22 ms /   257 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.70it/s]\n",
      " 58%|█████▊    | 51/88 [00:02<00:02, 15.89it/s]Llama.generate: 267 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      58.21 ms /   161 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.78it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   241 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      83.80 ms /   253 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n",
      " 60%|██████    | 53/88 [00:03<00:02, 14.86it/s]Llama.generate: 267 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      74.69 ms /   164 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      46.31 ms /   162 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.93it/s]\n",
      " 62%|██████▎   | 55/88 [00:03<00:02, 14.83it/s]Llama.generate: 281 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   155 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.09 ms /   159 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.57it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      40.67 ms /   161 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.80it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   171 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      47.16 ms /   177 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.58it/s]\n",
      " 66%|██████▌   | 58/88 [00:03<00:01, 16.61it/s]Llama.generate: 267 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      50.03 ms /   245 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.50it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      32.40 ms /   235 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.46it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      59.82 ms /   234 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.31it/s]\n",
      " 69%|██████▉   | 61/88 [00:03<00:01, 17.16it/s]Llama.generate: 267 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.90 ms /   142 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.75it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   141 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      46.05 ms /   147 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.02it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      60.82 ms /   240 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.98it/s]\n",
      " 73%|███████▎  | 64/88 [00:03<00:01, 17.44it/s]Llama.generate: 267 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   250 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      83.95 ms /   262 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.68it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   173 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      48.20 ms /   179 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.20it/s]\n",
      " 75%|███████▌  | 66/88 [00:03<00:01, 16.28it/s]Llama.generate: 267 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.02 ms /   137 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.72it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      58.39 ms /   172 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.74it/s]\n",
      " 77%|███████▋  | 68/88 [00:03<00:01, 16.75it/s]Llama.generate: 267 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     5 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      41.01 ms /   128 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.67it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      54.13 ms /   202 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.95it/s]\n",
      " 80%|███████▉  | 70/88 [00:04<00:01, 17.07it/s]Llama.generate: 267 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      63.20 ms /   162 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.49it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      58.60 ms /   175 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.65it/s]\n",
      " 82%|████████▏ | 72/88 [00:04<00:00, 16.25it/s]Llama.generate: 267 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      29.51 ms /   147 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.59it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      30.99 ms /   193 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.90it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      52.54 ms /   151 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.56it/s]\n",
      " 85%|████████▌ | 75/88 [00:04<00:00, 17.98it/s]Llama.generate: 267 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      60.87 ms /   244 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.02it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      47.17 ms /   159 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.63it/s]\n",
      " 88%|████████▊ | 77/88 [00:04<00:00, 17.39it/s]Llama.generate: 267 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   155 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      63.03 ms /   164 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.51it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.02 ms /   144 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.53it/s]\n",
      " 90%|████████▉ | 79/88 [00:04<00:00, 17.40it/s]Llama.generate: 267 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   172 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      36.22 ms /   176 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.51it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   152 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.20 ms /   156 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.51it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      58.06 ms /   154 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.81it/s]\n",
      " 93%|█████████▎| 82/88 [00:04<00:00, 18.09it/s]Llama.generate: 267 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   174 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      36.22 ms /   178 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.59it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   173 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      58.57 ms /   181 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.62it/s]\n",
      " 95%|█████████▌| 84/88 [00:04<00:00, 18.00it/s]Llama.generate: 267 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   150 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      35.16 ms /   154 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.29it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      39.17 ms /   215 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.78it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      72.56 ms /   222 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.50it/s]\n",
      " 99%|█████████▉| 87/88 [00:05<00:00, 17.72it/s]Llama.generate: 267 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      24.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   175 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      48.26 ms /   181 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.15it/s]\n",
      "100%|██████████| 88/88 [00:05<00:00, 17.19it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_init.model_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_query.initiate(n_queries=10, force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
