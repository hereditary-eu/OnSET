{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../backend/src\"))\n",
    "sys.path.append(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup for  GutBrainIE\n",
      "-- Loading and merging datasets\n",
      "0 datasets loaded\n"
     ]
    }
   ],
   "source": [
    "from rdflib.plugins.stores.sparqlstore import SPARQLStore\n",
    "from backend.ontology import Graph, OntologyManager, OntologyConfig\n",
    "from backend.datasetmatcher import DatasetManager\n",
    "from backend.explorative.explorative_support import GuidanceManager\n",
    "from backend.explorative.llm_query import LLMQuery\n",
    "from backend.explorative.topic_init import TopicInitator\n",
    "\n",
    "from backend.eval_config import (\n",
    "    DBPEDIA_CONFIGS,\n",
    "    OMA_CONFIGS,\n",
    "    UNIPROT_CONFIGS,\n",
    "    BTO_CONFIGS,\n",
    "    DNB_CONFIGS,\n",
    "    BTO_CONFIGS,\n",
    "    GUTBRAINIE_CONFIGS,\n",
    "    EvalConfig,\n",
    ")\n",
    "base_path = \"../data\"\n",
    "onto_path = f\"{base_path}/hero-ontology/hereditary_clinical.ttl\"\n",
    "\n",
    "\n",
    "# graph = Graph().parse(onto_path, format=\"turtle\")\n",
    "# graph.bind(\"bto\", \"http://www.semanticweb.org/ontologies/2020/3/bto#\")\n",
    "\n",
    "# store = SPARQLStore(\n",
    "#     \"http://localhost:7200/repositories/dpedia\",\n",
    "#     method=\"POST_FORM\",\n",
    "#     params={\"infer\": False, \"sameAs\": False},\n",
    "# )\n",
    "\n",
    "# store = SPARQLStore(\n",
    "#     \"http://localhost:3030/dbpedia/query\",\n",
    "#     method=\"POST_FORM\",\n",
    "#     params={\"infer\": False, \"sameAs\": False},\n",
    "# )\n",
    "setup: EvalConfig = GUTBRAINIE_CONFIGS[-1]\n",
    "setup.conn_str=\"postgresql+psycopg://postgres:postgres@localhost:5434/onset-gutbrainie-test\"\n",
    "print(\"Setup for \", setup.name)\n",
    "store = SPARQLStore(\n",
    "    setup.sparql_endpoint,\n",
    "    method=\"POST_FORM\",\n",
    "    params={\"infer\": False, \"sameAs\": False},\n",
    "    retries=10,\n",
    ")\n",
    "graph = Graph(store=store)\n",
    "\n",
    "config = OntologyConfig()\n",
    "\n",
    "ontology_manager = OntologyManager(config, graph)\n",
    "dataset_manager = DatasetManager(ontology_manager)\n",
    "dataset_manager.initialise(glob_path=\"data/datasets/ALS/**/*.csv\")\n",
    "\n",
    "# ontology_manager.load_full_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9dcd1ca8dd910b566efdb1f6a76eaacb3c2187b9718c6be6be696212682c6c9b\n",
      "Loading LLM model NousResearch/Hermes-3-Llama-3.2-3B-GGUF None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) - 23528 MiB free\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 255 tensors from /home/bkantz/.cache/huggingface/hub/models--NousResearch--Hermes-3-Llama-3.2-3B-GGUF/snapshots/3cd927095d8cbab12c743f932aa63b6f7bbfa141/./Hermes-3-Llama-3.2-3B.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hermes 3 Llama 3.2 3b Base Fft Chatml...\n",
      "llama_model_loader: - kv   3:                            general.version str              = 2024-11-24\n",
      "llama_model_loader: - kv   4:                       general.organization str              = Nous Research Core\n",
      "llama_model_loader: - kv   5:                           general.finetune str              = base-fft-chatml-rerun-8xh100-e4-4k-steps\n",
      "llama_model_loader: - kv   6:                           general.basename str              = Hermes-3-Llama-3.2\n",
      "llama_model_loader: - kv   7:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   8:                            general.license str              = llama3\n",
      "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Meta Llama 3.2 3B\n",
      "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Met...\n",
      "llama_model_loader: - kv  13:                               general.tags arr[str,12]      = [\"Llama-3\", \"instruct\", \"finetune\", \"...\n",
      "llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  15:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  16:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  17:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  18:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  19:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  20:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  21:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  22:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  23:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  24:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  25:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  26:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  27:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 128039\n",
      "llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  37:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q8_0:  197 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 3.18 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128040 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7994 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Hermes 3 Llama 3.2 3b Base Fft Chatml Rerun 8xh100 E4 2024 11 24 4k Steps\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128039 '<|im_end|>'\n",
      "print_info: EOT token        = 128039 '<|im_end|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: PAD token        = 128001 '<|end_of_text|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: EOG token        = 128039 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 28 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 29/29 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  3255.90 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   399.23 MiB\n",
      ".................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 10000\n",
      "llama_context: n_ctx_per_seq = 10000\n",
      "llama_context: n_batch       = 2048\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (10000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 10016 (padded)\n",
      "llama_kv_cache_unified: kv_size = 10016, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =  1095.50 MiB\n",
      "llama_kv_cache_unified: KV self size  = 1095.50 MiB, K (f16):  547.75 MiB, V (f16):  547.75 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      CUDA0 compute buffer size =   513.07 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    25.57 MiB\n",
      "llama_context: graph nodes  = 958\n",
      "llama_context: graph splits = 2\n",
      "CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'general.license': 'llama3', 'llama.attention.value_length': '128', 'general.size_label': '3B', 'general.type': 'model', 'general.organization': 'Nous Research Core', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Meta-Llama-3.2-3B', 'general.version': '2024-11-24', 'general.base_model.0.name': 'Meta Llama 3.2 3B', 'llama.rope.dimension_count': '128', 'llama.context_length': '131072', 'llama.embedding_length': '3072', 'general.basename': 'Hermes-3-Llama-3.2', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '8', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Meta Llama', 'llama.feed_forward_length': '8192', 'general.name': 'Hermes 3 Llama 3.2 3b Base Fft Chatml Rerun 8xh100 E4 2024 11 24 4k Steps', 'tokenizer.ggml.bos_token_id': '128000', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.eos_token_id': '128039', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.block_count': '28', 'llama.attention.head_count': '24', 'llama.attention.key_length': '128', 'general.finetune': 'base-fft-chatml-rerun-8xh100-e4-4k-steps', 'general.file_type': '7', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.vocab_size': '128256', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: {'new.pooler.dense.weight', 'new.pooler.dense.bias'}\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: NewModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Dense({'in_features': 1024, 'out_features': 1024, 'bias': True, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "guidance_man = GuidanceManager(\n",
    "    ontology_manager, conn_str=setup.conn_str, llm_model_id=setup.model_id\n",
    ")\n",
    "print(guidance_man.identifier)\n",
    "llm_query = LLMQuery(guidance_man)\n",
    "topic_init = TopicInitator(guidance_man)\n",
    "guidance_man.llama_model\n",
    "guidance_man.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving anonymous properties: 0it [00:00, ?it/s]\n",
      "Saving classes: 100%|██████████| 58/58 [00:47<00:00,  1.23it/s]\n",
      "Saving relations:   9%|▊         | 5/58 [00:00<00:01, 30.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn74\n",
      "Skipping _:bn74\n",
      "Skipping _:bn71\n",
      "Skipping _:bn71\n",
      "Skipping _:bn14\n",
      "Skipping _:bn14\n",
      "Skipping _:bn74\n",
      "Skipping _:bn33\n",
      "Skipping _:bn33\n",
      "Skipping _:bn74\n",
      "Skipping _:bn19\n",
      "Skipping _:bn19\n",
      "Skipping _:bn14\n",
      "Skipping _:bn74\n",
      "Skipping _:bn74\n",
      "Skipping _:bn33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  16%|█▌        | 9/58 [00:01<00:06,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  21%|██        | 12/58 [00:01<00:05,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn74\n",
      "Skipping _:bn33\n",
      "Skipping _:bn71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  28%|██▊       | 16/58 [00:01<00:03, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn74\n",
      "Skipping _:bn33\n",
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  38%|███▊      | 22/58 [00:01<00:01, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn19\n",
      "Skipping _:bn74\n",
      "Skipping _:bn71\n",
      "Skipping _:bn19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  43%|████▎     | 25/58 [00:01<00:02, 13.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn71\n",
      "Skipping _:bn19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations:  52%|█████▏    | 30/58 [00:02<00:01, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving relations: 100%|██████████| 58/58 [00:02<00:00, 20.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn19\n",
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixing props links: 100%|██████████| 193/193 [00:00<00:00, 1966.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11.73it/s]\n",
      "Building documents:   4%|▎         | 2/57 [00:01<00:43,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   7%|▋         | 4/57 [00:03<00:35,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:   9%|▉         | 5/57 [00:05<00:57,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn14\n",
      "Skipping _:bn74\n",
      "Skipping _:bn33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  14%|█▍        | 8/57 [00:06<00:35,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn19\n",
      "Skipping _:bn14\n",
      "Skipping _:bn74\n",
      "Skipping _:bn74\n",
      "Skipping _:bn33\n",
      "Skipping _:bn19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  16%|█▌        | 9/57 [00:07<00:30,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  18%|█▊        | 10/57 [00:09<00:46,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  19%|█▉        | 11/57 [00:11<00:56,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn33\n",
      "Skipping _:bn71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  23%|██▎       | 13/57 [00:12<00:38,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  26%|██▋       | 15/57 [00:12<00:27,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  32%|███▏      | 18/57 [00:13<00:17,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  37%|███▋      | 21/57 [00:14<00:14,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  39%|███▊      | 22/57 [00:16<00:22,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn71\n",
      "Skipping _:bn19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  42%|████▏     | 24/57 [00:16<00:16,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n",
      "Skipping _:bn71\n",
      "Skipping _:bn19\n",
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  63%|██████▎   | 36/57 [00:22<00:11,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents:  70%|███████   | 40/57 [00:25<00:10,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping _:bn74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building documents: 100%|██████████| 57/57 [00:48<00:00,  1.18it/s]\n",
      "2025-08-11 17:11:58,284 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fabe35bc534b829650471cf314b521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 17:11:59,909 - BERTopic - Embedding - Completed ✓\n",
      "2025-08-11 17:11:59,910 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-08-11 17:12:07,336 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-08-11 17:12:07,337 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-08-11 17:12:07,370 - BERTopic - Cluster - Completed ✓\n",
      "2025-08-11 17:12:07,374 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      61.05 ms /   439 tokens (    0.14 ms per token,  7191.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =      43.60 ms /     8 runs   (    5.45 ms per token,   183.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     110.07 ms /   447 tokens\n",
      "  1%|          | 1/85 [00:00<00:09,  8.93it/s]Llama.generate: 267 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.75 ms /   189 tokens (    0.07 ms per token, 13743.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.70 ms /     5 runs   (    5.34 ms per token,   187.29 tokens per second)\n",
      "llama_perf_context_print:       total time =      43.83 ms /   194 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      15.69 ms /   233 tokens (    0.07 ms per token, 14854.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =      10.96 ms /     2 runs   (    5.48 ms per token,   182.57 tokens per second)\n",
      "llama_perf_context_print:       total time =      28.25 ms /   235 tokens\n",
      "Llama.generate: 268 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      17.18 ms /   248 tokens (    0.07 ms per token, 14431.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =      19.10 ms /     3 runs   (    6.37 ms per token,   157.08 tokens per second)\n",
      "llama_perf_context_print:       total time =      38.51 ms /   251 tokens\n",
      "  5%|▍         | 4/85 [00:00<00:04, 19.25it/s]Llama.generate: 267 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.45 ms /   223 tokens (    0.06 ms per token, 15429.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.71 ms /     4 runs   (    5.43 ms per token,   184.24 tokens per second)\n",
      "llama_perf_context_print:       total time =      38.73 ms /   227 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      16.87 ms /   249 tokens (    0.07 ms per token, 14763.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.39 ms /     4 runs   (    5.35 ms per token,   187.01 tokens per second)\n",
      "llama_perf_context_print:       total time =      40.78 ms /   253 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.68 ms /   164 tokens (    0.08 ms per token, 12936.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.82 ms /     6 runs   (    5.30 ms per token,   188.55 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.89 ms /   170 tokens\n",
      "  8%|▊         | 7/85 [00:00<00:03, 21.05it/s]Llama.generate: 267 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.83 ms /   148 tokens (    0.08 ms per token, 12515.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.32 ms /     3 runs   (    5.44 ms per token,   183.78 tokens per second)\n",
      "llama_perf_context_print:       total time =      30.13 ms /   151 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.27 ms /   215 tokens (    0.07 ms per token, 15067.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.16 ms /     4 runs   (    5.29 ms per token,   189.00 tokens per second)\n",
      "llama_perf_context_print:       total time =      37.89 ms /   219 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.14 ms /   203 tokens (    0.07 ms per token, 14354.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.05 ms /     9 runs   (    5.23 ms per token,   191.28 tokens per second)\n",
      "llama_perf_context_print:       total time =      65.95 ms /   212 tokens\n",
      " 12%|█▏        | 10/85 [00:00<00:03, 21.35it/s]Llama.generate: 267 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.63 ms /   138 tokens (    0.08 ms per token, 11866.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.05 ms /     4 runs   (    5.26 ms per token,   190.07 tokens per second)\n",
      "llama_perf_context_print:       total time =      35.12 ms /   142 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 360 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      22.83 ms /   360 tokens (    0.06 ms per token, 15771.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =      67.51 ms /    13 runs   (    5.19 ms per token,   192.56 tokens per second)\n",
      "llama_perf_context_print:       total time =      97.02 ms /   373 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.00 ms /   183 tokens (    0.07 ms per token, 14073.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.96 ms /     6 runs   (    5.33 ms per token,   187.75 tokens per second)\n",
      "llama_perf_context_print:       total time =      48.34 ms /   189 tokens\n",
      " 15%|█▌        | 13/85 [00:00<00:03, 19.05it/s]Llama.generate: 267 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.84 ms /   151 tokens (    0.08 ms per token, 12751.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.34 ms /     5 runs   (    5.27 ms per token,   189.84 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.09 ms /   156 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.86 ms /   176 tokens (    0.07 ms per token, 13687.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.46 ms /     9 runs   (    5.27 ms per token,   189.62 tokens per second)\n",
      "llama_perf_context_print:       total time =      65.08 ms /   185 tokens\n",
      " 18%|█▊        | 15/85 [00:00<00:03, 18.87it/s]Llama.generate: 267 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.84 ms /   173 tokens (    0.07 ms per token, 13475.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.28 ms /     5 runs   (    5.26 ms per token,   190.24 tokens per second)\n",
      "llama_perf_context_print:       total time =      42.06 ms /   178 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.77 ms /   170 tokens (    0.08 ms per token, 13310.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.23 ms /     5 runs   (    5.25 ms per token,   190.64 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.91 ms /   175 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.86 ms /   178 tokens (    0.07 ms per token, 13837.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =      15.92 ms /     3 runs   (    5.31 ms per token,   188.49 tokens per second)\n",
      "llama_perf_context_print:       total time =      30.77 ms /   181 tokens\n",
      " 21%|██        | 18/85 [00:00<00:03, 20.79it/s]Llama.generate: 267 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      20.98 ms /   319 tokens (    0.07 ms per token, 15208.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =      11.40 ms /     2 runs   (    5.70 ms per token,   175.36 tokens per second)\n",
      "llama_perf_context_print:       total time =      33.98 ms /   321 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.82 ms /   174 tokens (    0.07 ms per token, 13571.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.42 ms /     3 runs   (    5.47 ms per token,   182.67 tokens per second)\n",
      "llama_perf_context_print:       total time =      31.24 ms /   177 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.79 ms /   170 tokens (    0.08 ms per token, 13296.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.07 ms /     4 runs   (    5.27 ms per token,   189.83 tokens per second)\n",
      "llama_perf_context_print:       total time =      36.31 ms /   174 tokens\n",
      " 25%|██▍       | 21/85 [00:01<00:02, 22.89it/s]Llama.generate: 267 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.68 ms /   161 tokens (    0.08 ms per token, 12695.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =      20.98 ms /     4 runs   (    5.25 ms per token,   190.63 tokens per second)\n",
      "llama_perf_context_print:       total time =      36.19 ms /   165 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.76 ms /   169 tokens (    0.08 ms per token, 13249.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.27 ms /     6 runs   (    5.21 ms per token,   191.85 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.41 ms /   175 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      15.19 ms /   229 tokens (    0.07 ms per token, 15080.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =      62.81 ms /    12 runs   (    5.23 ms per token,   191.06 tokens per second)\n",
      "llama_perf_context_print:       total time =      84.12 ms /   241 tokens\n",
      " 28%|██▊       | 24/85 [00:01<00:02, 20.77it/s]Llama.generate: 267 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.90 ms /   180 tokens (    0.07 ms per token, 13957.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.51 ms /     9 runs   (    5.28 ms per token,   189.43 tokens per second)\n",
      "llama_perf_context_print:       total time =      65.16 ms /   189 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 338 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      22.32 ms /   338 tokens (    0.07 ms per token, 15146.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =      37.49 ms /     7 runs   (    5.36 ms per token,   186.71 tokens per second)\n",
      "llama_perf_context_print:       total time =      63.78 ms /   345 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.08 ms /   187 tokens (    0.07 ms per token, 14299.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =      52.57 ms /    10 runs   (    5.26 ms per token,   190.21 tokens per second)\n",
      "llama_perf_context_print:       total time =      70.86 ms /   197 tokens\n",
      " 32%|███▏      | 27/85 [00:01<00:03, 18.35it/s]Llama.generate: 267 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.77 ms /   165 tokens (    0.08 ms per token, 12920.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.19 ms /     4 runs   (    5.30 ms per token,   188.78 tokens per second)\n",
      "llama_perf_context_print:       total time =      36.42 ms /   169 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.91 ms /   154 tokens (    0.08 ms per token, 12930.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =      15.94 ms /     3 runs   (    5.31 ms per token,   188.18 tokens per second)\n",
      "llama_perf_context_print:       total time =      29.85 ms /   157 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.12 ms /   204 tokens (    0.07 ms per token, 14451.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =      68.29 ms /    13 runs   (    5.25 ms per token,   190.35 tokens per second)\n",
      "llama_perf_context_print:       total time =      89.02 ms /   217 tokens\n",
      " 35%|███▌      | 30/85 [00:01<00:02, 18.51it/s]Llama.generate: 267 prefix-match hit, remaining 171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.77 ms /   171 tokens (    0.07 ms per token, 13386.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.25 ms /     5 runs   (    5.25 ms per token,   190.46 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.94 ms /   176 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.77 ms /   149 tokens (    0.08 ms per token, 12658.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.30 ms /     5 runs   (    5.26 ms per token,   190.15 tokens per second)\n",
      "llama_perf_context_print:       total time =      40.99 ms /   154 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.12 ms /   201 tokens (    0.07 ms per token, 14235.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =      78.68 ms /    15 runs   (    5.25 ms per token,   190.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.31 ms /   216 tokens\n",
      " 39%|███▉      | 33/85 [00:01<00:02, 17.67it/s]Llama.generate: 267 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.87 ms /   180 tokens (    0.07 ms per token, 13987.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.44 ms /     9 runs   (    5.27 ms per token,   189.72 tokens per second)\n",
      "llama_perf_context_print:       total time =      65.06 ms /   189 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.69 ms /   164 tokens (    0.08 ms per token, 12928.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.62 ms /     7 runs   (    5.23 ms per token,   191.17 tokens per second)\n",
      "llama_perf_context_print:       total time =      53.16 ms /   171 tokens\n",
      " 41%|████      | 35/85 [00:01<00:02, 17.40it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.18 ms /   205 tokens (    0.07 ms per token, 14462.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =      41.97 ms /     8 runs   (    5.25 ms per token,   190.60 tokens per second)\n",
      "llama_perf_context_print:       total time =      60.44 ms /   213 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.73 ms /   166 tokens (    0.08 ms per token, 13040.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.48 ms /     6 runs   (    5.25 ms per token,   190.57 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.58 ms /   172 tokens\n",
      " 44%|████▎     | 37/85 [00:01<00:02, 17.56it/s]Llama.generate: 267 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.77 ms /   172 tokens (    0.07 ms per token, 13472.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.64 ms /     7 runs   (    5.23 ms per token,   191.06 tokens per second)\n",
      "llama_perf_context_print:       total time =      53.26 ms /   179 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.80 ms /   149 tokens (    0.08 ms per token, 12626.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.61 ms /     7 runs   (    5.23 ms per token,   191.20 tokens per second)\n",
      "llama_perf_context_print:       total time =      52.25 ms /   156 tokens\n",
      " 46%|████▌     | 39/85 [00:02<00:02, 17.79it/s]Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.70 ms /   164 tokens (    0.08 ms per token, 12913.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =      46.94 ms /     9 runs   (    5.22 ms per token,   191.74 tokens per second)\n",
      "llama_perf_context_print:       total time =      64.42 ms /   173 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.94 ms /   159 tokens (    0.08 ms per token, 13313.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.42 ms /     6 runs   (    5.24 ms per token,   190.96 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.72 ms /   165 tokens\n",
      " 48%|████▊     | 41/85 [00:02<00:02, 17.74it/s]Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.77 ms /   149 tokens (    0.08 ms per token, 12655.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.42 ms /     6 runs   (    5.24 ms per token,   190.99 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.55 ms /   155 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      15.26 ms /   237 tokens (    0.06 ms per token, 15527.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =      58.66 ms /    11 runs   (    5.33 ms per token,   187.52 tokens per second)\n",
      "llama_perf_context_print:       total time =      79.71 ms /   248 tokens\n",
      " 51%|█████     | 43/85 [00:02<00:02, 17.07it/s]Llama.generate: 267 prefix-match hit, remaining 178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.86 ms /   178 tokens (    0.07 ms per token, 13839.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =      48.04 ms /     9 runs   (    5.34 ms per token,   187.35 tokens per second)\n",
      "llama_perf_context_print:       total time =      65.69 ms /   187 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.14 ms /   208 tokens (    0.07 ms per token, 14707.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =      73.55 ms /    14 runs   (    5.25 ms per token,   190.34 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.76 ms /   222 tokens\n",
      " 53%|█████▎    | 45/85 [00:02<00:02, 15.35it/s]Llama.generate: 267 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.91 ms /   154 tokens (    0.08 ms per token, 12929.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.12 ms /     4 runs   (    5.28 ms per token,   189.41 tokens per second)\n",
      "llama_perf_context_print:       total time =      35.48 ms /   158 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.95 ms /   160 tokens (    0.07 ms per token, 13392.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.26 ms /     5 runs   (    5.25 ms per token,   190.39 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.10 ms /   165 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.78 ms /   149 tokens (    0.08 ms per token, 12653.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =      41.77 ms /     8 runs   (    5.22 ms per token,   191.54 tokens per second)\n",
      "llama_perf_context_print:       total time =      57.83 ms /   157 tokens\n",
      " 56%|█████▋    | 48/85 [00:02<00:02, 17.27it/s]Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.79 ms /   149 tokens (    0.08 ms per token, 12632.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.55 ms /     7 runs   (    5.22 ms per token,   191.51 tokens per second)\n",
      "llama_perf_context_print:       total time =      52.16 ms /   156 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.76 ms /   169 tokens (    0.08 ms per token, 13249.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =      57.40 ms /    11 runs   (    5.22 ms per token,   191.63 tokens per second)\n",
      "llama_perf_context_print:       total time =      75.84 ms /   180 tokens\n",
      " 59%|█████▉    | 50/85 [00:02<00:02, 16.70it/s]Llama.generate: 267 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.31 ms /   216 tokens (    0.07 ms per token, 15096.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =      78.46 ms /    15 runs   (    5.23 ms per token,   191.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.32 ms /   231 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.82 ms /   150 tokens (    0.08 ms per token, 12691.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.51 ms /     6 runs   (    5.25 ms per token,   190.42 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.70 ms /   156 tokens\n",
      " 61%|██████    | 52/85 [00:02<00:02, 15.63it/s]Llama.generate: 267 prefix-match hit, remaining 178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.87 ms /   178 tokens (    0.07 ms per token, 13830.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =      42.26 ms /     8 runs   (    5.28 ms per token,   189.32 tokens per second)\n",
      "llama_perf_context_print:       total time =      59.42 ms /   186 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 163 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.69 ms /   163 tokens (    0.08 ms per token, 12843.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =      52.11 ms /    10 runs   (    5.21 ms per token,   191.91 tokens per second)\n",
      "llama_perf_context_print:       total time =      69.99 ms /   173 tokens\n",
      " 64%|██████▎   | 54/85 [00:03<00:02, 15.49it/s]Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.03 ms /   195 tokens (    0.07 ms per token, 13895.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =      62.68 ms /    12 runs   (    5.22 ms per token,   191.45 tokens per second)\n",
      "llama_perf_context_print:       total time =      82.87 ms /   207 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.89 ms /   154 tokens (    0.08 ms per token, 12947.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.64 ms /     7 runs   (    5.23 ms per token,   191.06 tokens per second)\n",
      "llama_perf_context_print:       total time =      52.36 ms /   161 tokens\n",
      " 66%|██████▌   | 56/85 [00:03<00:01, 15.19it/s]Llama.generate: 267 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.92 ms /   154 tokens (    0.08 ms per token, 12923.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =      62.46 ms /    12 runs   (    5.21 ms per token,   192.12 tokens per second)\n",
      "llama_perf_context_print:       total time =      80.52 ms /   166 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.72 ms /   145 tokens (    0.08 ms per token, 12367.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.77 ms /     6 runs   (    5.29 ms per token,   188.86 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.87 ms /   151 tokens\n",
      " 68%|██████▊   | 58/85 [00:03<00:01, 15.25it/s]Llama.generate: 267 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.89 ms /   180 tokens (    0.07 ms per token, 13959.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.36 ms /     3 runs   (    5.45 ms per token,   183.39 tokens per second)\n",
      "llama_perf_context_print:       total time =      31.26 ms /   183 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.89 ms /   154 tokens (    0.08 ms per token, 12946.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.60 ms /     7 runs   (    5.23 ms per token,   191.26 tokens per second)\n",
      "llama_perf_context_print:       total time =      52.32 ms /   161 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.82 ms /   150 tokens (    0.08 ms per token, 12686.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =      41.75 ms /     8 runs   (    5.22 ms per token,   191.64 tokens per second)\n",
      "llama_perf_context_print:       total time =      57.85 ms /   158 tokens\n",
      " 72%|███████▏  | 61/85 [00:03<00:01, 16.95it/s]Llama.generate: 267 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.89 ms /   177 tokens (    0.07 ms per token, 13728.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.83 ms /     6 runs   (    5.30 ms per token,   188.51 tokens per second)\n",
      "llama_perf_context_print:       total time =      48.09 ms /   183 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.65 ms /   161 tokens (    0.08 ms per token, 12725.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.27 ms /     5 runs   (    5.25 ms per token,   190.33 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.82 ms /   166 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.36 ms /   127 tokens (    0.10 ms per token, 10273.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =      15.90 ms /     3 runs   (    5.30 ms per token,   188.63 tokens per second)\n",
      "llama_perf_context_print:       total time =      30.24 ms /   130 tokens\n",
      " 75%|███████▌  | 64/85 [00:03<00:01, 18.97it/s]Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.69 ms /   164 tokens (    0.08 ms per token, 12924.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.48 ms /     6 runs   (    5.25 ms per token,   190.61 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.56 ms /   170 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.82 ms /   151 tokens (    0.08 ms per token, 12774.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =      46.94 ms /     9 runs   (    5.22 ms per token,   191.74 tokens per second)\n",
      "llama_perf_context_print:       total time =      63.50 ms /   160 tokens\n",
      " 78%|███████▊  | 66/85 [00:03<00:01, 18.60it/s]Llama.generate: 267 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.85 ms /   151 tokens (    0.08 ms per token, 12741.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.07 ms /     4 runs   (    5.27 ms per token,   189.88 tokens per second)\n",
      "llama_perf_context_print:       total time =      35.36 ms /   155 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.80 ms /   148 tokens (    0.08 ms per token, 12543.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.85 ms /     6 runs   (    5.31 ms per token,   188.37 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.02 ms /   154 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.85 ms /   151 tokens (    0.08 ms per token, 12747.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.06 ms /     4 runs   (    5.27 ms per token,   189.91 tokens per second)\n",
      "llama_perf_context_print:       total time =      35.35 ms /   155 tokens\n",
      " 81%|████████  | 69/85 [00:03<00:00, 20.37it/s]Llama.generate: 268 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.83 ms /   150 tokens (    0.08 ms per token, 12681.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =      15.90 ms /     3 runs   (    5.30 ms per token,   188.73 tokens per second)\n",
      "llama_perf_context_print:       total time =      29.68 ms /   153 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.92 ms /   155 tokens (    0.08 ms per token, 13008.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.40 ms /     6 runs   (    5.23 ms per token,   191.11 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.67 ms /   161 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.75 ms /   146 tokens (    0.08 ms per token, 12428.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =      15.91 ms /     3 runs   (    5.30 ms per token,   188.55 tokens per second)\n",
      "llama_perf_context_print:       total time =      29.74 ms /   149 tokens\n",
      " 85%|████████▍ | 72/85 [00:03<00:00, 22.26it/s]Llama.generate: 267 prefix-match hit, remaining 152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.90 ms /   152 tokens (    0.08 ms per token, 12777.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.65 ms /     7 runs   (    5.24 ms per token,   190.98 tokens per second)\n",
      "llama_perf_context_print:       total time =      52.38 ms /   159 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 152 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.87 ms /   152 tokens (    0.08 ms per token, 12806.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.64 ms /     7 runs   (    5.23 ms per token,   191.03 tokens per second)\n",
      "llama_perf_context_print:       total time =      52.44 ms /   159 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.43 ms /   223 tokens (    0.06 ms per token, 15456.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =      10.91 ms /     2 runs   (    5.46 ms per token,   183.25 tokens per second)\n",
      "llama_perf_context_print:       total time =      26.90 ms /   225 tokens\n",
      " 88%|████████▊ | 75/85 [00:04<00:00, 22.21it/s]Llama.generate: 270 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.38 ms /   220 tokens (    0.07 ms per token, 15304.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.01 ms /     3 runs   (    5.34 ms per token,   187.38 tokens per second)\n",
      "llama_perf_context_print:       total time =      32.40 ms /   223 tokens\n",
      "Llama.generate: 269 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.38 ms /   221 tokens (    0.07 ms per token, 15364.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =      10.80 ms /     2 runs   (    5.40 ms per token,   185.25 tokens per second)\n",
      "llama_perf_context_print:       total time =      26.73 ms /   223 tokens\n",
      "Llama.generate: 269 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.27 ms /   214 tokens (    0.07 ms per token, 14994.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =      10.79 ms /     2 runs   (    5.40 ms per token,   185.27 tokens per second)\n",
      "llama_perf_context_print:       total time =      26.61 ms /   216 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      18.42 ms /   268 tokens (    0.07 ms per token, 14546.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.91 ms /     7 runs   (    5.27 ms per token,   189.62 tokens per second)\n",
      "llama_perf_context_print:       total time =      59.20 ms /   275 tokens\n",
      " 93%|█████████▎| 79/85 [00:04<00:00, 23.73it/s]Llama.generate: 267 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.96 ms /   159 tokens (    0.08 ms per token, 13297.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =      37.13 ms /     7 runs   (    5.30 ms per token,   188.55 tokens per second)\n",
      "llama_perf_context_print:       total time =      52.92 ms /   166 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.78 ms /   169 tokens (    0.08 ms per token, 13224.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.17 ms /     5 runs   (    5.23 ms per token,   191.04 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.85 ms /   174 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.26 ms /   213 tokens (    0.07 ms per token, 14935.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.66 ms /     6 runs   (    5.28 ms per token,   189.52 tokens per second)\n",
      "llama_perf_context_print:       total time =      49.34 ms /   219 tokens\n",
      " 96%|█████████▋| 82/85 [00:04<00:00, 22.60it/s]Llama.generate: 267 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.38 ms /   127 tokens (    0.10 ms per token, 10258.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.25 ms /     5 runs   (    5.25 ms per token,   190.46 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.53 ms /   132 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.12 ms /   202 tokens (    0.07 ms per token, 14303.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =      10.84 ms /     2 runs   (    5.42 ms per token,   184.55 tokens per second)\n",
      "llama_perf_context_print:       total time =      26.50 ms /   204 tokens\n",
      "Llama.generate: 267 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.79 ms /   172 tokens (    0.07 ms per token, 13453.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.49 ms /     6 runs   (    5.25 ms per token,   190.56 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.65 ms /   178 tokens\n",
      "100%|██████████| 85/85 [00:04<00:00, 19.03it/s]\n",
      "2025-08-11 17:12:11,943 - BERTopic - Representation - Completed ✓\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]Llama.generate: 267 prefix-match hit, remaining 162 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.84 ms /   162 tokens (    0.08 ms per token, 12618.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.31 ms /     3 runs   (    5.44 ms per token,   183.91 tokens per second)\n",
      "llama_perf_context_print:       total time =      31.24 ms /   165 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.80it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.80 ms /   146 tokens (    0.08 ms per token, 12374.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.12 ms /     3 runs   (    5.37 ms per token,   186.13 tokens per second)\n",
      "llama_perf_context_print:       total time =      29.97 ms /   149 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.04it/s]\n",
      "Llama.generate: 280 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.81 ms /   146 tokens (    0.08 ms per token, 12364.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.09 ms /     3 runs   (    5.36 ms per token,   186.43 tokens per second)\n",
      "llama_perf_context_print:       total time =      29.96 ms /   149 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.90it/s]\n",
      "  4%|▎         | 3/83 [00:00<00:02, 27.56it/s]Llama.generate: 267 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.80 ms /   169 tokens (    0.08 ms per token, 13204.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.39 ms /     5 runs   (    5.28 ms per token,   189.44 tokens per second)\n",
      "llama_perf_context_print:       total time =      42.14 ms /   174 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.08it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.08 ms /   187 tokens (    0.07 ms per token, 14293.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.93 ms /     7 runs   (    5.28 ms per token,   189.57 tokens per second)\n",
      "llama_perf_context_print:       total time =      53.87 ms /   194 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.19it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.91 ms /   179 tokens (    0.07 ms per token, 13860.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.83 ms /     4 runs   (    5.46 ms per token,   183.27 tokens per second)\n",
      "llama_perf_context_print:       total time =      37.22 ms /   183 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.90it/s]\n",
      "  7%|▋         | 6/83 [00:00<00:03, 22.58it/s]Llama.generate: 267 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      23.72 ms /   390 tokens (    0.06 ms per token, 16442.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =      52.67 ms /    10 runs   (    5.27 ms per token,   189.87 tokens per second)\n",
      "llama_perf_context_print:       total time =      81.72 ms /   400 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      20.98 ms /   319 tokens (    0.07 ms per token, 15202.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.30 ms /     4 runs   (    5.32 ms per token,   187.80 tokens per second)\n",
      "llama_perf_context_print:       total time =      44.81 ms /   323 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.83 ms /   170 tokens (    0.08 ms per token, 13251.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =      37.31 ms /     7 runs   (    5.33 ms per token,   187.62 tokens per second)\n",
      "llama_perf_context_print:       total time =      54.01 ms /   177 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.07it/s]\n",
      " 11%|█         | 9/83 [00:00<00:04, 18.48it/s]Llama.generate: 267 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.73 ms /   141 tokens (    0.08 ms per token, 12019.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.43 ms /     5 runs   (    5.29 ms per token,   189.19 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.09 ms /   146 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.64it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.88 ms /   176 tokens (    0.07 ms per token, 13667.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.42 ms /     5 runs   (    5.28 ms per token,   189.28 tokens per second)\n",
      "llama_perf_context_print:       total time =      42.23 ms /   181 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.00it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      20.99 ms /   319 tokens (    0.07 ms per token, 15196.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =      11.59 ms /     2 runs   (    5.79 ms per token,   172.56 tokens per second)\n",
      "llama_perf_context_print:       total time =      34.18 ms /   321 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.28it/s]\n",
      " 14%|█▍        | 12/83 [00:00<00:03, 19.85it/s]Llama.generate: 267 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.10 ms /   185 tokens (    0.07 ms per token, 14119.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =      32.25 ms /     6 runs   (    5.38 ms per token,   186.05 tokens per second)\n",
      "llama_perf_context_print:       total time =      48.76 ms /   191 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.93it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.78 ms /   165 tokens (    0.08 ms per token, 12914.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.67 ms /     6 runs   (    5.28 ms per token,   189.45 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.85 ms /   171 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.44it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.94 ms /   156 tokens (    0.08 ms per token, 13065.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.35 ms /     5 runs   (    5.27 ms per token,   189.74 tokens per second)\n",
      "llama_perf_context_print:       total time =      41.27 ms /   161 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.39it/s]\n",
      " 18%|█▊        | 15/83 [00:00<00:03, 19.66it/s]Llama.generate: 267 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      21.01 ms /   319 tokens (    0.07 ms per token, 15183.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =      11.61 ms /     2 runs   (    5.80 ms per token,   172.28 tokens per second)\n",
      "llama_perf_context_print:       total time =      34.32 ms /   321 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.18it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 367 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      23.23 ms /   367 tokens (    0.06 ms per token, 15795.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =      62.88 ms /    12 runs   (    5.24 ms per token,   190.84 tokens per second)\n",
      "llama_perf_context_print:       total time =      92.47 ms /   379 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.64it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      21.00 ms /   319 tokens (    0.07 ms per token, 15187.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =      10.92 ms /     2 runs   (    5.46 ms per token,   183.23 tokens per second)\n",
      "llama_perf_context_print:       total time =      33.52 ms /   321 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.88it/s]\n",
      " 22%|██▏       | 18/83 [00:00<00:03, 18.64it/s]Llama.generate: 267 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.16 ms /   191 tokens (    0.07 ms per token, 14515.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =      27.09 ms /     5 runs   (    5.42 ms per token,   184.55 tokens per second)\n",
      "llama_perf_context_print:       total time =      43.19 ms /   196 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.50it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.88 ms /   176 tokens (    0.07 ms per token, 13668.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =      37.49 ms /     7 runs   (    5.36 ms per token,   186.71 tokens per second)\n",
      "llama_perf_context_print:       total time =      54.23 ms /   183 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.06it/s]\n",
      " 24%|██▍       | 20/83 [00:01<00:03, 18.59it/s]Llama.generate: 267 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      20.88 ms /   313 tokens (    0.07 ms per token, 14991.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =      11.60 ms /     2 runs   (    5.80 ms per token,   172.41 tokens per second)\n",
      "llama_perf_context_print:       total time =      34.07 ms /   315 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.56it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      22.74 ms /   352 tokens (    0.06 ms per token, 15481.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =      72.42 ms /    14 runs   (    5.17 ms per token,   193.32 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.31 ms /   366 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n",
      " 27%|██▋       | 22/83 [00:01<00:03, 16.96it/s]Llama.generate: 267 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      15.21 ms /   230 tokens (    0.07 ms per token, 15124.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =      78.81 ms /    15 runs   (    5.25 ms per token,   190.32 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.60 ms /   245 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.71it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.96 ms /   155 tokens (    0.08 ms per token, 12958.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.33 ms /     4 runs   (    5.33 ms per token,   187.51 tokens per second)\n",
      "llama_perf_context_print:       total time =      35.75 ms /   159 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.19it/s]\n",
      " 29%|██▉       | 24/83 [00:01<00:03, 15.85it/s]Llama.generate: 267 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.35 ms /   216 tokens (    0.07 ms per token, 15049.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =      42.12 ms /     8 runs   (    5.27 ms per token,   189.92 tokens per second)\n",
      "llama_perf_context_print:       total time =      60.81 ms /   224 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.18it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.83 ms /   149 tokens (    0.08 ms per token, 12590.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.60 ms /     6 runs   (    5.27 ms per token,   189.89 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.79 ms /   155 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.92it/s]\n",
      " 31%|███▏      | 26/83 [00:01<00:03, 16.11it/s]Llama.generate: 267 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.10 ms /   186 tokens (    0.07 ms per token, 14200.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.47 ms /     5 runs   (    5.29 ms per token,   188.88 tokens per second)\n",
      "llama_perf_context_print:       total time =      42.49 ms /   191 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.94it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.85 ms /   172 tokens (    0.07 ms per token, 13390.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =      41.94 ms /     8 runs   (    5.24 ms per token,   190.75 tokens per second)\n",
      "llama_perf_context_print:       total time =      59.07 ms /   180 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.66it/s]\n",
      " 34%|███▎      | 28/83 [00:01<00:03, 16.57it/s]Llama.generate: 267 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.95 ms /   159 tokens (    0.08 ms per token, 13304.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =      52.21 ms /    10 runs   (    5.22 ms per token,   191.52 tokens per second)\n",
      "llama_perf_context_print:       total time =      69.44 ms /   169 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.18it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.99 ms /   158 tokens (    0.08 ms per token, 13182.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.54 ms /     6 runs   (    5.26 ms per token,   190.24 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.89 ms /   164 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.86it/s]\n",
      " 36%|███▌      | 30/83 [00:01<00:03, 16.36it/s]Llama.generate: 267 prefix-match hit, remaining 163 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.69 ms /   163 tokens (    0.08 ms per token, 12840.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.01 ms /     3 runs   (    5.34 ms per token,   187.41 tokens per second)\n",
      "llama_perf_context_print:       total time =      30.68 ms /   166 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.56it/s]\n",
      "Llama.generate: 275 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.97 ms /   159 tokens (    0.08 ms per token, 13277.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.51 ms /     6 runs   (    5.25 ms per token,   190.44 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.86 ms /   165 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.87it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.95 ms /   155 tokens (    0.08 ms per token, 12971.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.62 ms /     6 runs   (    5.27 ms per token,   189.78 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.94 ms /   161 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.84it/s]\n",
      " 40%|███▉      | 33/83 [00:01<00:02, 18.01it/s]Llama.generate: 267 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.02 ms /   160 tokens (    0.08 ms per token, 13316.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.21 ms /     4 runs   (    5.30 ms per token,   188.61 tokens per second)\n",
      "llama_perf_context_print:       total time =      35.66 ms /   164 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.18it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.07 ms /   195 tokens (    0.07 ms per token, 13858.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.31 ms /     4 runs   (    5.33 ms per token,   187.68 tokens per second)\n",
      "llama_perf_context_print:       total time =      37.84 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.68it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.77 ms /   167 tokens (    0.08 ms per token, 13074.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.45 ms /     5 runs   (    5.29 ms per token,   189.01 tokens per second)\n",
      "llama_perf_context_print:       total time =      42.14 ms /   172 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.13it/s]\n",
      " 43%|████▎     | 36/83 [00:01<00:02, 19.46it/s]Llama.generate: 267 prefix-match hit, remaining 158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.96 ms /   158 tokens (    0.08 ms per token, 13206.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =      67.73 ms /    13 runs   (    5.21 ms per token,   191.95 tokens per second)\n",
      "llama_perf_context_print:       total time =      86.28 ms /   171 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.43it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.00 ms /   158 tokens (    0.08 ms per token, 13166.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.16 ms /     4 runs   (    5.29 ms per token,   189.07 tokens per second)\n",
      "llama_perf_context_print:       total time =      35.61 ms /   162 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.23it/s]\n",
      " 46%|████▌     | 38/83 [00:02<00:02, 18.18it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.14 ms /   204 tokens (    0.07 ms per token, 14425.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.72 ms /     6 runs   (    5.29 ms per token,   189.17 tokens per second)\n",
      "llama_perf_context_print:       total time =      49.25 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.89it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 158 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.97 ms /   158 tokens (    0.08 ms per token, 13204.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.57 ms /     6 runs   (    5.26 ms per token,   190.04 tokens per second)\n",
      "llama_perf_context_print:       total time =      46.91 ms /   164 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.85it/s]\n",
      " 48%|████▊     | 40/83 [00:02<00:02, 18.34it/s]Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.70 ms /   164 tokens (    0.08 ms per token, 12910.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =      57.37 ms /    11 runs   (    5.22 ms per token,   191.72 tokens per second)\n",
      "llama_perf_context_print:       total time =      75.75 ms /   175 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.01it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.70 ms /   164 tokens (    0.08 ms per token, 12911.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =      62.57 ms /    12 runs   (    5.21 ms per token,   191.79 tokens per second)\n",
      "llama_perf_context_print:       total time =      81.40 ms /   176 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      " 51%|█████     | 42/83 [00:02<00:02, 16.00it/s]Llama.generate: 267 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.77 ms /   165 tokens (    0.08 ms per token, 12923.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.18 ms /     4 runs   (    5.30 ms per token,   188.83 tokens per second)\n",
      "llama_perf_context_print:       total time =      36.41 ms /   169 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.72it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.74 ms /   165 tokens (    0.08 ms per token, 12951.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =      62.56 ms /    12 runs   (    5.21 ms per token,   191.83 tokens per second)\n",
      "llama_perf_context_print:       total time =      81.45 ms /   177 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      " 53%|█████▎    | 44/83 [00:02<00:02, 15.88it/s]Llama.generate: 267 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.77 ms /   165 tokens (    0.08 ms per token, 12916.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.63 ms /     6 runs   (    5.27 ms per token,   189.68 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.81 ms /   171 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.38it/s]\n",
      "Llama.generate: 273 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.76 ms /   166 tokens (    0.08 ms per token, 13012.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =      63.28 ms /    12 runs   (    5.27 ms per token,   189.62 tokens per second)\n",
      "llama_perf_context_print:       total time =      82.19 ms /   178 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.97it/s]\n",
      " 55%|█████▌    | 46/83 [00:02<00:02, 15.34it/s]Llama.generate: 267 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.91 ms /   177 tokens (    0.07 ms per token, 13715.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =      32.02 ms /     6 runs   (    5.34 ms per token,   187.37 tokens per second)\n",
      "llama_perf_context_print:       total time =      48.32 ms /   183 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.20it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.90 ms /   177 tokens (    0.07 ms per token, 13721.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =      37.15 ms /     7 runs   (    5.31 ms per token,   188.41 tokens per second)\n",
      "llama_perf_context_print:       total time =      53.90 ms /   184 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.11it/s]\n",
      " 58%|█████▊    | 48/83 [00:02<00:02, 15.93it/s]Llama.generate: 267 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.89 ms /   175 tokens (    0.07 ms per token, 13578.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.51 ms /     9 runs   (    5.28 ms per token,   189.45 tokens per second)\n",
      "llama_perf_context_print:       total time =      65.17 ms /   184 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.05it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.01 ms /   183 tokens (    0.07 ms per token, 14062.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =      15.99 ms /     3 runs   (    5.33 ms per token,   187.64 tokens per second)\n",
      "llama_perf_context_print:       total time =      31.01 ms /   186 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.91it/s]\n",
      " 60%|██████    | 50/83 [00:02<00:01, 16.62it/s]Llama.generate: 267 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.01 ms /   184 tokens (    0.07 ms per token, 14140.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.57 ms /     6 runs   (    5.26 ms per token,   190.04 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.97 ms /   190 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.30it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.09 ms /   186 tokens (    0.07 ms per token, 14208.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.60 ms /     6 runs   (    5.27 ms per token,   189.89 tokens per second)\n",
      "llama_perf_context_print:       total time =      48.10 ms /   192 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.33it/s]\n",
      " 63%|██████▎   | 52/83 [00:02<00:01, 17.16it/s]Llama.generate: 267 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.35 ms /   214 tokens (    0.07 ms per token, 14909.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.53 ms /     5 runs   (    5.31 ms per token,   188.45 tokens per second)\n",
      "llama_perf_context_print:       total time =      43.82 ms /   219 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.10it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.20 ms /   208 tokens (    0.07 ms per token, 14649.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =      79.04 ms /    15 runs   (    5.27 ms per token,   189.78 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.81 ms /   223 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]\n",
      " 65%|██████▌   | 54/83 [00:03<00:01, 15.57it/s]Llama.generate: 267 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.44 ms /   221 tokens (    0.07 ms per token, 15305.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.07 ms /     3 runs   (    5.36 ms per token,   186.73 tokens per second)\n",
      "llama_perf_context_print:       total time =      32.53 ms /   224 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.59it/s]\n",
      "Llama.generate: 278 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.16 ms /   189 tokens (    0.07 ms per token, 14363.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.69 ms /     6 runs   (    5.28 ms per token,   189.35 tokens per second)\n",
      "llama_perf_context_print:       total time =      48.33 ms /   195 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.16it/s]\n",
      "Llama.generate: 278 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.07 ms /   188 tokens (    0.07 ms per token, 14385.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =      67.94 ms /    13 runs   (    5.23 ms per token,   191.33 tokens per second)\n",
      "llama_perf_context_print:       total time =      87.62 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.26it/s]\n",
      " 69%|██████▊   | 57/83 [00:03<00:01, 15.81it/s]Llama.generate: 267 prefix-match hit, remaining 179 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.94 ms /   179 tokens (    0.07 ms per token, 13838.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.82 ms /     4 runs   (    5.46 ms per token,   183.29 tokens per second)\n",
      "llama_perf_context_print:       total time =      37.26 ms /   183 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.01it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.15 ms /   199 tokens (    0.07 ms per token, 14067.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.24 ms /     4 runs   (    5.31 ms per token,   188.34 tokens per second)\n",
      "llama_perf_context_print:       total time =      37.85 ms /   203 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.65it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.07 ms /   195 tokens (    0.07 ms per token, 13858.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.61 ms /     6 runs   (    5.27 ms per token,   189.81 tokens per second)\n",
      "llama_perf_context_print:       total time =      49.08 ms /   201 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.94it/s]\n",
      " 72%|███████▏  | 60/83 [00:03<00:01, 17.43it/s]Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.12 ms /   200 tokens (    0.07 ms per token, 14160.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.43 ms /     5 runs   (    5.29 ms per token,   189.21 tokens per second)\n",
      "llama_perf_context_print:       total time =      43.48 ms /   205 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.44it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.18 ms /   204 tokens (    0.07 ms per token, 14390.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.20 ms /     9 runs   (    5.24 ms per token,   190.68 tokens per second)\n",
      "llama_perf_context_print:       total time =      66.13 ms /   213 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.88it/s]\n",
      " 75%|███████▍  | 62/83 [00:03<00:01, 17.22it/s]Llama.generate: 267 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.16 ms /   201 tokens (    0.07 ms per token, 14191.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.63 ms /     6 runs   (    5.27 ms per token,   189.71 tokens per second)\n",
      "llama_perf_context_print:       total time =      49.17 ms /   207 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.83it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.11 ms /   200 tokens (    0.07 ms per token, 14174.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.65 ms /     6 runs   (    5.28 ms per token,   189.56 tokens per second)\n",
      "llama_perf_context_print:       total time =      49.15 ms /   206 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.88it/s]\n",
      " 77%|███████▋  | 64/83 [00:03<00:01, 17.47it/s]Llama.generate: 267 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.27 ms /   211 tokens (    0.07 ms per token, 14791.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =      32.20 ms /     6 runs   (    5.37 ms per token,   186.31 tokens per second)\n",
      "llama_perf_context_print:       total time =      49.86 ms /   217 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.60it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.07 ms /   187 tokens (    0.07 ms per token, 14303.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =      62.70 ms /    12 runs   (    5.23 ms per token,   191.39 tokens per second)\n",
      "llama_perf_context_print:       total time =      81.91 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      " 80%|███████▉  | 66/83 [00:03<00:01, 16.33it/s]Llama.generate: 267 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.10 ms /   187 tokens (    0.07 ms per token, 14274.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.48 ms /     5 runs   (    5.30 ms per token,   188.84 tokens per second)\n",
      "llama_perf_context_print:       total time =      42.49 ms /   192 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.89it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      15.27 ms /   234 tokens (    0.07 ms per token, 15329.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.80 ms /     6 runs   (    5.30 ms per token,   188.69 tokens per second)\n",
      "llama_perf_context_print:       total time =      50.47 ms /   240 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.35it/s]\n",
      " 82%|████████▏ | 68/83 [00:03<00:00, 17.03it/s]Llama.generate: 267 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.12 ms /   189 tokens (    0.07 ms per token, 14405.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =      52.43 ms /    10 runs   (    5.24 ms per token,   190.73 tokens per second)\n",
      "llama_perf_context_print:       total time =      70.79 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.89it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.85 ms /   151 tokens (    0.08 ms per token, 12739.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =      16.06 ms /     3 runs   (    5.35 ms per token,   186.75 tokens per second)\n",
      "llama_perf_context_print:       total time =      29.90 ms /   154 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.19it/s]\n",
      " 84%|████████▍ | 70/83 [00:04<00:00, 17.23it/s]Llama.generate: 267 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      15.15 ms /   225 tokens (    0.07 ms per token, 14853.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =      36.94 ms /     7 runs   (    5.28 ms per token,   189.50 tokens per second)\n",
      "llama_perf_context_print:       total time =      55.94 ms /   232 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.50it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.07 ms /   195 tokens (    0.07 ms per token, 13862.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.28 ms /     9 runs   (    5.25 ms per token,   190.36 tokens per second)\n",
      "llama_perf_context_print:       total time =      66.11 ms /   204 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.88it/s]\n",
      " 87%|████████▋ | 72/83 [00:04<00:00, 16.48it/s]Llama.generate: 267 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.20 ms /   205 tokens (    0.07 ms per token, 14440.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =      26.49 ms /     5 runs   (    5.30 ms per token,   188.74 tokens per second)\n",
      "llama_perf_context_print:       total time =      43.62 ms /   210 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.32it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.91 ms /   177 tokens (    0.07 ms per token, 13713.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =      42.50 ms /     8 runs   (    5.31 ms per token,   188.24 tokens per second)\n",
      "llama_perf_context_print:       total time =      59.70 ms /   185 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.45it/s]\n",
      " 89%|████████▉ | 74/83 [00:04<00:00, 16.70it/s]Llama.generate: 267 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.11 ms /   190 tokens (    0.07 ms per token, 14494.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.57 ms /     6 runs   (    5.26 ms per token,   190.05 tokens per second)\n",
      "llama_perf_context_print:       total time =      48.05 ms /   196 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.32it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.72 ms /   161 tokens (    0.08 ms per token, 12653.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =      57.40 ms /    11 runs   (    5.22 ms per token,   191.62 tokens per second)\n",
      "llama_perf_context_print:       total time =      75.83 ms /   172 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.02it/s]\n",
      " 92%|█████████▏| 76/83 [00:04<00:00, 16.06it/s]Llama.generate: 267 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.17 ms /   204 tokens (    0.07 ms per token, 14398.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =      21.31 ms /     4 runs   (    5.33 ms per token,   187.67 tokens per second)\n",
      "llama_perf_context_print:       total time =      37.95 ms /   208 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.60it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      14.10 ms /   197 tokens (    0.07 ms per token, 13971.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =      57.57 ms /    11 runs   (    5.23 ms per token,   191.06 tokens per second)\n",
      "llama_perf_context_print:       total time =      77.38 ms /   208 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      " 94%|█████████▍| 78/83 [00:04<00:00, 15.91it/s]Llama.generate: 279 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      13.04 ms /   184 tokens (    0.07 ms per token, 14108.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =      78.35 ms /    15 runs   (    5.22 ms per token,   191.45 tokens per second)\n",
      "llama_perf_context_print:       total time =      98.98 ms /   199 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.90it/s]\n",
      "Llama.generate: 279 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.00 ms /   160 tokens (    0.07 ms per token, 13337.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =      63.25 ms /    12 runs   (    5.27 ms per token,   189.72 tokens per second)\n",
      "llama_perf_context_print:       total time =      81.42 ms /   172 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      " 96%|█████████▋| 80/83 [00:04<00:00, 13.66it/s]Llama.generate: 308 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      11.63 ms /   131 tokens (    0.09 ms per token, 11261.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =      73.51 ms /    14 runs   (    5.25 ms per token,   190.44 tokens per second)\n",
      "llama_perf_context_print:       total time =      92.22 ms /   145 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.70it/s]\n",
      "Llama.generate: 267 prefix-match hit, remaining 171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.80 ms /   171 tokens (    0.07 ms per token, 13360.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =      31.53 ms /     6 runs   (    5.25 ms per token,   190.31 tokens per second)\n",
      "llama_perf_context_print:       total time =      47.73 ms /   177 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.42it/s]\n",
      " 99%|█████████▉| 82/83 [00:04<00:00, 13.43it/s]Llama.generate: 267 prefix-match hit, remaining 171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      61.73 ms\n",
      "llama_perf_context_print: prompt eval time =      12.81 ms /   171 tokens (    0.07 ms per token, 13352.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =      47.30 ms /     9 runs   (    5.26 ms per token,   190.28 tokens per second)\n",
      "llama_perf_context_print:       total time =      64.88 ms /   180 tokens\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.12it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 16.71it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_init.initate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =      57.73 ms /   185 tokens (    0.31 ms per token,  3204.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =      42.37 ms /     8 runs   (    5.30 ms per token,   188.83 tokens per second)\n",
      "llama_perf_context_print:       total time =     104.91 ms /   193 tokens\n",
      " 10%|█         | 1/10 [00:00<00:01,  5.86it/s]Llama.generate: 143 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =      10.20 ms /    73 tokens (    0.14 ms per token,  7154.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =      51.84 ms /    10 runs   (    5.18 ms per token,   192.88 tokens per second)\n",
      "llama_perf_context_print:       total time =      67.47 ms /    83 tokens\n",
      " 20%|██        | 2/10 [00:00<00:01,  6.49it/s]Llama.generate: 132 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =       9.54 ms /    56 tokens (    0.17 ms per token,  5870.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =      51.23 ms /    10 runs   (    5.12 ms per token,   195.19 tokens per second)\n",
      "llama_perf_context_print:       total time =      66.34 ms /    66 tokens\n",
      " 30%|███       | 3/10 [00:00<00:00,  7.35it/s]Llama.generate: 132 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =       9.37 ms /    53 tokens (    0.18 ms per token,  5658.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =      41.15 ms /     8 runs   (    5.14 ms per token,   194.43 tokens per second)\n",
      "llama_perf_context_print:       total time =      54.96 ms /    61 tokens\n",
      "Llama.generate: 132 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =       9.36 ms /    53 tokens (    0.18 ms per token,  5663.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =      41.16 ms /     8 runs   (    5.15 ms per token,   194.36 tokens per second)\n",
      "llama_perf_context_print:       total time =      54.94 ms /    61 tokens\n",
      " 50%|█████     | 5/10 [00:00<00:00,  8.75it/s]Llama.generate: 142 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =      10.29 ms /    81 tokens (    0.13 ms per token,  7870.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =      81.48 ms /    16 runs   (    5.09 ms per token,   196.37 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.99 ms /    97 tokens\n",
      " 60%|██████    | 6/10 [00:00<00:00,  7.62it/s]Llama.generate: 145 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =       8.84 ms /    46 tokens (    0.19 ms per token,  5205.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =      51.25 ms /    10 runs   (    5.12 ms per token,   195.14 tokens per second)\n",
      "llama_perf_context_print:       total time =      65.45 ms /    56 tokens\n",
      " 70%|███████   | 7/10 [00:00<00:00,  7.94it/s]Llama.generate: 132 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =       9.38 ms /    56 tokens (    0.17 ms per token,  5969.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =      46.23 ms /     9 runs   (    5.14 ms per token,   194.69 tokens per second)\n",
      "llama_perf_context_print:       total time =      60.53 ms /    65 tokens\n",
      " 80%|████████  | 8/10 [00:01<00:00,  8.26it/s]Llama.generate: 132 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =       9.46 ms /    59 tokens (    0.16 ms per token,  6240.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =      51.28 ms /    10 runs   (    5.13 ms per token,   195.02 tokens per second)\n",
      "llama_perf_context_print:       total time =      66.14 ms /    69 tokens\n",
      " 90%|█████████ | 9/10 [00:01<00:00,  8.40it/s]Llama.generate: 142 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      57.92 ms\n",
      "llama_perf_context_print: prompt eval time =       9.90 ms /    78 tokens (    0.13 ms per token,  7879.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =      66.48 ms /    13 runs   (    5.11 ms per token,   195.55 tokens per second)\n",
      "llama_perf_context_print:       total time =      83.20 ms /    91 tokens\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the sentence is located in the mentioned context',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Mention', relation='locatedIn', target='Sentence', link=SubjectLink(link_id=118, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', label='Sentence', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Sentence', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3893), instance_count=12076))], entities=[EnrichedEntity(identifier='Mention', type='Mention', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668)), EnrichedEntity(identifier='Sentence', type='Sentence', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', label='Sentence', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Sentence', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3893))], message='Found Relations and Entities')),\n",
       " ('a mention located in a part of the microbiome',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Mention', relation='locatedIn', target='Sentence', link=SubjectLink(link_id=118, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', label='Sentence', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Sentence', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3893), instance_count=12076)), EnrichedRelation(entity='Sentence', relation='partOf', target='Microbiome', link=SubjectLink(link_id=185, label='partOf', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Microbiome>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/partOf>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', label='Sentence', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Sentence', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3893), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Microbiome>', label='Microbiome', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Microbiome', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=21), instance_count=4320))], entities=[EnrichedEntity(identifier='Mention', type='Mention', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668)), EnrichedEntity(identifier='Sentence', type='Sentence', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', label='Sentence', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Sentence', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3893)), EnrichedEntity(identifier='Microbiome', type='Microbiome', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Microbiome>', label='Microbiome', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Microbiome', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=21))], message='Found Relations and Entities')),\n",
       " ('An anatomic site is located in a sentence.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Anatomic Site', relation='locatedIn', target='Sentence', link=SubjectLink(link_id=9, label='locatedIn', from_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', label='Sentence', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Sentence', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3893), instance_count=12076))], entities=[EnrichedEntity(identifier='Anatomic Site', type='Anatomic Site', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84)), EnrichedEntity(identifier='Sentence', type='Sentence', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Sentence>', label='Sentence', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Sentence', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3893))], message='Found Relations and Entities')),\n",
       " ('A mention is located in a human.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Mention', relation='locatedIn', target='Human', link=SubjectLink(link_id=117, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', label='Human', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Human', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=72), instance_count=12076))], entities=[EnrichedEntity(identifier='Mention', type='Mention', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668)), EnrichedEntity(identifier='Human', type='Human', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', label='Human', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Human', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=72))], message='Found Relations and Entities')),\n",
       " ('A chemical is located in an animal.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Chemical', relation='locatedIn', target='Animal', link=SubjectLink(link_id=83, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Animal>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', label='Chemical', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Chemical', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/InterventionOrProcedure>', label='Intervention or Procedure')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=368), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Animal>', label='Animal', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Animal', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=22), instance_count=12076))], entities=[EnrichedEntity(identifier='Chemical', type='Chemical', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', label='Chemical', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Chemical', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/InterventionOrProcedure>', label='Intervention or Procedure')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=368)), EnrichedEntity(identifier='Animal', type='Animal', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Animal>', label='Animal', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Animal', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=22))], message='Found Relations and Entities')),\n",
       " ('A chemical compound located in an anatomic site is part of a metabolite.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Chemical', relation='locatedIn', target='Anatomic Site', link=SubjectLink(link_id=82, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', label='Chemical', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Chemical', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/InterventionOrProcedure>', label='Intervention or Procedure')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=368), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84), instance_count=12076)), EnrichedRelation(entity='Metabolite', relation='partOf', target='Chemical', link=SubjectLink(link_id=122, label='partOf', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Metabolite>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/partOf>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Metabolite>', label='Metabolite', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Metabolite', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=0), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', label='Chemical', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Chemical', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/InterventionOrProcedure>', label='Intervention or Procedure')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=368), instance_count=4320))], entities=[EnrichedEntity(identifier='Chemical', type='Chemical', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', label='Chemical', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Chemical', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/InterventionOrProcedure>', label='Intervention or Procedure')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=368)), EnrichedEntity(identifier='Anatomic Site', type='Anatomic Site', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84)), EnrichedEntity(identifier='Metabolite', type='Metabolite', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Metabolite>', label='Metabolite', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Metabolite', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=0))], message='Found Relations and Entities')),\n",
       " ('A chemical is located in an anatomic site.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Chemical', relation='locatedIn', target='Anatomic Site', link=SubjectLink(link_id=82, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', label='Chemical', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Chemical', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/InterventionOrProcedure>', label='Intervention or Procedure')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=368), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84), instance_count=12076))], entities=[EnrichedEntity(identifier='Chemical', type='Chemical', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Chemical>', label='Chemical', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Chemical', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/brainteaser/ontology/schema/InterventionOrProcedure>', label='Intervention or Procedure')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=368)), EnrichedEntity(identifier='Anatomic Site', type='Anatomic Site', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84))], message='Found Relations and Entities')),\n",
       " ('Anatomic Site is located in the Human.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Anatomic Site', relation='locatedIn', target='Human', link=SubjectLink(link_id=8, label='locatedIn', from_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', label='Human', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Human', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=72), instance_count=12076))], entities=[EnrichedEntity(identifier='Anatomic Site', type='Anatomic Site', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84)), EnrichedEntity(identifier='Human', type='Human', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', label='Human', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Human', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=72))], message='Found Relations and Entities')),\n",
       " ('A mention is located in an anatomic site.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Mention', relation='locatedIn', target='Anatomic Site', link=SubjectLink(link_id=116, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668), to_subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84), instance_count=12076))], entities=[EnrichedEntity(identifier='Mention', type='Mention', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668)), EnrichedEntity(identifier='Anatomic Site', type='Anatomic Site', constraints=[], subject=Subject(subject_id='<https://w3id.org/brainteaser/ontology/schema/AnatomicalSite>', label='Anatomic Site', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Anatomic Site', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=84))], message='Found Relations and Entities')),\n",
       " ('A dietary supplement is contained within a mention related to a human.',\n",
       "  EnrichedEntitiesRelations(relations=[EnrichedRelation(entity='Mention', relation='locatedIn', target='Human', link=SubjectLink(link_id=117, label='locatedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/locatedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', label='Human', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Human', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=72), instance_count=12076)), EnrichedRelation(entity='Dietary Supplement', relation='containedIn', target='Mention', link=SubjectLink(link_id=42, label='containedIn', from_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/DietarySupplement>', link_type='owl:ObjectProperty', to_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', to_proptype=None, property_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/containedIn>', from_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/DietarySupplement>', label='Dietary Supplement', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Dietary Supplement', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Food>', label='Food')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=62), to_subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668), instance_count=3708))], entities=[EnrichedEntity(identifier='Mention', type='Mention', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Mention>', label='Mention', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Mention', label=None)])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=3668)), EnrichedEntity(identifier='Human', type='Human', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/Human>', label='Human', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Human', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Organism>', label='Organism')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=72)), EnrichedEntity(identifier='Dietary Supplement', type='Dietary Supplement', constraints=[], subject=Subject(subject_id='<https://w3id.org/hereditary/ontology/gutbrain/schema/DietarySupplement>', label='Dietary Supplement', spos={'rdf:type': Property(property='rdf:type', label=None, values=[PropertyValue(value='owl:Class', label=None)]), 'rdfs:label': Property(property='rdfs:label', label=None, values=[PropertyValue(value='Dietary Supplement', label=None)]), 'rdfs:subClassOf': Property(property='rdfs:subClassOf', label=None, values=[PropertyValue(value='<https://w3id.org/hereditary/ontology/gutbrain/schema/Food>', label='Food')])}, subject_type='class', refcount=0, descendants={}, total_descendants=0, properties={}, instance_count=62))], message='Found Relations and Entities'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_query.initiate(n_queries=10, force=True, seed=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onset-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
