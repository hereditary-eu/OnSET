{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../backend\"))\n",
    "sys.path.append(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.assistant.iterative_assistant import (\n",
    "    IterativeAssistant,\n",
    "    QueryGraph,\n",
    "    AssistantSubject,\n",
    "    AssistantLink,\n",
    "    AssistantSubQuery,\n",
    "    AssistantSubQueryType,\n",
    ")\n",
    "\n",
    "from rdflib.plugins.stores.sparqlstore import SPARQLStore\n",
    "from backend.ontology import OntologyManager, OntologyConfig, Graph\n",
    "from backend.explorative.explorative_support import GuidanceManager\n",
    "from backend.explorative.llm_query import (\n",
    "    EnrichedEntitiesRelations,\n",
    "    LLMQuery,\n",
    "    QueryProgress,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = SPARQLStore(\n",
    "    \"http://localhost:7012/\",\n",
    "    method=\"POST_FORM\",\n",
    "    params={\"infer\": False, \"sameAs\": False},\n",
    ")\n",
    "graph = Graph(store=store)\n",
    "\n",
    "config = OntologyConfig()\n",
    "\n",
    "ontology_manager = OntologyManager(config, graph)\n",
    "topic_man = GuidanceManager(ontology_manager)\n",
    "\n",
    "assistant = IterativeAssistant(topic_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = QueryGraph(\n",
    "    subjects=[\n",
    "        AssistantSubject(\n",
    "            internal_id=\"1\",\n",
    "            subject_id=\"Person\",\n",
    "        ),\n",
    "        \n",
    "        AssistantSubject(\n",
    "            internal_id=\"2\",\n",
    "            subject_id=\"Place\",\n",
    "        )\n",
    "    ],\n",
    "    links=[\n",
    "        AssistantLink(\n",
    "            from_id=\"Person\",\n",
    "            to_id=\"Place\",\n",
    "            link_id=\"birthPlace\",\n",
    "            \n",
    "            from_internal_id=\"1\",\n",
    "            to_internal_id=\"2\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 163 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     420.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   16065.69 ms /   234 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Ops operations=[Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='Author', internal_id='3', subqueries=[AssistantSubQuery(type='subquery', constraint_type=<AssistantSubQueryType.SUBJECT: 'subject'>, field='author', from_internal_id='1', from_id='Person'), AssistantSubQuery(type='subquery', constraint_type=<AssistantSubQueryType.SUBJECT: 'subject'>, field='author', from_internal_id='2', from_id='Place')], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='Work', to_id='Author', from_internal_id='4', to_internal_id='3', link_id='author'))]\n",
      "Candidate Ops operations=[Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='writer', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='novel', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='Playwright', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='poet', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='historian', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='author')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='auteur')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='written work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='author of preface')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='sequence of works', from_internal_id='4', to_internal_id='3', link_id='other works')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='work', from_internal_id='4', to_internal_id='3', link_id='subsequent work'))]\n",
      "operations=[Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='writer', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='novel', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='Playwright', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='poet', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='historian', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='author')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='auteur')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='written work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='author of preface')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='sequence of works', from_internal_id='4', to_internal_id='3', link_id='other works')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='work', from_internal_id='4', to_internal_id='3', link_id='subsequent work'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 163 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     420.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12800.41 ms /   185 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained Ops operations=[ConstrainedOperation(operation=<OperationType.ADD: 'add'>, data=ConstrainedAssistantSubject(type='subject', subject_id=<SubjectID.writer: 'writer'>, internal_id='3', subqueries=[AssistantSubQuery(type='subquery', constraint_type=<AssistantSubQueryType.SUBJECT: 'subject'>, field='author', from_internal_id='1', from_id='Person')], x=0.0, y=0.0)), ConstrainedOperation(operation=<OperationType.ADD: 'add'>, data=ConstrainedAssistantLink(type='link', from_internal_id='1', to_internal_id='3', link_id=<LinkID.written work_author of preface_person: 'written work|author of preface|person'>))]\n",
      "type='link' from_internal_id='1' to_internal_id='3' link_id=<LinkID.written work_author of preface_person: 'written work|author of preface|person'>\n",
      "Constrained Ops operations=[Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='writer', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='novel', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='Playwright', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='poet', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='historian', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='author')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='auteur')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='written work', to_id='person', from_internal_id='4', to_internal_id='3', link_id='author of preface')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='sequence of works', from_internal_id='4', to_internal_id='3', link_id='other works')), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='work', to_id='work', from_internal_id='4', to_internal_id='3', link_id='subsequent work'))]\n",
      "Corrected Ops operations=[Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='<http://dbpedia.org/ontology/Writer>', internal_id='3', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantLink(type='link', from_id='<http://dbpedia.org/ontology/WrittenWork>', to_id='<http://dbpedia.org/ontology/Person>', from_internal_id='1', to_internal_id='3', link_id=2439)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='<http://dbpedia.org/ontology/WrittenWork>', internal_id='1', subqueries=[], x=0.0, y=0.0)), Operation(operation=<OperationType.ADD: 'add'>, data=AssistantSubject(type='subject', subject_id='<http://dbpedia.org/ontology/Person>', internal_id='3', subqueries=[], x=0.0, y=0.0))]\n"
     ]
    }
   ],
   "source": [
    "resulting_ops = assistant.run_query(\"add the author link from a work the person\", graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= (\" \"| \"\\n\") grammar-models\n",
      "grammar-models ::= combiner-type\n",
      "combiner-type ::= \"{\"  ws \"\\\"combined_type\\\"\" \": \" combiner-type-combined-type \",\"  ws \"\\\"enum_literal\\\"\" \": \" (\"\\\"test\\\"\") \",\"  ws \"\\\"int_literal\\\"\" \": \" (\"1\"|\"5\")  ws \"}\"\n",
      "test-type ::= \"{\"  ws \"\\\"type\\\"\" \": \" (\"\\\"test\\\"\") \",\"  ws \"\\\"name\\\"\" \": \" string  ws \"}\"\n",
      "target-type ::= \"{\"  ws \"\\\"type\\\"\" \": \" (\"\\\"target\\\"\") \",\"  ws \"\\\"other\\\"\" \": \" string  ws \"}\"\n",
      "combiner-type-combined-type-element-union ::= test-type | target-type\n",
      "combiner-type-combined-type ::= \"[\" ws (combiner-type-combined-type-element-union)? (\",\" ws combiner-type-combined-type-element-union)* ws \"]\" \n",
      "\n",
      "boolean ::= \"true\" | \"false\"\n",
      "null ::= \"null\"\n",
      "string ::= \"\\\"\" (\n",
      "        [^\"\\\\] |\n",
      "        \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])\n",
      "      )* \"\\\"\"\n",
      "ws ::= ([ \\t\\n]+)\n",
      "number ::= \"-\"? ([0-9]+ | [0-9]+ \".\" [0-9]+) ([eE] [-+]? [0-9]+)?\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp_agent.gbnf_grammar_generator.gbnf_grammar_from_pydantic_models import (\n",
    "    generate_gbnf_grammar_from_pydantic_models,\n",
    ")\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any, Literal\n",
    "from enum import Enum\n",
    "\n",
    "class TestEnum(str, Enum):\n",
    "    TEST = \"test\"\n",
    "    TARGET = \"target\"\n",
    "class TestType(BaseModel):\n",
    "    type: Literal[\"test\"]\n",
    "    name: str\n",
    "\n",
    "\n",
    "class TargetType(BaseModel):\n",
    "    type: Literal[\"target\"]\n",
    "    other: str\n",
    "\n",
    "\n",
    "class CombinerType(BaseModel):\n",
    "    combined_type: list[TestType | TargetType]\n",
    "    enum_literal: Literal[TestEnum.TEST]\n",
    "    int_literal: Literal[1,5]\n",
    "\n",
    "grammar = generate_gbnf_grammar_from_pydantic_models(\n",
    "    [CombinerType],\n",
    ")\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_cpp.llama import Llama, LlamaGrammar\n",
    "grammar_constrained = LlamaGrammar.from_string(grammar)\n",
    "# grammar_constrained._grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 8 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     420.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3386.94 ms /    61 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"combined_type\": [\n",
      "      {\n",
      "        \"type\": \"target\",\n",
      "        \"other\": \"test\"\n",
      "      }\n",
      "    ],\n",
      "    \"enum_literal\": \"test\",\n",
      "    \"int_literal\": 5\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = topic_man.llama_model.create_chat_completion(\n",
    "    grammar=grammar_constrained,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"set the int literal 5 and enum literal test, create target\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=128,\n",
    "    temperature=0.1,\n",
    "    seed=2\n",
    ")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinerType(combined_type=[TargetType(type='target', other='test')], enum_literal=<TestEnum.TEST: 'test'>, int_literal=5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CombinerType.model_validate_json(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
