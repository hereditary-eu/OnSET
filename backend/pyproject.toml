[project]
name = "onset-backend"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = "==3.12.8"
dependencies = [
    "fastapi[standard]",
    "python-multipart",
    "pandas",
    "numpy",
    "owlready2",
    "networkx",
    "pyvis",
    "rdflib[networkx,html,lxml,orjson]",
    "fastapi",
    "psycopg[binary]",
    "sentence-transformers",
    "SQLAlchemy",
    "sqlmodel",
    "torch",
    "jupyter",
    "ipywidgets",
    "transformers",
    "bertopic",
    "pgvector",
    "cachetools",
    "redis-om",
    "matplotlib",
    "llama-cpp-python==0.3.4",
    "llvmlite>=0.44.0",
    "llama-cpp-agent",
    "seqeval",
    "sparql-llm>=0.0.3",
    "langchain>=0.3.14",
    "langchain-huggingface>=0.1.2",
    "langchain-community>=0.3.14",
    "qlever>=0.5.15",
    "qdrant-client>=1.13.0",
    "sparqlwrapper>=2.0.0",
    "openai>=1.59.9",
    # "onnxruntime-gpu",
    # "fastembed-gpu",
    "line-profiler>=4.2.0",
    "json-repair>=0.46.0",
    "tikzplotlib-patched",
    "seaborn>=0.13.2",
]

[project.optional-dependencies]
dev = [
    "tuna>=0.5.11",
]

[tool.uv.sources]
llama-cpp-python = [
    { index = "llama-metal", marker = "platform_system == 'Darwin'" },
    { index = "llama-gpu", marker = "platform_system == 'Linux'" },
]
llama-cpp-agent = [
    { git = "https://github.com/Dakantz/llama-cpp-agent.git", branch = "add-literal-support" },
]
tikzplotlib-patched = [
    { git = "https://github.com/JasonGross/tikzplotlib.git", branch = "patched" },
]


[[tool.uv.index]]
name = "llama-metal"
url = "https://abetlen.github.io/llama-cpp-python/whl/metal/"
explicit = true

[[tool.uv.index]]
name = "llama-gpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124/"
explicit = true

