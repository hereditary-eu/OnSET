[project]
name = "onset-backend"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = "==3.12.8"
dependencies = [
    "fastapi[standard]",
    "python-multipart",
    "pandas",
    "numpy",
    "owlready2",
    "networkx",
    "pyvis",
    "rdflib[networkx,html,lxml,orjson]",
    "fastapi",
    "psycopg[binary]",
    "sentence-transformers",
    "SQLAlchemy",
    "sqlmodel",
    "torch",
    "jupyter",
    "ipywidgets",
    "transformers",
    "bertopic",
    "pgvector",
    "cachetools",
    "redis-om",
    "matplotlib",
    "llama-cpp-python==0.3.4",
    "llvmlite",
    "llama-cpp-agent",
    "seqeval",
    "sparql-llm>=0.0.3",
    "langchain>=0.3.14",
    "langchain-huggingface>=0.1.2",
    "langchain-community>=0.3.14",
    "qlever>=0.5.15",
    "qdrant-client>=1.13.0",
    "sparqlwrapper>=2.0.0",
    "openai>=1.59.9",
    "onnxruntime-gpu",
    "fastembed-gpu",
]

[tool.uv.sources]
llama-cpp-python = [
    { index = "llama-metal", marker = "platform_system == 'Darwin'" },
    { index = "llama-gpu", marker = "platform_system == 'Linux'" },
]
onnxruntime-gpu = [
    { index = "onnxruntime-gpu", marker = "platform_system == 'Linux'" },
]
llama-cpp-agent = [
    { git = "https://github.com/Dakantz/llama-cpp-agent.git", branch = "fix-gbnf-generation-trailing-bracket" },
]


[[tool.uv.index]]
name = "llama-metal"
url = "https://abetlen.github.io/llama-cpp-python/whl/metal/"
explicit = true

[[tool.uv.index]]
name = "llama-gpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124/"
explicit = true

[[tool.uv.index]]
name = "onnxruntime-gpu"
url = "https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/"
explicit = true
